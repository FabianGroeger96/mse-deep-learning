{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)\n",
    "*Give two desired properties of word embeddings when used in a classification task (such as sentiment classification).*\n",
    "\n",
    " - In the embedding space, simmilar words should be close to each other and dissimilar ones far apart such that a classifier can easily deduce a meaningful decision boundary. Hence we want the space to exploit \"good\" features. \n",
    " - The embedding space should already be initialized with basic knowledge (pretraining) and should only be fine tuned on a specific task such as sentiment classification.\n",
    "\n",
    "# (b)\n",
    "*What are the three strategies to use an embedding layer in a deep system ?*\n",
    "\n",
    " - use a pretrained embedding space and regard it as fixed\n",
    " - use a pretrained embedding space and fine tune it on the given task\n",
    " - use a randomly initialized embedding space and relearn it from scratch (requires a lot of data)\n",
    "\n",
    "# (c)\n",
    "*Why is the system of exercise 1 working in the end ?*\n",
    "\n",
    "I guess because context similar words tend to have a similar sentiment.\n",
    "\n",
    "\n",
    "# (d)\n",
    "*What is the difference between Word2Vec and FastText ?*\n",
    "\n",
    "The Word2Vec approach models words as a whole, whereas FastText models words as a composition of n-grams which enables it to handle out-of-vocabulary words.\n",
    "\n",
    "\n",
    "# (e)\n",
    "*What are the different forms of sequence mapping allowed by recurrent neural networks ? Give for each form an example of application.*\n",
    "\n",
    " - **DUPLICATE OF PW 11**\n",
    "\n",
    "# (f)\n",
    "*Compute the number of parameters to be trained for a two-layer SimpleRNN and softmax with hidden state dimensions 32 and 64, respectively, 10 classes to classify in the softmax and inputs given by sequences of length 100 and each element a vector of dimension 30.*\n",
    "\n",
    " - **DUPLICATE OF PW 11**\n",
    "\n",
    "\n",
    "\n",
    "# (g)\n",
    "*Compute the number of parameters to be trained for a two-layer LSTM and softmax with hidden state dimensions 32 and 64, respectively, 10 classes to classify in the softmax and inputs given by sequences of length 100 and each element a vector of dimension 30.*\n",
    "\n",
    "$$\n",
    "4\\cdot(32\\times 30 + 32 \\times 32 + 32) + 4\\cdot(64\\times32 +64\\times 64 + 64) + (10\\times64 + 10) = 33546\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 32)           8064      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 33,546\n",
      "Trainable params: 33,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(32, input_shape=(100, 30), \n",
    "                    return_sequences=True))\n",
    "model.add(keras.layers.LSTM(64, \n",
    "                    return_sequences=False))\n",
    "model.add(keras.layers.Dense(10))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (h)\n",
    "*Why is gradient clipping rather needed in long than in short sentences ?*\n",
    "\n",
    " - **DUPLICATE OF PW 11**\n",
    "\n",
    "\n",
    "\n",
    "# (i)\n",
    "*In what situations would you expect LSTMs (by its design) to perform better than SimpleRNNs?*\n",
    "\n",
    "As soon as we have to work with long sequences, where long term dependencies are important for the task at hand.\n",
    "\n",
    "\n",
    "# (j)\n",
    "*Describe why SimpleRNNs have problems in learning long-term dependencies.*\n",
    "\n",
    " - **DUPLICATE OF PW 11**\n",
    "\n",
    "\n",
    "\n",
    "# (k)\n",
    "*How can you define a generative system for sequence data ? Describe the two approaches seen in the class to build generative systems for sequence data.*\n",
    "\n",
    " - **DUPLICATE OF PW 11**\n",
    "\n",
    "\n",
    "\n",
    "# (l)\n",
    "*Draw the architecture of a Seq2Seq model and explain the encoder/decoder concept.*\n",
    "\n",
    "The Seq2Seq architecture consists of two main building blocks, an encoder and a decoder. The encoder first *reads* the whole input sequence and *somehow* encodes the knowledge contained in it which could be refered to as a *context based* embedding. After the encoder is finished, the decoder starts to generate the required output sequence, based on the knowledge previously encoded by the encoder and on the generated token by the decoder itself.   \n",
    "\n",
    "<img src=\"https://d2l.ai/_images/seq2seq-details.svg\" /> \n",
    "\n",
    "During the training a method called *teacher forcing* is used to train the decoder. Instead of providing the decoder with the previously predicted tokens, it is provided with the actual ground truth tokens in an autoregressive manner.\n",
    "\n",
    "\n",
    "# (m)\n",
    "*What is the main problem of a basic Seq2Seq model used for machine translation and the solution that can be used to overcome this problem ?*\n",
    "\n",
    "If a Seq2Seq model is based on RNN's, the encoder must compress the whole information of the input sequence in a single vector (hidden state of RNN). This might be possible in theory, but as we already know learning long term dependencies isn't exactly the strength of RNN's. Especially in the case of machine translation the start of the input sequence can be very important for the end of the target sequence. A solution for that problem is the aplication of a technique called *Attention*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
