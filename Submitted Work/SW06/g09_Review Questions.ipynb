{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)\n",
    "*In computational graphs, we say that the \"+\" gate is a gradient distributor. Why ? How can we qualify the multiplication gate and the max gate ?*\n",
    "\n",
    "An addition gate is a gradient distributor because the derivative operator is distributive\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} (x\\cdot y + x\\cdot z) = \\frac{d}{dx}(x\\cdot y) + \\frac{d}{dx}(x\\cdot z)\n",
    "$$\n",
    "\n",
    "The multiplication gate can be called a switcher due to\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} (x\\cdot y) = y \\quad \\frac{d}{dy} (x\\cdot y) = x\n",
    "$$\n",
    "\n",
    "And the max gate a routing gate as\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\text{max}(x, y) = 1 \\quad \\frac{d}{dy} \\text{max}(x, y) = 0 \\quad \\text{assuming that } x > y\n",
    "$$\n",
    "\n",
    "## (b)\n",
    "*Give 4 advantages of using computational graphs for learning strategies.*\n",
    " - The gradient calculation can be implemented once at a granular level and can then be used to derivate arbitrary complex graph calculations.\n",
    " - Backpropagation of gradient is intuitive.\n",
    " - Graphs can be compiled and hence optimized.\n",
    " - Nodes of a graph can be implemented as classes and therefore can easily be adapted or extended.\n",
    " - Custom nodes can be composed of sevral atomic operations or can also be factorized in order to reduce computational complexity.\n",
    " - The loss function, gradient calculation and weights update can be included into the graph. Therefore only a minimal dataflow between the GPU memory and the CPU memory is required.\n",
    " \n",
    "\n",
    "## (c)\n",
    "*What are the expected advantages / disadvantages of a static graph strategy (TensorFlow) versus a dynamic graph stategy (Pytorch) ?*\n",
    "\n",
    "A static graph strategy is expected to be more efficient in terms of computational complexity where as a dynamic graph strategy is most probably more convenient to work with in the experiental / development phase.\n",
    "\n",
    "## (d)\n",
    "*What is the use of the @tf.function decorator for functions in TensorFlow 2.0 ?*\n",
    "\n",
    "It compiles a callable function into a TensorFlow graph.\n",
    "\n",
    "## (e)\n",
    "*What does the function tf.gradients(ys, xs) ? Describe precisely the output.*\n",
    "\n",
    "It calculates the gradients (local rate of change) of the tensors in `ys` with respect to the tensors in `xs`. It returns a list of tensors of length `len(xs)` where each value $x$ in each tensor is the sum $\\sum_{y\\in ys}\\frac{dy}{dx}$. \n",
    "\n",
    "\n",
    "## (f)\n",
    "*Explain the difference between the Keras sequential and functional API.*\n",
    "\n",
    " - Sequential API allows you to create models layer-by-layer by stacking them. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs\n",
    " \n",
    "```python\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(4, input_shape=(10,2)))\n",
    "seq_model.add(layers.Dense(4))\n",
    "seq_model.add(layers.Dense(1))\n",
    "seq_model.summary()\n",
    "```\n",
    " \n",
    " - Keras functional API provides a more flexibility as you can easily define models where layers connect to more than just the previous and next layers, and you can connect layers to any other layers. As a result, you can create complex network such as Residual Network.\n",
    "\n",
    " \n",
    "```python\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input1 = Input(shape=(10,2))\n",
    "lay1 = layers.Dense(4, input_shape=(10,2))(input1)\n",
    "lay2 = layers.Dense(4)(lay1)\n",
    "out1 = layers.Dense(1)(lay2)\n",
    "out2 = layers.Dense(1)(lay2)\n",
    "func_model = Model(inputs=input1, outputs=[out1, out2])\n",
    "func_model.summary()\n",
    "```\n",
    "\n",
    " - There is a third (in my opinion the most elegant) way to build a keras model, subclassing the Model class: in that case, you should define your layers in `__init__` and you should implement the model's forward pass in `call`. Notice that the `training` flag has to be manually set if the call method is executed during training.\n",
    " \n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.relu)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "    \n",
    "    def call(self, inputs, training=False)\n",
    "        x=self.dense1(inputs)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)\n",
    "   \n",
    "model = MyModel()\n",
    "```\n",
    "\n",
    "from: https://medium.com/analytics-vidhya/keras-model-sequential-api-vs-functional-api-fc1439a6fb10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
