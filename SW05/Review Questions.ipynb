{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)\n",
    "\n",
    "$$\n",
    "784 \\cdot 100 + 100 + 100 \\cdot 200 + 200 + 200 \\cdot 50 + 50 + 50 \\cdot 10 + 10 = 109'260 \\text{ Parameters}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 109,260\n",
      "Trainable params: 109,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, input_dim=784, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "When the dimensionality $d$ of the data increases, the number of samples required to cover the vector space with a constant density of $b$ samples per bin increases exponentially with $b^d$. So with a higher dimensionality of the input data, you need exponentially more data to reach an equal performance with for example a KNN model.  <br>\n",
    "Normally the information of interest lies on some lower dimensional manifold in the high dimensional space. Meaning that for example in order to detect a digit between 0 and 9 on a 28x28 px image, you do not need 28x28 features. Neural networks are assumed to learn those lower dimensional manifolds in the hidden layer (feature extraction) and then predict the correct quantity from the coordinates on the manifold of the given sample without suffering as much from the curse of dimensionality as one might expect. \n",
    "\n",
    "## (c)\n",
    "\n",
    "The successive layers should learn some hierarchy of features, starting at low level features and going to high level features.\n",
    "\n",
    "## (d)\n",
    "\n",
    "The backpropagation algorithm calculates every gradient only once. Hence we save time, power and memory.\n",
    "\n",
    "## (e)\n",
    "\n",
    "We need to remember the logits and the activations. We also could just remember one of the both and deduce the other one when calculating the gradients, but in order to save time it is more efficient to remember both seperately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
