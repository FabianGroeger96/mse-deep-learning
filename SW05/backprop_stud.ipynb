{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data (MNIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ### \n",
    "data_home = \"./data\"\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# in case you have trouble with the fetch_openml, uncomment the following two lines:\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def load_mnist(data_home):\n",
    "    \"\"\"\n",
    "    Loads the mnist dataset, prints the shape of the dataset and \n",
    "    returns the array with the images, the array with associated labels \n",
    "    and the shape of the images.     \n",
    "    Parameters: \n",
    "    data_home -- Absolute path to the DATA_HOME  \n",
    "    \n",
    "    Returns:\n",
    "    x -- array with images of shape (784,m) where m is the number of images\n",
    "    y -- array with associated labels with shape (1,m) where m is the number of images\n",
    "    shape -- (28,28)\n",
    "    \"\"\"\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, cache=True, data_home=data_home)\n",
    "    x, y = mnist['data'].T, np.array(mnist['target'], dtype='int').T\n",
    "    m = x.shape[1]\n",
    "    y = y.reshape(1,m)\n",
    "    print(\"Loaded MNIST original:\")\n",
    "    print(\"Image Data Shape\" , x.shape)\n",
    "    print(\"Label Data Shape\", y.shape)\n",
    "    return x,y,(28,28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and split into train/validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MNIST original:\n",
      "Image Data Shape (784, 70000)\n",
      "Label Data Shape (1, 70000)\n",
      "Train/Val Set: (x: (784, 60000), y: (1, 60000)) | Test Set: (x: (784, 10000), y: (1, 10000))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 10000\n",
    "\n",
    "x0,y0,shape = load_mnist(data_home)\n",
    "x, xtest, y, ytest = train_test_split(x0.T, y0.T, test_size=test_size, random_state=1)\n",
    "x, xtest, y, ytest = x.T, xtest.T, y.T, ytest.T\n",
    "\n",
    "print(\"Train/Val Set: (x: %s, y: %s) | Test Set: (x: %s, y: %s)\"%(x.shape, y.shape,xtest.shape,ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class\n",
    "\n",
    "Used to split into train and validation set, normalize and provide batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset(object):\n",
    "    \n",
    "    def __init__(self, x, y, val_size, random_state=0):\n",
    "        \"\"\"Splits dataset into train and val and normalize the train and val set (min/max normalization).\n",
    "        \n",
    "        Parameters:\n",
    "        x - input features, numpy array of shape(nx,m)\n",
    "        y - labels, numpy array of shape (1,m)\n",
    "        \"\"\"\n",
    "        xtrain, xval, ytrain, yval = train_test_split(x.T, y.T, test_size=val_size, random_state=random_state)\n",
    "        self.xtrain = xtrain.T\n",
    "        self.xval = xval.T\n",
    "        self.ytrain = ytrain.T\n",
    "        self.yval = yval.T\n",
    "        self._normalize()\n",
    "        \n",
    "        self.nx = self.xtrain.shape[0]\n",
    "        self.mtrain = self.xtrain.shape[1]\n",
    "        self.mval = self.xval.shape[1]\n",
    "        print(\"Training Data: x=%s | y=%s, Val Data: x=%s | y=%s\"%(str(self.xtrain.shape), str(self.ytrain.shape), \n",
    "                                                                    str(self.xval.shape), str(self.yval.shape)))\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        return 2*(x - self.xmin) / (self.xmax - self.xmin) - 1\n",
    "    \n",
    "    \n",
    "    def _normalize(self):\n",
    "        \"\"\"Applies min/max-normalization - min and max values computed from the training set.\n",
    "        Common min and max values for all features are used.\"\"\"\n",
    "        self.xmax, self.xmin = np.max(self.xtrain), np.min(self.xtrain)\n",
    "        self.xtrain = self.normalize(self.xtrain)\n",
    "        self.xval = self.normalize(self.xval)\n",
    "            \n",
    "    def prepare_batches(self, batchsize=None):\n",
    "        \"\"\"Initialize training set to provide batches of size batchsize. It reshuffles the samples \n",
    "        so that new batches are provided after calling this method. \n",
    "        \"\"\"\n",
    "        if not batchsize:\n",
    "            self.batchsize = self.mtrain\n",
    "        else:\n",
    "            self.batchsize = batchsize\n",
    "        self.mb = int(self.mtrain/self.batchsize)\n",
    "        self.indices = np.arange(self.mtrain)\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.counter = 0\n",
    "      \n",
    "    def number_of_batches(self):\n",
    "        \"\"\"Returns the number of batches. Provides a non-none result only after calling the `prepare_batches`-method.\n",
    "        \n",
    "        Returns:\n",
    "        mb -- number of (complete) batches.  \n",
    "        \"\"\"\n",
    "        return self.mb\n",
    "    \n",
    "    def next_batch(self):\n",
    "        \"\"\"Provides the next batch. If there are no more batches, the `prepare_batches` is called. \n",
    "        \n",
    "        Returns:\n",
    "        xbatch -- numpy array of shape (nx,batchsize)\n",
    "        ybatch -- numpy array of shape (1,batchsize)\n",
    "        \"\"\"\n",
    "        if (self.counter+1) >= self.mb:\n",
    "            self.prepare_batches(self.batchsize)\n",
    "        it = self.indices[self.counter*self.batchsize:(self.counter+1)*self.batchsize]\n",
    "        self.counter += 1\n",
    "        xbatch = self.xtrain[:,it].reshape(self.nx, self.batchsize)\n",
    "        ybatch = self.ytrain[:,it].reshape(1,self.batchsize)\n",
    "        return xbatch, ybatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: x=(784, 50000) | y=(1, 50000), Val Data: x=(784, 10000) | y=(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(x, y, val_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "#### Components\n",
    "\n",
    "In this section, we define the different components of the MLP model, including \n",
    "\n",
    "* _Activation function_ including suitable methods to compute values and derivatives: here, the sigmoid activation function will be used. \n",
    "* _Softmax activation function_: Here, you just need to provide the possibility to compute the softmax values. The derivatives are needed here but will be implemented in the softmax layer.  \n",
    "* _Initializer_: For initializing the weights and bias parameters. Here, normally distributed initial values will be provided.\n",
    "* _Cost Function_ used for training - including suitable methods for computing the values and derivatives: here, the cross-entropy cost will be used.\n",
    "* _Layer_ : The first core component to be implemented/completed by the students.\n",
    "* _MLP_ : The second core component to be implemented/completed by the students. Allows to configure an arbitrary number of layers into a sequential structure.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(object):\n",
    "    \"\"\"\n",
    "    Empty parent implementation of all activation functions. All child implementations should implement \n",
    "    the two methods defined below.\n",
    "    \"\"\"\n",
    "    def compute_value(self, z):\n",
    "        \"\"\"\n",
    "        Computes the value of the activation function element wise for input array z of arbitrary shape. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented in the child implementation.\")\n",
    "        \n",
    "    def compute_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function element wise for input array z of arbitrary shape. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"To be implemented in the child implementation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction(ActivationFunction):\n",
    "        \n",
    "    def compute_value(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "        \n",
    "    def compute_derivative(self, z):\n",
    "        s = self.compute_value(z)\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxActivationFunction(ActivationFunction):\n",
    "    \n",
    "    def compute_value(self, z):\n",
    "        expz = np.exp(z)\n",
    "        norm = np.sum(expz, axis=0)        \n",
    "        return expz / norm\n",
    "        \n",
    "    def compute_derivative(self, z):\n",
    "        raise NotImplementedError(\"Computation of the gradient implemented in the Softmax Layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormInitializer(object):\n",
    "    \n",
    "    def initialize_weights(self, size, mu=0.0, sigma=1.0):\n",
    "        return np.random.normal(size=size, loc=mu, scale=sigma)\n",
    "    \n",
    "    def initialize_bias(self, size, mu=0.0, sigma=1.0):\n",
    "        return np.random.normal(size=size, loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost(object):\n",
    "    \n",
    "    def compute_value(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the value of the cost function for given labels y and predicted probs. \n",
    "        \n",
    "        Arguments:\n",
    "        y -- labels, a numpy array of shape (1,m)\n",
    "        prob -- predicted probabilities for the different classes, a numpy array of shape (ny,m)\n",
    "        \n",
    "        Returns:\n",
    "        cost -- a scalar        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"\")\n",
    "        \n",
    "    def compute_derivative(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the cost function w.r.t. predicted probs for given labels y and predicted probs.\n",
    "\n",
    "        Parameters:\n",
    "        y -- labels, a numpy array of shape (1,m)\n",
    "        prob -- predicted probabilities for the different classes, a numpy array of shape (ny,m)\n",
    "        \n",
    "        Returns:\n",
    "        Gradient of cost with respect to the predicted probabilities, a numpy array of shape (ny,m)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Cost):\n",
    "\n",
    "    def compute_value(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the value of the cost function for given labels y and predicted probs.\n",
    "        \n",
    "        Arguments:\n",
    "        y -- labels, a numpy array of shape (1,m)\n",
    "        prob -- predicted probabilities for the different classes, a numpy array of shape (ny,m)\n",
    "        \n",
    "        Returns:\n",
    "        cost -- a scalar\n",
    "        \"\"\"\n",
    "        n, m = prob.shape\n",
    "        assert(np.max(y) <= n)\n",
    "        py = prob[y, np.arange(m)]\n",
    "        J = -np.sum(np.log(py)) / m    \n",
    "        return J\n",
    "    \n",
    "        \n",
    "    def compute_derivative(self, y, prob):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the cost function w.r.t. predicted probs for given labels y and predicted probs.\n",
    "\n",
    "        Parameters:\n",
    "        y -- labels, a numpy array of shape (1,m)\n",
    "        prob -- predicted probabilities for the different classes, a numpy array of shape (ny,m)\n",
    "        \n",
    "        Returns:\n",
    "        Gradient of cost with respect to the predicted probabilities, a numpy array of shape (ny,m)\n",
    "        \"\"\"\n",
    "        n, m = prob.shape\n",
    "        result = np.zeros((n, m),dtype=float)\n",
    "        result[y[0,:], np.arange(m)] = 1.0\n",
    "        result /= prob\n",
    "        return -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, layerid, nunits, nunits_prev, activ_func, initializer):\n",
    "        \"\"\"\n",
    "        Instantiates a fully connected layer for an MLP, with given number of input and output activations.\n",
    "        \n",
    "        Arguments:\n",
    "        layerid -- integer id for the layer.\n",
    "        nunits -- number of units in the given layer.\n",
    "        nunits_prev -- number of units in the previous layer (= number of input activations).\n",
    "        activ_func -- activation function to be used (with a `compute_value`- and `compute_derivative`-method).\n",
    "        initializer -- initializer for the weights and the biases.        \n",
    "        \"\"\"\n",
    "        self.layerid = layerid\n",
    "        self.nunits = nunits\n",
    "        self.nunits_prev = nunits_prev\n",
    "        self.initializer = initializer\n",
    "        self.activ_func = activ_func\n",
    "        \n",
    "        self.weights = None # shape (nunits,nunits_prev)\n",
    "        self.bias = None # shape (nunits,1)\n",
    "        self.logits = None # z-values, shape(nunits,m) - will be needed for the backprop part ...\n",
    "        self.activations = None # shape(nunits,m)\n",
    "        self.grad_logits = None # grad w.r.t. z, shape(nunits,m) - will be needed as basis for different gradients\n",
    "        \n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases. It uses Xavier normalisation (to be discussed in \"regularisation\").\n",
    "        \"\"\"\n",
    "        sigmaw = np.sqrt(2.0/(self.nunits+self.nunits_prev)) # suited for sigmoid activation function\n",
    "        sigmab = np.sqrt(1.0/self.nunits)\n",
    "        self.weights = self.initializer.initialize_weights(size=(self.nunits,self.nunits_prev), mu=0.0, sigma=sigmaw)\n",
    "        self.bias = self.initializer.initialize_bias(size=(self.nunits,1), mu=0.0, sigma=sigmab)\n",
    "        \n",
    "    \n",
    "    def propagate(self, activations_prev):\n",
    "        \"\"\"\n",
    "        Computes the activations of the layer given the activations of the previous layer.\n",
    "        Caches the computed logits (z-values) and activations since the values will be needed \n",
    "        when using backpropagation to compute the gradients w.r.t. weigths and biases.\n",
    "\n",
    "        Arguments:\n",
    "        activations_prev -- activations of the previous layer (or input layer). A numpy array of shape \n",
    "        (nunits_prev,m).\n",
    "        \n",
    "        Returns:\n",
    "        activations -- activations of this layer, a numpy array of shape (nunits,m)\n",
    "        \"\"\"\n",
    "        np.testing.assert_equal(activations_prev.shape[0],self.nunits_prev)\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        self.logits = self.weights.dot(activations_prev) + self.bias\n",
    "        self.activations = self.activ_func.compute_value(self.logits)\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "        return self.activations\n",
    "\n",
    "    \n",
    "    def backpropagate(self, grad_activations):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. to the input activations (activations of the previous \n",
    "        layer a^[l-1]) of the given layer. It also computes the gradient w.r.t. the logits (z-values) of \n",
    "        the given layer. This will be needed as the basis for computing the gradient of the cost w.r.t. \n",
    "        the weights and bias of the given layer. \n",
    "        \n",
    "        The method assumes that the forward propagation (`propagate`) has been invoked for the given mini-batch \n",
    "        so that consistent logit-values (self.logits) and activations (self.activations) are available.  \n",
    "        \n",
    "        Arguments:\n",
    "        grad_activations -- gradient of the cost w.r.t. to the output activations of the given layer (a^[l]). \n",
    "        A numpy array of shape (nunits_prev,m) \n",
    "        \n",
    "        Returns:\n",
    "        grad_activations_prev -- gradient of the cost w.r.t. to the input activations of the given layer.\n",
    "        \"\"\"\n",
    "        nsamples =  self.logits.shape[1]\n",
    "        np.testing.assert_equal(grad_activations.shape, (self.nunits, nsamples))\n",
    "\n",
    "        ### START YOUR CODE ###        \n",
    "        self.grad_logits = grad_activations * self.activ_func.compute_derivative(self.logits)\n",
    "        \n",
    "        # useful for testing the shapes - once self.grad_logits is computed check shape with:\n",
    "        np.testing.assert_equal(self.grad_logits.shape,(self.nunits,nsamples))\n",
    "        \n",
    "        grad_activations_prev = self.weights.T.dot(self.grad_logits)\n",
    "        \n",
    "        # useful for testing the shapes - once self.grad_activations_prev is computed check shape with:\n",
    "        np.testing.assert_equal(grad_activations_prev.shape,(self.nunits_prev,nsamples))        \n",
    "\n",
    "        \n",
    "        return grad_activations_prev\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    def gradient_weights(self, activations_prev):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. the weights of the given layer and for the given mini-batch.  \n",
    "\n",
    "        The method assumes that `backpropagate` (and `propagate`) has been invoked for the given mini-batch \n",
    "        so that consistent the gradient w.r.t. the logits is available.  \n",
    "\n",
    "        Arguments:\n",
    "        activations_prev -- activations of the previous layer (or input layer). A numpy array of shape \n",
    "        (nunits_prev,m).\n",
    "        \n",
    "        Returns:\n",
    "        grad_weights -- the gradient w.r.t. to the weights. A numpy array of shape (nunits, nunits_prev).\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        return self.grad_logits.dot(activations_prev.T)     \n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "\n",
    "    def gradient_bias(self):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. the bias of the given layer and for the given mini-batch.  \n",
    "                \n",
    "        The method assumes that `backpropagate` (and `propagate`) has been invoked for the given mini-batch \n",
    "        so that consistent the gradient w.r.t. the logits is available.  \n",
    "                \n",
    "        Returns:\n",
    "        grad_bias -- the gradient w.r.t. to the bias. A numpy array of shape (nunits, 1).\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###        \n",
    "\n",
    "        return self.grad_logits.sum(axis=-1, keepdims=True)         \n",
    "        \n",
    "        ### END YOUR CODE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the derivative of the softmax activation function values $a_i$ with respect to the logits $z_i$ where the layer has $n$ neurons, hence $0\\leq i \\leq n$, results in\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_j}{\\partial z_i} = \\left\\{\n",
    "\\begin{array}\n",
    "\\ \\sigma(z_i)(1-\\sigma(z_j)) \\\\\n",
    "-\\sigma(z_i)\\sigma(z_j) \n",
    "\\end{array}\n",
    "\\begin{array}\n",
    "  \\quad  \\quad  \\text{if } i = j\\\\\n",
    " \\quad \\text{if } i \\neq j\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "\\sigma(z_i)(\\delta_{ij}-\\sigma(z_j)) \\quad \\text{where } \\delta_{ij} \\text{ describes the Kronecker delta}\n",
    "$$\n",
    "\n",
    "In order to get rid of the Kronecker delta in the calculation and vectorize the formula we can reformulate \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial z} = \\text{diag}(\\sigma(z)) - \\sigma(z)\\sigma(z)^\\top \n",
    "$$\n",
    "\n",
    "This will result in a $n\\times n$ matrix.\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}\n",
    "\\ \\frac{\\partial \\mathcal a_1}{\\partial z_1} & \\ldots & \\frac{\\partial \\mathcal a_1}{\\partial z_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial \\mathcal a_n}{\\partial z_1} & \\ldots & \\frac{\\partial \\mathcal a_n}{\\partial z_n}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Now to calculate $\\frac{\\partial \\mathcal L}{\\partial z_i}$ we simply multiply by the gradient of the loss with respect to the softmax activations. As the change of the logit $z_i$ will influence the softmax activations for all $a_i$, we have to accumulate their gradients\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_i} = \\sum_{j=1}^n \\frac{\\partial \\mathcal L}{\\partial a_i} \\frac{\\partial a_j}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "In a vectorized format this reads as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal L}{\\partial z} = \\frac{\\partial \\mathcal L}{\\partial a}^\\top\\cdot \\frac{\\partial a}{\\partial z}= \\frac{\\partial \\mathcal L}{\\partial a}^\\top\\cdot \\left( \\text{diag}(\\sigma(z)) - \\sigma(z)\\sigma(z)^\\top \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "\n",
    "    def __init__(self, layerid, nunits, nunits_prev, initializer):\n",
    "        \"\"\"\n",
    "        Instantiates a Softmax layer with given number of input activations and normalised scores.\n",
    "        \n",
    "        Arguments:\n",
    "        layerid -- integer id for the layer.\n",
    "        nunits -- number of units in the given layer (=number of classes).\n",
    "        nunits_prev -- number of units in the previous layer (= number of input activations).\n",
    "        initializer -- initializer for the weights and the biases.        \n",
    "        \"\"\"\n",
    "        super().__init__(layerid, nunits, nunits_prev, SoftmaxActivationFunction(), initializer)\n",
    "        \n",
    "    def backpropagate(self, grad_activations):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cost w.r.t. to the input activations (activations of the previous \n",
    "        layer, activations_prev) of the softmax layer. It also computes the gradient w.r.t. the logits (z-values) of \n",
    "        the softmax layer. This will be needed as the basis for computing the gradient of the cost w.r.t. \n",
    "        the weights and bias of the layer. \n",
    "        \n",
    "        The method assumes that the forward propagation (`propagate`) has been invoked for the given mini-batch \n",
    "        so that consistent logit-values (z-logits) and activations (self.activations) are available.  \n",
    "        \n",
    "        Arguments:\n",
    "        grad_activations -- gradient of the cost w.r.t. to the output activations of the given layer. \n",
    "        A numpy array of shape (nunits_prev,m) \n",
    "        \n",
    "        Returns:\n",
    "        grad_activations_prev -- gradient of the cost w.r.t. to the input activations of the given layer.\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "        m = grad_activations.shape[1]\n",
    "        \n",
    "        # Calculate logits gradient knowing that CE is used as loss function\n",
    "        # delta = (grad_activations != 0).astype(int)\n",
    "        # self.grad_logits = (self.activations - delta) / m\n",
    "        \n",
    "        # Calculate logits gradient only with respect to softmax, not knowing that CE is used as loss function\n",
    "        def softmax_grad(x, dL_da):\n",
    "            s = x.reshape(-1,1)\n",
    "            aiaj = np.dot(s, s.T)\n",
    "            ai = np.diagflat(s)\n",
    "            da_dz = ai - aiaj\n",
    "            return dL_da.reshape(1, -1).dot(da_dz)\n",
    "        \n",
    "        # I did not know how to verctorize this formula\n",
    "        self.grad_logits = np.squeeze([softmax_grad(self.activations[:, i], grad_activations[:, i]) for i in range(m)]).T / m\n",
    "        \n",
    "        grad_activations_prev = self.weights.T.dot(self.grad_logits)\n",
    "        \n",
    "        return grad_activations_prev\n",
    "        \n",
    "        ### END YOUR CODE ###            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, units_per_layer, activ_func, initializer, softmax_as_last_layer=True):\n",
    "        \"\"\"\n",
    "        Instantiates a (fully connected) MLP with architecture specified by the list `units_per_layer` \n",
    "        which contains the number of units for the layers (including the input and the output layer).\n",
    "        It instantiates the Layer-objects and uses layerid=0 for the first hidden layer, layerid=1 \n",
    "        for the second, etc.\n",
    "        \n",
    "        Arguments:\n",
    "        units_per_layer -- list with the number of units per layers (including the input and the output layer)\n",
    "        activ_func -- activation function to be used in the different layers except possibly in the output \n",
    "        layer (in case softmax_as_last_layer=True)\n",
    "        initializer -- initializer for the weights and biases of all the layers.\n",
    "        softmax_as_last_layer -- flag to indicate whether the last layer should be a softmax layer.\n",
    "        \"\"\"\n",
    "        self.layers = [] # list of layers (instances of class Layer or Softmax or the like); ordered along the forward path.\n",
    "        self.number_of_hidden_layers = None # number of hidden layers (int) (excl. input / sofmax layer)\n",
    "        self.x = None # input data for one iteration (propagate and backpropagate) \n",
    "        \n",
    "        # Initialize self.layers and self.number_of_hidden_layers\n",
    "        \n",
    "        ### START YOUR CODE ###        \n",
    "\n",
    "        self.number_of_hidden_layers = len(units_per_layer) - 2\n",
    "        \n",
    "        for layer_id in range(1, self.number_of_hidden_layers + 1):\n",
    "            nunits = units_per_layer[layer_id]\n",
    "            nunits_prev = units_per_layer[layer_id - 1]\n",
    "            self.layers.append(\n",
    "                Layer(\n",
    "                    layer_id, nunits, nunits_prev,\n",
    "                    SigmoidActivationFunction(), NormInitializer()))\n",
    "        \n",
    "        self.layers.append(\n",
    "            Softmax(\n",
    "                self.number_of_hidden_layers, units_per_layer[-1],\n",
    "                units_per_layer[-2], NormInitializer()))\n",
    "        \n",
    "\n",
    "        ### END YOUR CODE ###        \n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Calls the `initialize`-method of the layers which are used to properly initialize the weights and biases. \n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.initialize()\n",
    "    \n",
    "    def propagate(self, x): \n",
    "        \"\"\"\n",
    "        Computes the output of the MLP for given input (by using the propagate-method). By executing this method \n",
    "        on a given input mini-batch, the activations and the logits of all the layers are computed \n",
    "        and cached (consistent with the mini-batch). \n",
    "        \n",
    "        Arguments:\n",
    "        x -- input of shape (n_0,m)\n",
    "        \n",
    "        Returns: \n",
    "        a -- activations of the last layer of shape (n_L,m)\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###                \n",
    "        \n",
    "        self.x = x\n",
    "        a = self.x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            a = layer.propagate(a)\n",
    "            \n",
    "        return a\n",
    "        \n",
    "        ### END YOUR CODE ###        \n",
    "\n",
    "    def backpropagate(self, grady):\n",
    "        \"\"\"\n",
    "        Executes backpropagation for the given MLP (after having executed the `propagate`-method).\n",
    "        It starts with passing in the gradient of the cost w.r.t. the activations of the last layer \n",
    "        (i.e. the input to the cost function) and ends up with the gradient of the cost w.r.t. the \n",
    "        input to the first layer (e.g. input x). Once this method has been run, the gradients w.r.t.\n",
    "        to the logits (z-values) are computed and cached. These will be used to update the weights \n",
    "        and biases in accordance with the gradient descent principle. \n",
    "        \n",
    "        Arguments:\n",
    "        grady -- gradient with respect to the output of the network, i.e. the activations of the last layer\n",
    "        that is input to the cost function. A numpy array of shape (n_L,m)\n",
    "        \n",
    "        Returns:\n",
    "        gradient with respect to the inputs to the network. A numpy array of shape (n_0,m)\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###        \n",
    "\n",
    "        grad = grady\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backpropagate(grad)\n",
    "        \n",
    "        ### END YOUR CODE ###        \n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of all the layers consistent with the gradient descent principle.\n",
    "        It assumes that the propagate and backpropagate methods have been executed.\n",
    "        \n",
    "        Arguments:\n",
    "        learning_rate -- learning rate to be used in the update rule.   \n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "\n",
    "        prev_activations = self.x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= learning_rate * layer.gradient_weights(prev_activations)\n",
    "            layer.bias -= learning_rate * layer.gradient_bias()\n",
    "            \n",
    "            prev_activations = layer.activations\n",
    "        \n",
    "\n",
    "        ### END YOUR CODE ###        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Implementation of Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x00 = np.array([0.2,0.1,-0.3, 0.2, 0.5,-1.0, 1.0,1.5,-1.0]).reshape(3,3)\n",
    "layersizes = [3,10,20,10,5]\n",
    "np.random.seed(1)\n",
    "mlp = MLP(layersizes, SigmoidActivationFunction(), NormInitializer(), True)\n",
    "for layerid in range(len(layersizes)-1):\n",
    "    layer = mlp.layers[layerid]\n",
    "    layer.weights = np.ones(shape=(layer.nunits,layer.nunits_prev), dtype='float')*0.1\n",
    "    layer.bias = np.zeros(shape=(layer.nunits,1), dtype='float')\n",
    "mlp.initialize()\n",
    "y00 = mlp.propagate(x00)\n",
    "np.testing.assert_equal(y00.shape, (5,3))\n",
    "y00_expected = np.array([\n",
    "    [0.18559891, 0.18553251, 0.18809197],\n",
    "    [0.28155687, 0.28171852, 0.27895721],\n",
    "    [0.05074394, 0.05071549, 0.05141478],\n",
    "    [0.13742613, 0.13757631, 0.13833757],\n",
    "    [0.34467414, 0.34445717, 0.34319847]])\n",
    "np.testing.assert_array_almost_equal(y00, y00_expected, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Performance of Forward Propagation\n",
    "\n",
    "Measure the runtimes for propagating all the MNIST training set (60'000 samples) with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time for batchsize     1 : 101.625 sec\n",
      "Exec time for batchsize    10 : 26.306 sec\n",
      "Exec time for batchsize   100 : 11.921 sec\n",
      "Exec time for batchsize  1000 : 11.917 sec\n",
      "Exec time for batchsize 10000 : 12.712 sec\n",
      "Exec time for batchsize 50000 : 12.348 sec\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "layersizes = [ds.nx,1000,800,700,600,400,200,100,100,10]\n",
    "mlp = MLP(layersizes, SigmoidActivationFunction(), NormInitializer(), True)\n",
    "mlp.initialize()\n",
    "batchsizes = [1,10,100,1000,10000,50000]\n",
    "runtimes = {}\n",
    "nsamples = ds.mtrain\n",
    "\n",
    "for batchsize in batchsizes:\n",
    "    start = timer()\n",
    "    ds.prepare_batches(batchsize)\n",
    "    for i in range(ds.number_of_batches()):\n",
    "        xx, yy = ds.next_batch()\n",
    "        yypred = mlp.propagate(xx)\n",
    "    end = timer()\n",
    "    runtime = end-start\n",
    "    print(\"Exec time for batchsize %5i : %6.3f sec\"%(batchsize,runtime))\n",
    "    runtimes[batchsize] = runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Explanation of the Observed Runtime vs Batch Size\n",
    "\n",
    "\n",
    "<font style=\"color: red;\">When using a batch size of 1, all the calculations for the forward passes for every sample in the training set will be executed sequentially. When using a larger batchsize, the calculations can be done in an optimized manner, depending on the hardware of the current machine. The runtime seems to stagnate at ~10s as at some point the hardware cannot handle larger batchsizes anymore and hence runs part of the batches sequentially again. </font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Implementation of the Gradient\n",
    "\n",
    "For checking the implementation, compute a numeric approximation of the gradient ('numeric gradient') by using the formula as explained in the class. Compare this with the analytic formulas ('analytic gradient') obtained when implementing backprop (or the derivatives of the cross entropy cost).\n",
    "\n",
    "Actually, do this checking for the cross entropy cost as well as for the MLP model.\n",
    "\n",
    "When choosing delta0~1.0e-8, we expect a difference of the numeric and the analytic gradient of <= 3.0e-7.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1854167203395605e-08\n",
      "0.0\n",
      "1.414096573171264e-07\n"
     ]
    }
   ],
   "source": [
    "# Check Gradient of Cross Entropy Cost\n",
    "\n",
    "ce = CrossEntropy()\n",
    "\n",
    "y = np.array([0,2]).reshape(1,2)\n",
    "probs0 = np.array([[0.7,0.1,0.2]]).reshape(1,3).T\n",
    "J0 = ce.compute_value(y,probs0)\n",
    "\n",
    "delta0 = 1.0e-8\n",
    "for i in range(3):\n",
    "    delta = np.zeros((3,1),dtype='float')\n",
    "    delta[i,0] = delta0\n",
    "    probs1 = probs0 + delta\n",
    "    J1 = ce.compute_value(y,probs1)\n",
    "    numeric = (J1-J0)/delta0\n",
    "    analytic = ce.compute_derivative(y,probs0)[i,0]\n",
    "    d = np.abs(numeric-analytic)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing layer with id 0\n",
      "Testing layer with id 1\n",
      "Testing layer with id 2\n",
      "Testing layer with id 3\n"
     ]
    }
   ],
   "source": [
    "# Check gradient of cost w.r.t. weights and biases of MLP\n",
    "# An output with a discrepancy is provided only if the difference between numeric and analytic gradient \n",
    "# exceeds the accuray of 3.0e-7. Definitely, the difference should not get much larger than this accuracy.\n",
    "\n",
    "accuracy = 3.0e-7\n",
    "\n",
    "layersizes = [100,200,300,100,10]\n",
    "mlp0 = MLP(layersizes, SigmoidActivationFunction(), NormInitializer(), True)\n",
    "mlp0.initialize()\n",
    "mlp1 = MLP(layersizes, SigmoidActivationFunction(), NormInitializer(), True)\n",
    "mlp1.initialize()\n",
    "for layerid in range(len(layersizes)-1):\n",
    "    mlp1.layers[layerid].weights = mlp0.layers[layerid].weights.copy()\n",
    "    mlp1.layers[layerid].bias = mlp0.layers[layerid].bias.copy()\n",
    "\n",
    "m = 2\n",
    "x = np.random.uniform(-0.5,0.5,size=(layersizes[0],m))\n",
    "y = np.random.randint(0,3,size=(1,m))\n",
    "probs0 = mlp0.propagate(x)\n",
    "gradJ = ce.compute_derivative(y,probs0)\n",
    "mlp0.backpropagate(gradJ)\n",
    "J0 = ce.compute_value(y,probs0)\n",
    "\n",
    "delta0 = 1.0e-8\n",
    "for layerid in range(len(layersizes)-1):\n",
    "    print(\"Testing layer with id %s\"%(layerid))\n",
    "    if layerid==0:\n",
    "        activations_prev = x\n",
    "    else:\n",
    "        activations_prev = mlp0.layers[layerid-1].activations\n",
    "    for i in range(layersizes[layerid+1]):\n",
    "        for j in range(layersizes[layerid]):\n",
    "            mlp1.layers[layerid].weights[i,j]+=delta0\n",
    "            probs1 = mlp1.propagate(x)\n",
    "            J1 = ce.compute_value(y,probs1)\n",
    "            numeric = (J1-J0)/delta0\n",
    "            analytic = mlp0.layers[layerid].gradient_weights(activations_prev)[i,j]\n",
    "            d = np.abs(numeric-analytic)\n",
    "            if d > accuracy:\n",
    "                print(\"Layer %i (%i,%i)\"%(layerid,i,j), d, numeric, analytic)\n",
    "            mlp1.layers[layerid].weights[i,j]-=delta0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Class\n",
    "\n",
    "For not littering the optimization loop with code to keep track of the learning results over the epochs we defined a suitable metrics object that keeps all the data (cost function, classification error vs epochs). It also provides utility methods for updating, printing values or plotting the learning curves.\n",
    "\n",
    "It is defined as python class the metrics object then needs to be instantiated from. It means that some small knowledge about object-oriented programming is needed here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Metrics():\n",
    "    \"\"\"\n",
    "    Allows to collect statistics (such as classification error or cost) that are of interest over the course of training\n",
    "    and for creating learning curves that are a useful tool for analyzing the quality of the learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cost):\n",
    "        \"\"\"\n",
    "        Constructor for a metrics object. \n",
    "        Initializes all the statistics to track in form of python lists.\n",
    "        \n",
    "        Parameters:\n",
    "        cost -- cost function to use (a python function)\n",
    "        \"\"\"\n",
    "        self.epochs = []\n",
    "        self.train_costs_last = []\n",
    "        self.val_costs_last = []\n",
    "        self.train_errors_last = []\n",
    "        self.val_errors_last = []\n",
    "\n",
    "        self.cost = cost\n",
    "        self.init_epoch()\n",
    "\n",
    "            \n",
    "    def init_epoch(self):\n",
    "        self.train_costs_epoch = []\n",
    "        self.val_costs_epoch = []\n",
    "        self.train_errors_epoch = []\n",
    "        self.val_errors_epoch = []\n",
    "        \n",
    "        \n",
    "    def update_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Computes the average of the metrics over the epoch and adds the result to the per epoch history\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- the epoch to add to the per epoch cache\n",
    "        \"\"\"\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_costs_last.append(self.train_costs_epoch[-1])\n",
    "        self.val_costs_last.append(self.val_costs_epoch[-1])\n",
    "        self.train_errors_last.append(self.train_errors_epoch[-1])\n",
    "        self.val_errors_last.append(self.val_errors_epoch[-1])\n",
    "        \n",
    "        self.init_epoch()\n",
    "    \n",
    "    def error_rate(self, y, probs):\n",
    "        m = y.shape[1]\n",
    "        ypred = np.argmax(probs, axis=0).reshape(1,m)\n",
    "        rate = np.sum(y != ypred) / m\n",
    "        return rate        \n",
    "        \n",
    "    def update_iteration(self, ypred_train, y_train, ypred_val, y_val):\n",
    "        \"\"\"\n",
    "        Allows to update the statistics to be tracked for a new epoch.\n",
    "        The cost is computed by using the function object passed to the constructor.\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- Epoch\n",
    "        ypred_train -- predicted values on the training samples, a numpy array of shape (1,m1)\n",
    "        y_train -- ground truth labels associated with the training samples, a numpy array of shape (1,m1)\n",
    "        ypred_val -- predicted values on the val samples, a numpy array of shape (1,m2)\n",
    "        y_val -- ground truth labels associated with the val samples, a numpy array of shape (1,m2)\n",
    "        \"\"\"\n",
    "        Jtrain = self.cost.compute_value(y_train, ypred_train)\n",
    "        Jval = self.cost.compute_value(y_val, ypred_val)\n",
    "        train_error = self.error_rate(y_train, ypred_train)\n",
    "        val_error = self.error_rate(y_val, ypred_val)\n",
    "\n",
    "        self.train_costs_epoch.append(Jtrain)\n",
    "        self.val_costs_epoch.append(Jval)\n",
    "        self.train_errors_epoch.append(train_error)\n",
    "        self.val_errors_epoch.append(val_error)\n",
    "        \n",
    "        \n",
    "    def print_latest_errors(self):\n",
    "        print (\"Train/val error after epoch %i: %f, %f\" %(self.epochs[-1], self.train_errors_last[-1], self.val_errors_last[-1]))\n",
    "\n",
    "    def print_latest_costs(self):\n",
    "        print (\"Train/val cost after epoch %i: %f, %f\" %(self.epochs[-1], self.train_costs_last[-1], self.val_costs_last[-1]))\n",
    "\n",
    "    def plot_cost_curves(self, xrange=None, yrange=None, logscale=True):\n",
    "        if logscale:\n",
    "            plt.semilogy(self.epochs, self.train_costs_last, \"b-\", label=\"train\")\n",
    "            plt.semilogy(self.epochs, self.val_costs_last, \"r-\", label=\"val\")\n",
    "        else:\n",
    "            plt.plot(self.epochs, self.train_costs_last, \"b-\", label=\"train\")\n",
    "            plt.plot(self.epochs, self.val_costs_last, \"r-\", label=\"val\")            \n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epochs')\n",
    "        if not xrange:\n",
    "            xrange=(0,self.epochs[-1])\n",
    "        if not yrange:\n",
    "            ymin = min(max(1e-5,np.min(self.train_costs_last)),max(1e-5,np.min(self.val_costs_last))) * 0.8\n",
    "            ymax = max(np.max(self.train_costs_last),np.max(self.val_costs_last)) * 1.2\n",
    "            yrange = (ymin,ymax)\n",
    "        plt.axis([xrange[0],xrange[1],yrange[0],yrange[1]])\n",
    "        plt.legend()\n",
    "        plt.show()        \n",
    "    \n",
    "    def plot_error_curves(self, xrange=None, yrange=None, logscale=True):\n",
    "        if logscale:\n",
    "            plt.semilogy(self.epochs, self.train_errors_last, \"b-\", label=\"train\")\n",
    "            plt.semilogy(self.epochs, self.val_errors_last, \"r-\", label=\"val\")\n",
    "        else:\n",
    "            plt.plot(self.epochs, self.train_errors_last, \"b-\", label=\"train\")\n",
    "            plt.plot(self.epochs, self.val_errors_last, \"r-\", label=\"val\")            \n",
    "        plt.ylabel('Errors')\n",
    "        plt.xlabel('Epochs')\n",
    "        if not xrange:\n",
    "            xrange=(0,self.epochs[-1])\n",
    "        if not yrange:\n",
    "            ymin = min(max(1e-5,np.min(self.train_errors_last)),max(1e-5,np.min(self.val_errors_last))) * 0.8\n",
    "            ymax = max(np.max(self.train_errors_last),np.max(self.val_errors_last)) * 1.2\n",
    "            yrange = (ymin,ymax)\n",
    "        plt.axis([xrange[0],xrange[1],yrange[0],yrange[1]])        \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(mlp, cost, ds, nepochs, learning_rate, batchsize=32, debug=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    mlp -- instance of MLP\n",
    "    cost -- instance of cost function to be used.\n",
    "    ds -- instance of Dataset class\n",
    "    nepochs -- number of epochs (sweeps through the training dataset in the optimization loop)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    batchsize -- batch size, defaults to 32\n",
    "    debug -- if true prints training and test error values after each epoch. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    metrics -- contain the information about the learning curves\n",
    "    \"\"\" \n",
    "    mlp.initialize()\n",
    "    metrics = Metrics(cost = cost)\n",
    "    ds.prepare_batches(batchsize)\n",
    "    \n",
    "    # compute and set the initial values for the metrics curves\n",
    "    ypred_train = mlp.propagate(ds.xtrain)    \n",
    "    ypred_val = mlp.propagate(ds.xval)    \n",
    "    metrics.update_iteration(ypred_train, ds.ytrain, ypred_val, ds.yval)\n",
    "    metrics.update_epoch(0)\n",
    "    \n",
    "    # Loop over the epochs    \n",
    "    for i in range(nepochs):\n",
    "                \n",
    "        batches = ds.prepare_batches(batchsize)\n",
    "        \n",
    "        ### START YOUR CODE ### \n",
    "        # Loop over the batches: \n",
    "        # propagate and then back-propagate the gradient of the cost\n",
    "        # update the parameters by using the gradient of the costs w.r.t. these parameters and the learning rate\n",
    "        # finally update the metrics object\n",
    "\n",
    "        \n",
    "        for j in range(ds.mb):\n",
    "            xbatch, ybatch = ds.next_batch()\n",
    "            ypred = mlp.propagate(xbatch)\n",
    "            \n",
    "            J0 = cost.compute_derivative(ybatch, ypred)\n",
    "            mlp.backpropagate(J0)\n",
    "            \n",
    "            mlp.update_params(learning_rate)\n",
    "            \n",
    "            \n",
    "        ypred_train = mlp.propagate(ds.xtrain)    \n",
    "        ypred_val = mlp.propagate(ds.xval)    \n",
    "        metrics.update_iteration(ypred_train, ds.ytrain, ypred_val, ds.yval)\n",
    "        metrics.update_epoch(i)\n",
    "        \n",
    "        ### END YOUR CODE ### \n",
    "            \n",
    "        if debug:\n",
    "            metrics.print_latest_errors()\n",
    "        \n",
    "    metrics.print_latest_costs()\n",
    "    metrics.print_latest_errors()\n",
    "\n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val error after epoch 0: 0.159660, 0.154700\n",
      "Train/val error after epoch 1: 0.128860, 0.125900\n",
      "Train/val error after epoch 2: 0.116280, 0.113800\n",
      "Train/val error after epoch 3: 0.108320, 0.107600\n",
      "Train/val error after epoch 4: 0.103300, 0.104200\n",
      "Train/val error after epoch 5: 0.099500, 0.100000\n",
      "Train/val error after epoch 6: 0.097640, 0.097500\n",
      "Train/val error after epoch 7: 0.097520, 0.097500\n",
      "Train/val error after epoch 8: 0.093740, 0.093100\n",
      "Train/val error after epoch 9: 0.091180, 0.090000\n",
      "Train/val error after epoch 10: 0.090300, 0.089500\n",
      "Train/val error after epoch 11: 0.089100, 0.089700\n",
      "Train/val error after epoch 12: 0.087860, 0.087000\n",
      "Train/val error after epoch 13: 0.086420, 0.086400\n",
      "Train/val error after epoch 14: 0.084340, 0.083700\n",
      "Train/val error after epoch 15: 0.083520, 0.083200\n",
      "Train/val error after epoch 16: 0.083100, 0.083000\n",
      "Train/val error after epoch 17: 0.082300, 0.082100\n",
      "Train/val error after epoch 18: 0.079920, 0.080200\n",
      "Train/val error after epoch 19: 0.079940, 0.081400\n",
      "Train/val error after epoch 20: 0.079060, 0.079800\n",
      "Train/val error after epoch 21: 0.079560, 0.078700\n",
      "Train/val error after epoch 22: 0.077160, 0.078300\n",
      "Train/val error after epoch 23: 0.077320, 0.078300\n",
      "Train/val error after epoch 24: 0.076080, 0.077300\n",
      "Train/val error after epoch 25: 0.075260, 0.075200\n",
      "Train/val error after epoch 26: 0.075100, 0.076100\n",
      "Train/val error after epoch 27: 0.073180, 0.074400\n",
      "Train/val error after epoch 28: 0.073900, 0.075800\n",
      "Train/val error after epoch 29: 0.073460, 0.074900\n",
      "Train/val error after epoch 30: 0.071760, 0.072900\n",
      "Train/val error after epoch 31: 0.070620, 0.071700\n",
      "Train/val error after epoch 32: 0.069820, 0.070700\n",
      "Train/val error after epoch 33: 0.069980, 0.071200\n",
      "Train/val error after epoch 34: 0.069660, 0.072100\n",
      "Train/val error after epoch 35: 0.068120, 0.070800\n",
      "Train/val error after epoch 36: 0.067140, 0.070000\n",
      "Train/val error after epoch 37: 0.066820, 0.070000\n",
      "Train/val error after epoch 38: 0.066160, 0.069400\n",
      "Train/val error after epoch 39: 0.065420, 0.068300\n",
      "Train/val error after epoch 40: 0.064940, 0.067800\n",
      "Train/val error after epoch 41: 0.063960, 0.067500\n",
      "Train/val error after epoch 42: 0.063440, 0.067700\n",
      "Train/val error after epoch 43: 0.063620, 0.066900\n",
      "Train/val error after epoch 44: 0.062260, 0.065500\n",
      "Train/val error after epoch 45: 0.061820, 0.065500\n",
      "Train/val error after epoch 46: 0.061240, 0.064800\n",
      "Train/val error after epoch 47: 0.059540, 0.062500\n",
      "Train/val error after epoch 48: 0.059900, 0.063600\n",
      "Train/val error after epoch 49: 0.059240, 0.063500\n",
      "Train/val error after epoch 50: 0.058580, 0.061900\n",
      "Train/val error after epoch 51: 0.057860, 0.061000\n",
      "Train/val error after epoch 52: 0.057320, 0.061000\n",
      "Train/val error after epoch 53: 0.056620, 0.060200\n",
      "Train/val error after epoch 54: 0.056440, 0.059700\n",
      "Train/val error after epoch 55: 0.055560, 0.058400\n",
      "Train/val error after epoch 56: 0.055220, 0.058300\n",
      "Train/val error after epoch 57: 0.054540, 0.059100\n",
      "Train/val error after epoch 58: 0.053800, 0.057500\n",
      "Train/val error after epoch 59: 0.053280, 0.056700\n",
      "Train/val error after epoch 60: 0.052760, 0.057100\n",
      "Train/val error after epoch 61: 0.052680, 0.057600\n",
      "Train/val error after epoch 62: 0.052360, 0.056800\n",
      "Train/val error after epoch 63: 0.050960, 0.056200\n",
      "Train/val error after epoch 64: 0.050300, 0.055400\n",
      "Train/val error after epoch 65: 0.050540, 0.054800\n",
      "Train/val error after epoch 66: 0.049320, 0.054100\n",
      "Train/val error after epoch 67: 0.049060, 0.054200\n",
      "Train/val error after epoch 68: 0.049280, 0.053400\n",
      "Train/val error after epoch 69: 0.048220, 0.053500\n",
      "Train/val error after epoch 70: 0.048180, 0.053400\n",
      "Train/val error after epoch 71: 0.047320, 0.053000\n",
      "Train/val error after epoch 72: 0.047400, 0.051800\n",
      "Train/val error after epoch 73: 0.047500, 0.053300\n",
      "Train/val error after epoch 74: 0.045700, 0.052600\n",
      "Train/val error after epoch 75: 0.045480, 0.051800\n",
      "Train/val error after epoch 76: 0.045220, 0.050700\n",
      "Train/val error after epoch 77: 0.044740, 0.050900\n",
      "Train/val error after epoch 78: 0.044260, 0.050100\n",
      "Train/val error after epoch 79: 0.043960, 0.050800\n",
      "Train/val error after epoch 80: 0.044180, 0.049600\n",
      "Train/val error after epoch 81: 0.043420, 0.049200\n",
      "Train/val error after epoch 82: 0.043120, 0.049700\n",
      "Train/val error after epoch 83: 0.043060, 0.049500\n",
      "Train/val error after epoch 84: 0.042340, 0.047700\n",
      "Train/val error after epoch 85: 0.042120, 0.047800\n",
      "Train/val error after epoch 86: 0.040940, 0.048200\n",
      "Train/val error after epoch 87: 0.040960, 0.047900\n",
      "Train/val error after epoch 88: 0.040720, 0.047800\n",
      "Train/val error after epoch 89: 0.040600, 0.046300\n",
      "Train/val error after epoch 90: 0.040180, 0.046100\n",
      "Train/val error after epoch 91: 0.039560, 0.046600\n",
      "Train/val error after epoch 92: 0.038900, 0.045300\n",
      "Train/val error after epoch 93: 0.039180, 0.045500\n",
      "Train/val error after epoch 94: 0.039260, 0.044300\n",
      "Train/val error after epoch 95: 0.038600, 0.045100\n",
      "Train/val error after epoch 96: 0.037880, 0.044100\n",
      "Train/val error after epoch 97: 0.037020, 0.043300\n",
      "Train/val error after epoch 98: 0.037880, 0.044300\n",
      "Train/val error after epoch 99: 0.036580, 0.044000\n",
      "Train/val error after epoch 100: 0.036860, 0.043200\n",
      "Train/val error after epoch 101: 0.036600, 0.042600\n",
      "Train/val error after epoch 102: 0.036100, 0.042700\n",
      "Train/val error after epoch 103: 0.035680, 0.042700\n",
      "Train/val error after epoch 104: 0.035460, 0.042400\n",
      "Train/val error after epoch 105: 0.034940, 0.041500\n",
      "Train/val error after epoch 106: 0.035060, 0.041900\n",
      "Train/val error after epoch 107: 0.034680, 0.042400\n",
      "Train/val error after epoch 108: 0.033920, 0.041600\n",
      "Train/val error after epoch 109: 0.034740, 0.042000\n",
      "Train/val error after epoch 110: 0.033380, 0.040600\n",
      "Train/val error after epoch 111: 0.033900, 0.041300\n",
      "Train/val error after epoch 112: 0.032860, 0.041300\n",
      "Train/val error after epoch 113: 0.033060, 0.040500\n",
      "Train/val error after epoch 114: 0.032540, 0.039700\n",
      "Train/val error after epoch 115: 0.032680, 0.039700\n",
      "Train/val error after epoch 116: 0.032260, 0.039400\n",
      "Train/val error after epoch 117: 0.031860, 0.040300\n",
      "Train/val error after epoch 118: 0.031880, 0.039900\n",
      "Train/val error after epoch 119: 0.031400, 0.038700\n",
      "Train/val error after epoch 120: 0.031440, 0.039200\n",
      "Train/val error after epoch 121: 0.030780, 0.038800\n",
      "Train/val error after epoch 122: 0.030960, 0.037900\n",
      "Train/val error after epoch 123: 0.030780, 0.038500\n",
      "Train/val error after epoch 124: 0.030380, 0.038600\n",
      "Train/val error after epoch 125: 0.030020, 0.038400\n",
      "Train/val error after epoch 126: 0.030040, 0.037700\n",
      "Train/val error after epoch 127: 0.029700, 0.037900\n",
      "Train/val error after epoch 128: 0.029200, 0.037800\n",
      "Train/val error after epoch 129: 0.029040, 0.037400\n",
      "Train/val error after epoch 130: 0.028940, 0.037500\n",
      "Train/val error after epoch 131: 0.028920, 0.037600\n",
      "Train/val error after epoch 132: 0.028820, 0.038700\n",
      "Train/val error after epoch 133: 0.028060, 0.036900\n",
      "Train/val error after epoch 134: 0.028320, 0.037600\n",
      "Train/val error after epoch 135: 0.027880, 0.036400\n",
      "Train/val error after epoch 136: 0.028040, 0.036600\n",
      "Train/val error after epoch 137: 0.027340, 0.035700\n",
      "Train/val error after epoch 138: 0.027440, 0.036200\n",
      "Train/val error after epoch 139: 0.027620, 0.036400\n",
      "Train/val error after epoch 140: 0.027360, 0.036200\n",
      "Train/val error after epoch 141: 0.027480, 0.036100\n",
      "Train/val error after epoch 142: 0.026680, 0.035500\n",
      "Train/val error after epoch 143: 0.026900, 0.035500\n",
      "Train/val error after epoch 144: 0.026600, 0.035000\n",
      "Train/val error after epoch 145: 0.026060, 0.035000\n",
      "Train/val error after epoch 146: 0.025440, 0.034500\n",
      "Train/val error after epoch 147: 0.026120, 0.035100\n",
      "Train/val error after epoch 148: 0.025720, 0.032900\n",
      "Train/val error after epoch 149: 0.025860, 0.034300\n",
      "Train/val error after epoch 150: 0.025200, 0.034900\n",
      "Train/val error after epoch 151: 0.024860, 0.033800\n",
      "Train/val error after epoch 152: 0.025020, 0.034200\n",
      "Train/val error after epoch 153: 0.024960, 0.034100\n",
      "Train/val error after epoch 154: 0.024460, 0.034400\n",
      "Train/val error after epoch 155: 0.024340, 0.034100\n",
      "Train/val error after epoch 156: 0.024380, 0.034200\n",
      "Train/val error after epoch 157: 0.023840, 0.033300\n",
      "Train/val error after epoch 158: 0.023820, 0.033500\n",
      "Train/val error after epoch 159: 0.023760, 0.032600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val error after epoch 160: 0.023900, 0.033800\n",
      "Train/val error after epoch 161: 0.022980, 0.032300\n",
      "Train/val error after epoch 162: 0.023260, 0.033300\n",
      "Train/val error after epoch 163: 0.023300, 0.033000\n",
      "Train/val error after epoch 164: 0.023040, 0.031900\n",
      "Train/val error after epoch 165: 0.022900, 0.031700\n",
      "Train/val error after epoch 166: 0.022480, 0.032200\n",
      "Train/val error after epoch 167: 0.022280, 0.032700\n",
      "Train/val error after epoch 168: 0.022580, 0.031500\n",
      "Train/val error after epoch 169: 0.022040, 0.032500\n",
      "Train/val error after epoch 170: 0.022140, 0.032100\n",
      "Train/val error after epoch 171: 0.021840, 0.032300\n",
      "Train/val error after epoch 172: 0.021980, 0.032100\n",
      "Train/val error after epoch 173: 0.021700, 0.032000\n",
      "Train/val error after epoch 174: 0.021620, 0.031800\n",
      "Train/val error after epoch 175: 0.021740, 0.031200\n",
      "Train/val error after epoch 176: 0.020880, 0.030300\n",
      "Train/val error after epoch 177: 0.020880, 0.031400\n",
      "Train/val error after epoch 178: 0.020860, 0.031900\n",
      "Train/val error after epoch 179: 0.020360, 0.031300\n",
      "Train/val error after epoch 180: 0.020720, 0.031100\n",
      "Train/val error after epoch 181: 0.020380, 0.030600\n",
      "Train/val error after epoch 182: 0.020800, 0.030000\n",
      "Train/val error after epoch 183: 0.020480, 0.030200\n",
      "Train/val error after epoch 184: 0.020240, 0.031300\n",
      "Train/val error after epoch 185: 0.020220, 0.030000\n",
      "Train/val error after epoch 186: 0.019780, 0.030500\n",
      "Train/val error after epoch 187: 0.020000, 0.029300\n",
      "Train/val error after epoch 188: 0.019400, 0.029100\n",
      "Train/val error after epoch 189: 0.019540, 0.030000\n",
      "Train/val error after epoch 190: 0.019840, 0.030400\n",
      "Train/val error after epoch 191: 0.019140, 0.029600\n",
      "Train/val error after epoch 192: 0.018960, 0.028500\n",
      "Train/val error after epoch 193: 0.018520, 0.029400\n",
      "Train/val error after epoch 194: 0.018780, 0.029400\n",
      "Train/val error after epoch 195: 0.018540, 0.029100\n",
      "Train/val error after epoch 196: 0.018660, 0.029900\n",
      "Train/val error after epoch 197: 0.018300, 0.029200\n",
      "Train/val error after epoch 198: 0.018740, 0.029700\n",
      "Train/val error after epoch 199: 0.018380, 0.028100\n",
      "Train/val error after epoch 200: 0.018260, 0.028700\n",
      "Train/val error after epoch 201: 0.017740, 0.029100\n",
      "Train/val error after epoch 202: 0.017940, 0.028900\n",
      "Train/val error after epoch 203: 0.017800, 0.028800\n",
      "Train/val error after epoch 204: 0.017640, 0.028800\n",
      "Train/val error after epoch 205: 0.017720, 0.029500\n",
      "Train/val error after epoch 206: 0.017580, 0.028400\n",
      "Train/val error after epoch 207: 0.017360, 0.028900\n",
      "Train/val error after epoch 208: 0.018000, 0.029100\n",
      "Train/val error after epoch 209: 0.017000, 0.027800\n",
      "Train/val error after epoch 210: 0.016880, 0.027900\n",
      "Train/val error after epoch 211: 0.016780, 0.027700\n",
      "Train/val error after epoch 212: 0.017340, 0.027900\n",
      "Train/val error after epoch 213: 0.017080, 0.028300\n",
      "Train/val error after epoch 214: 0.016680, 0.027600\n",
      "Train/val error after epoch 215: 0.016440, 0.026800\n",
      "Train/val error after epoch 216: 0.016240, 0.026700\n",
      "Train/val error after epoch 217: 0.016560, 0.027300\n",
      "Train/val error after epoch 218: 0.016320, 0.027700\n",
      "Train/val error after epoch 219: 0.016620, 0.027200\n",
      "Train/val error after epoch 220: 0.016260, 0.026800\n",
      "Train/val error after epoch 221: 0.015960, 0.028500\n",
      "Train/val error after epoch 222: 0.016020, 0.027100\n",
      "Train/val error after epoch 223: 0.015960, 0.026700\n",
      "Train/val error after epoch 224: 0.016020, 0.026700\n",
      "Train/val error after epoch 225: 0.015340, 0.027300\n",
      "Train/val error after epoch 226: 0.015240, 0.026400\n",
      "Train/val error after epoch 227: 0.015620, 0.026800\n",
      "Train/val error after epoch 228: 0.015860, 0.027100\n",
      "Train/val error after epoch 229: 0.015160, 0.026600\n",
      "Train/val error after epoch 230: 0.015640, 0.025800\n",
      "Train/val error after epoch 231: 0.014920, 0.026700\n",
      "Train/val error after epoch 232: 0.015140, 0.026200\n",
      "Train/val error after epoch 233: 0.014700, 0.025800\n",
      "Train/val error after epoch 234: 0.014760, 0.026600\n",
      "Train/val error after epoch 235: 0.014560, 0.025900\n",
      "Train/val error after epoch 236: 0.014540, 0.026800\n",
      "Train/val error after epoch 237: 0.014840, 0.025700\n",
      "Train/val error after epoch 238: 0.014380, 0.026300\n",
      "Train/val error after epoch 239: 0.014360, 0.025700\n",
      "Train/val error after epoch 240: 0.014220, 0.026000\n",
      "Train/val error after epoch 241: 0.014020, 0.025000\n",
      "Train/val error after epoch 242: 0.013940, 0.025400\n",
      "Train/val error after epoch 243: 0.014220, 0.025100\n",
      "Train/val error after epoch 244: 0.013780, 0.025700\n",
      "Train/val error after epoch 245: 0.013740, 0.026000\n",
      "Train/val error after epoch 246: 0.013540, 0.024900\n",
      "Train/val error after epoch 247: 0.013620, 0.024900\n",
      "Train/val error after epoch 248: 0.013560, 0.025000\n",
      "Train/val error after epoch 249: 0.013320, 0.025500\n",
      "Train/val error after epoch 250: 0.013640, 0.025500\n",
      "Train/val error after epoch 251: 0.013540, 0.025100\n",
      "Train/val error after epoch 252: 0.013400, 0.025900\n",
      "Train/val error after epoch 253: 0.013000, 0.024700\n",
      "Train/val error after epoch 254: 0.013320, 0.025400\n",
      "Train/val error after epoch 255: 0.013140, 0.025300\n",
      "Train/val error after epoch 256: 0.013140, 0.025600\n",
      "Train/val error after epoch 257: 0.012520, 0.025100\n",
      "Train/val error after epoch 258: 0.012980, 0.025100\n",
      "Train/val error after epoch 259: 0.012800, 0.025500\n",
      "Train/val error after epoch 260: 0.012580, 0.024900\n",
      "Train/val error after epoch 261: 0.012720, 0.025400\n",
      "Train/val error after epoch 262: 0.012220, 0.024700\n",
      "Train/val error after epoch 263: 0.012220, 0.024900\n",
      "Train/val error after epoch 264: 0.012400, 0.024200\n",
      "Train/val error after epoch 265: 0.012460, 0.025200\n",
      "Train/val error after epoch 266: 0.012100, 0.025000\n",
      "Train/val error after epoch 267: 0.011820, 0.024900\n",
      "Train/val error after epoch 268: 0.011880, 0.025400\n",
      "Train/val error after epoch 269: 0.012140, 0.024500\n",
      "Train/val error after epoch 270: 0.011920, 0.025600\n",
      "Train/val error after epoch 271: 0.012160, 0.025200\n",
      "Train/val error after epoch 272: 0.012260, 0.024900\n",
      "Train/val error after epoch 273: 0.012140, 0.025000\n",
      "Train/val error after epoch 274: 0.011600, 0.024600\n",
      "Train/val error after epoch 275: 0.011420, 0.024500\n",
      "Train/val error after epoch 276: 0.011580, 0.023800\n",
      "Train/val error after epoch 277: 0.011480, 0.024500\n",
      "Train/val error after epoch 278: 0.011120, 0.024500\n",
      "Train/val error after epoch 279: 0.011520, 0.023600\n",
      "Train/val error after epoch 280: 0.011060, 0.024300\n",
      "Train/val error after epoch 281: 0.011020, 0.024000\n",
      "Train/val error after epoch 282: 0.011200, 0.024100\n",
      "Train/val error after epoch 283: 0.011120, 0.024400\n",
      "Train/val error after epoch 284: 0.011100, 0.024300\n",
      "Train/val error after epoch 285: 0.010840, 0.024500\n",
      "Train/val error after epoch 286: 0.010920, 0.023900\n",
      "Train/val error after epoch 287: 0.010680, 0.024000\n",
      "Train/val error after epoch 288: 0.010880, 0.023800\n",
      "Train/val error after epoch 289: 0.010700, 0.023800\n",
      "Train/val error after epoch 290: 0.010500, 0.024100\n",
      "Train/val error after epoch 291: 0.010820, 0.023700\n",
      "Train/val error after epoch 292: 0.010380, 0.024000\n",
      "Train/val error after epoch 293: 0.010380, 0.023700\n",
      "Train/val error after epoch 294: 0.010240, 0.023600\n",
      "Train/val error after epoch 295: 0.010240, 0.023700\n",
      "Train/val error after epoch 296: 0.010280, 0.023700\n",
      "Train/val error after epoch 297: 0.010200, 0.023800\n",
      "Train/val error after epoch 298: 0.009980, 0.023200\n",
      "Train/val error after epoch 299: 0.010240, 0.023400\n",
      "Train/val error after epoch 300: 0.010260, 0.023100\n",
      "Train/val error after epoch 301: 0.009920, 0.022600\n",
      "Train/val error after epoch 302: 0.009800, 0.023800\n",
      "Train/val error after epoch 303: 0.009660, 0.023600\n",
      "Train/val error after epoch 304: 0.009860, 0.023600\n",
      "Train/val error after epoch 305: 0.009700, 0.023100\n",
      "Train/val error after epoch 306: 0.009660, 0.024000\n",
      "Train/val error after epoch 307: 0.009580, 0.023600\n",
      "Train/val error after epoch 308: 0.009620, 0.022800\n",
      "Train/val error after epoch 309: 0.009680, 0.024400\n",
      "Train/val error after epoch 310: 0.009340, 0.023200\n",
      "Train/val error after epoch 311: 0.009280, 0.022800\n",
      "Train/val error after epoch 312: 0.009380, 0.023700\n",
      "Train/val error after epoch 313: 0.009340, 0.022600\n",
      "Train/val error after epoch 314: 0.009240, 0.023200\n",
      "Train/val error after epoch 315: 0.009380, 0.022800\n",
      "Train/val error after epoch 316: 0.009160, 0.023100\n",
      "Train/val error after epoch 317: 0.009040, 0.023000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val error after epoch 318: 0.009180, 0.022600\n",
      "Train/val error after epoch 319: 0.008800, 0.023200\n",
      "Train/val error after epoch 320: 0.008880, 0.023100\n",
      "Train/val error after epoch 321: 0.008840, 0.022900\n",
      "Train/val error after epoch 322: 0.008780, 0.023600\n",
      "Train/val error after epoch 323: 0.009020, 0.022200\n",
      "Train/val error after epoch 324: 0.008900, 0.023300\n",
      "Train/val error after epoch 325: 0.008640, 0.022700\n",
      "Train/val error after epoch 326: 0.008540, 0.022900\n",
      "Train/val error after epoch 327: 0.008520, 0.023500\n",
      "Train/val error after epoch 328: 0.008660, 0.022600\n",
      "Train/val error after epoch 329: 0.008700, 0.023500\n",
      "Train/val error after epoch 330: 0.008420, 0.022400\n",
      "Train/val error after epoch 331: 0.008320, 0.022400\n",
      "Train/val error after epoch 332: 0.008600, 0.023000\n",
      "Train/val error after epoch 333: 0.008220, 0.022800\n",
      "Train/val error after epoch 334: 0.008120, 0.021900\n",
      "Train/val error after epoch 335: 0.008340, 0.022600\n",
      "Train/val error after epoch 336: 0.008080, 0.022400\n",
      "Train/val error after epoch 337: 0.008160, 0.022500\n",
      "Train/val error after epoch 338: 0.007900, 0.022500\n",
      "Train/val error after epoch 339: 0.008160, 0.022300\n",
      "Train/val error after epoch 340: 0.008080, 0.022400\n",
      "Train/val error after epoch 341: 0.008000, 0.021800\n",
      "Train/val error after epoch 342: 0.007940, 0.022100\n",
      "Train/val error after epoch 343: 0.007860, 0.022000\n",
      "Train/val error after epoch 344: 0.007580, 0.022400\n",
      "Train/val error after epoch 345: 0.007840, 0.022200\n",
      "Train/val error after epoch 346: 0.007640, 0.022000\n",
      "Train/val error after epoch 347: 0.007800, 0.022900\n",
      "Train/val error after epoch 348: 0.007660, 0.022300\n",
      "Train/val error after epoch 349: 0.007460, 0.022200\n",
      "Train/val error after epoch 350: 0.007660, 0.022000\n",
      "Train/val error after epoch 351: 0.007440, 0.021900\n",
      "Train/val error after epoch 352: 0.007340, 0.021600\n",
      "Train/val error after epoch 353: 0.007400, 0.022100\n",
      "Train/val error after epoch 354: 0.007080, 0.021900\n",
      "Train/val error after epoch 355: 0.007360, 0.021700\n",
      "Train/val error after epoch 356: 0.007220, 0.022000\n",
      "Train/val error after epoch 357: 0.007020, 0.021600\n",
      "Train/val error after epoch 358: 0.007140, 0.021600\n",
      "Train/val error after epoch 359: 0.006800, 0.022500\n",
      "Train/val error after epoch 360: 0.006900, 0.021300\n",
      "Train/val error after epoch 361: 0.007000, 0.021800\n",
      "Train/val error after epoch 362: 0.006800, 0.021400\n",
      "Train/val error after epoch 363: 0.007160, 0.021500\n",
      "Train/val error after epoch 364: 0.006820, 0.021500\n",
      "Train/val error after epoch 365: 0.006780, 0.022000\n",
      "Train/val error after epoch 366: 0.006720, 0.021800\n",
      "Train/val error after epoch 367: 0.006680, 0.021600\n",
      "Train/val error after epoch 368: 0.006720, 0.021700\n",
      "Train/val error after epoch 369: 0.007280, 0.022200\n",
      "Train/val error after epoch 370: 0.006640, 0.021500\n",
      "Train/val error after epoch 371: 0.006560, 0.022100\n",
      "Train/val error after epoch 372: 0.006440, 0.021700\n",
      "Train/val error after epoch 373: 0.006600, 0.021300\n",
      "Train/val error after epoch 374: 0.006400, 0.021600\n",
      "Train/val error after epoch 375: 0.006520, 0.022100\n",
      "Train/val error after epoch 376: 0.006760, 0.021900\n",
      "Train/val error after epoch 377: 0.006380, 0.022100\n",
      "Train/val error after epoch 378: 0.006420, 0.021900\n",
      "Train/val error after epoch 379: 0.006380, 0.021500\n",
      "Train/val error after epoch 380: 0.006200, 0.021500\n",
      "Train/val error after epoch 381: 0.006080, 0.020900\n",
      "Train/val error after epoch 382: 0.006000, 0.021700\n",
      "Train/val error after epoch 383: 0.006080, 0.021000\n",
      "Train/val error after epoch 384: 0.006140, 0.021200\n",
      "Train/val error after epoch 385: 0.006180, 0.021600\n",
      "Train/val error after epoch 386: 0.005860, 0.021300\n",
      "Train/val error after epoch 387: 0.005800, 0.021600\n",
      "Train/val error after epoch 388: 0.005960, 0.021000\n",
      "Train/val error after epoch 389: 0.005760, 0.021800\n",
      "Train/val error after epoch 390: 0.005640, 0.021200\n",
      "Train/val error after epoch 391: 0.005600, 0.021000\n",
      "Train/val error after epoch 392: 0.006020, 0.020900\n",
      "Train/val error after epoch 393: 0.005520, 0.021200\n",
      "Train/val error after epoch 394: 0.005680, 0.021200\n",
      "Train/val error after epoch 395: 0.005800, 0.020900\n",
      "Train/val error after epoch 396: 0.005460, 0.021200\n",
      "Train/val error after epoch 397: 0.005520, 0.021000\n",
      "Train/val error after epoch 398: 0.005560, 0.021000\n",
      "Train/val error after epoch 399: 0.005700, 0.021300\n",
      "Train/val error after epoch 400: 0.005440, 0.020900\n",
      "Train/val error after epoch 401: 0.005740, 0.021200\n",
      "Train/val error after epoch 402: 0.005380, 0.021000\n",
      "Train/val error after epoch 403: 0.005160, 0.020500\n",
      "Train/val error after epoch 404: 0.005360, 0.020900\n",
      "Train/val error after epoch 405: 0.005320, 0.020000\n",
      "Train/val error after epoch 406: 0.005360, 0.020900\n",
      "Train/val error after epoch 407: 0.005320, 0.020700\n",
      "Train/val error after epoch 408: 0.005180, 0.020900\n",
      "Train/val error after epoch 409: 0.005020, 0.020700\n",
      "Train/val error after epoch 410: 0.005020, 0.021400\n",
      "Train/val error after epoch 411: 0.005140, 0.021100\n",
      "Train/val error after epoch 412: 0.005120, 0.021000\n",
      "Train/val error after epoch 413: 0.005220, 0.021000\n",
      "Train/val error after epoch 414: 0.004940, 0.020400\n",
      "Train/val error after epoch 415: 0.005040, 0.020600\n",
      "Train/val error after epoch 416: 0.004980, 0.020600\n",
      "Train/val error after epoch 417: 0.004820, 0.020100\n",
      "Train/val error after epoch 418: 0.004760, 0.020600\n",
      "Train/val error after epoch 419: 0.004780, 0.020800\n",
      "Train/val error after epoch 420: 0.004640, 0.021000\n",
      "Train/val error after epoch 421: 0.005080, 0.021100\n",
      "Train/val error after epoch 422: 0.005060, 0.020700\n",
      "Train/val error after epoch 423: 0.004780, 0.020800\n",
      "Train/val error after epoch 424: 0.004740, 0.020900\n",
      "Train/val error after epoch 425: 0.004720, 0.020800\n",
      "Train/val error after epoch 426: 0.004740, 0.021200\n",
      "Train/val error after epoch 427: 0.004800, 0.020300\n",
      "Train/val error after epoch 428: 0.004520, 0.020700\n",
      "Train/val error after epoch 429: 0.004500, 0.020800\n",
      "Train/val error after epoch 430: 0.004500, 0.021300\n",
      "Train/val error after epoch 431: 0.004500, 0.021000\n",
      "Train/val error after epoch 432: 0.004360, 0.020300\n",
      "Train/val error after epoch 433: 0.004440, 0.020500\n",
      "Train/val error after epoch 434: 0.004480, 0.021000\n",
      "Train/val error after epoch 435: 0.004440, 0.020200\n",
      "Train/val error after epoch 436: 0.004380, 0.020300\n",
      "Train/val error after epoch 437: 0.004260, 0.020700\n",
      "Train/val error after epoch 438: 0.004280, 0.020400\n",
      "Train/val error after epoch 439: 0.004360, 0.020800\n",
      "Train/val error after epoch 440: 0.004320, 0.020800\n",
      "Train/val error after epoch 441: 0.004140, 0.020800\n",
      "Train/val error after epoch 442: 0.004840, 0.021300\n",
      "Train/val error after epoch 443: 0.004220, 0.020600\n",
      "Train/val error after epoch 444: 0.004060, 0.020400\n",
      "Train/val error after epoch 445: 0.004180, 0.020000\n",
      "Train/val error after epoch 446: 0.004160, 0.020700\n",
      "Train/val error after epoch 447: 0.004240, 0.020300\n",
      "Train/val error after epoch 448: 0.003920, 0.020500\n",
      "Train/val error after epoch 449: 0.004020, 0.020300\n",
      "Train/val error after epoch 450: 0.004080, 0.020500\n",
      "Train/val error after epoch 451: 0.003900, 0.020800\n",
      "Train/val error after epoch 452: 0.003900, 0.020300\n",
      "Train/val error after epoch 453: 0.003960, 0.020400\n",
      "Train/val error after epoch 454: 0.003920, 0.020100\n",
      "Train/val error after epoch 455: 0.003780, 0.020300\n",
      "Train/val error after epoch 456: 0.003980, 0.020300\n",
      "Train/val error after epoch 457: 0.003720, 0.020100\n",
      "Train/val error after epoch 458: 0.003820, 0.020000\n",
      "Train/val error after epoch 459: 0.003740, 0.020500\n",
      "Train/val error after epoch 460: 0.003440, 0.020100\n",
      "Train/val error after epoch 461: 0.003660, 0.020600\n",
      "Train/val error after epoch 462: 0.003600, 0.019600\n",
      "Train/val error after epoch 463: 0.003600, 0.019600\n",
      "Train/val error after epoch 464: 0.003860, 0.020600\n",
      "Train/val error after epoch 465: 0.003620, 0.020300\n",
      "Train/val error after epoch 466: 0.003760, 0.020400\n",
      "Train/val error after epoch 467: 0.003440, 0.020000\n",
      "Train/val error after epoch 468: 0.003480, 0.020000\n",
      "Train/val error after epoch 469: 0.003400, 0.020100\n",
      "Train/val error after epoch 470: 0.003520, 0.020300\n",
      "Train/val error after epoch 471: 0.003300, 0.020500\n",
      "Train/val error after epoch 472: 0.003380, 0.020000\n",
      "Train/val error after epoch 473: 0.003420, 0.019500\n",
      "Train/val error after epoch 474: 0.003440, 0.020300\n",
      "Train/val error after epoch 475: 0.003360, 0.020400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val error after epoch 476: 0.003340, 0.020500\n",
      "Train/val error after epoch 477: 0.003300, 0.019600\n",
      "Train/val error after epoch 478: 0.003140, 0.020100\n",
      "Train/val error after epoch 479: 0.003060, 0.020000\n",
      "Train/val error after epoch 480: 0.003420, 0.019800\n",
      "Train/val error after epoch 481: 0.003200, 0.020100\n",
      "Train/val error after epoch 482: 0.003260, 0.020000\n",
      "Train/val error after epoch 483: 0.003220, 0.019600\n",
      "Train/val error after epoch 484: 0.003300, 0.019600\n",
      "Train/val error after epoch 485: 0.003000, 0.020000\n",
      "Train/val error after epoch 486: 0.003100, 0.020000\n",
      "Train/val error after epoch 487: 0.003180, 0.020800\n",
      "Train/val error after epoch 488: 0.003040, 0.019700\n",
      "Train/val error after epoch 489: 0.003000, 0.020300\n",
      "Train/val error after epoch 490: 0.003040, 0.019500\n",
      "Train/val error after epoch 491: 0.003000, 0.020000\n",
      "Train/val error after epoch 492: 0.002980, 0.019600\n",
      "Train/val error after epoch 493: 0.003000, 0.019800\n",
      "Train/val error after epoch 494: 0.002880, 0.019800\n",
      "Train/val error after epoch 495: 0.002880, 0.020000\n",
      "Train/val error after epoch 496: 0.002740, 0.019800\n",
      "Train/val error after epoch 497: 0.002840, 0.020000\n",
      "Train/val error after epoch 498: 0.002740, 0.019700\n",
      "Train/val error after epoch 499: 0.002820, 0.019600\n",
      "Train/val cost after epoch 499: 0.019390, 0.068646\n",
      "Train/val error after epoch 499: 0.002820, 0.019600\n",
      "7549.3610245\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "### START YOUR CODE ### \n",
    "learning_rate = 0.01\n",
    "batchsize = 64\n",
    "nepochs = 500\n",
    "layersizes = (784, 780, 10)\n",
    "### END YOUR CODE ### \n",
    "\n",
    "mlp = MLP(layersizes, SigmoidActivationFunction(), NormInitializer(), True)\n",
    "ce = CrossEntropy()\n",
    "\n",
    "start = timer()\n",
    "metrics = optimize(mlp, ce, ds, nepochs, learning_rate, batchsize=batchsize, debug=True)\n",
    "end = timer()\n",
    "lapsetime = end-start\n",
    "print(lapsetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.plot_cost_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/val error after epoch 499: 0.002820, 0.019600\n"
     ]
    }
   ],
   "source": [
    "metrics.plot_error_curves(yrange=(0,0.1), logscale=False)\n",
    "metrics.print_latest_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results\n",
    "\n",
    "\n",
    "\n",
    "TODO: \n",
    "- Describe here your findings on the best hyper parameter settings for the two architectures and the resulting validation error.\n",
    "- Choose your best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the final model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:  0.01961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-482fd3ffdbda>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ### \n",
    "\n",
    "ypred = mlp.propagate(xtest)\n",
    "\n",
    "error = (np.argmax(ypred, axis=0) != ytest).sum() / ypred.size\n",
    "\n",
    "### END YOUR CODE ### \n",
    "print(\"Test error: \", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
