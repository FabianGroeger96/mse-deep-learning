{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation, Parameter Initialisation, Batchnorm, Optimisers\n",
    "\n",
    "Create and compare different models (as described below).\n",
    "\n",
    "Inspect the results by using tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bceaaeadb2b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-15202120268b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersizes = [50,50,50,10]\n",
    "batchsize = 32 \n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "outdir = os.path.join(os.getcwd(), tensorboard_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* Default parameter initialisation of Keras: What is the default?\n",
    "* Sigmoid activation (last layer always softmax)\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Now, create the baseline model. \n",
    "\n",
    "Possibly, add convenient naming to the layers so that you can more easily read the outputs in tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layersizes, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    activation -- string specifying the activation function for the hidden layers to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Use cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"baseline\"\n",
    "rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "# start tensorboard on command line with tensorboard -logs <path to outdir> \n",
    "\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "Provide here suitable plots and comments:\n",
    "\n",
    "* Learning curves: train / test accuracy and loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialisation\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* __Parameter Initialisation: Compare GlorotNormal, Random Normal (mean 0, stdev 1), Zero, HeNormal__\n",
    "* __Sigmoid Activation (last layer always softmax): Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Hence, for each of the 4 initializers train and test a model sigmoid and relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_param_init(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run with the different settings.\n",
    "Don't forget to configure the proper tensorboard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "Provide here suitable plots and comments:\n",
    "\n",
    "* Comparison of the different learning curves: \n",
    "    * train accuracy vs epochs for different models\n",
    "    * train loss vs epochs for different models\n",
    "    * test accuracy vs epochs for different models\n",
    "    * test loss vs epochs for different models\n",
    "    \n",
    "Interpret the result and report your findings: Is it consistent with what you have learned in the lecture?\n",
    "\n",
    "Are there ways (e.g. change in model) so that the effects of parameter initialisation become more clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation\n",
    "\n",
    "* No regularisation\n",
    "* __Batch Norm__: with / without \n",
    "* __Parameter Initialisation: Random Normal (0,1), GlorotNormal__\n",
    "* __Activation: Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Run with/without batchnorm in combination with sigmoid or relu (with GlorotNormal).<br>\n",
    "Run with/without batchnorm in combination with GlorotNormal or RandomNormal (with sigmoid).<br>\n",
    "Hence run 8 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_batchnorm(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run the different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "Provide here suitable plots and comments:\n",
    "\n",
    "* Comparison of the different learning curves: \n",
    "    * train accuracy vs epochs for different models\n",
    "    * train loss vs epochs for different models\n",
    "    * test accuracy vs epochs for different models\n",
    "    * test loss vs epochs for different models\n",
    "    \n",
    "* Inspect the histograms of the activations and compare them for the different models.\n",
    "\n",
    "* Find the max learning rate for the model with and without Batch Norm. \n",
    "\n",
    "Interpret the result and report your findings: Is it consistent with what you have learned in the lecture?\n",
    "\n",
    "Are there ways (e.g. change in model) so that the effects of batch norm become more clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "* No regularisation\n",
    "* No BatchNorm \n",
    "* Parameter Initialisation: GlorotNormal\n",
    "* Activation: ReLu\n",
    "* Optimizers: Compare \n",
    "    * SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop)\n",
    "    * RmsProp\n",
    "    * Momentum\n",
    "\n",
    "Create an according model and train it with the different optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "TODO: \n",
    "* Comparison of the different learning curves: \n",
    "    * train accuracy vs epochs for optimizers\n",
    "    * train loss vs epochs for optimizers\n",
    "    * test accuracy vs epochs for optimizers\n",
    "    * test loss vs epochs for optimizers\n",
    "    \n",
    "Interpret the result and report your findings: Is it consistent with what you have learned in the lecture?\n",
    "\n",
    "Are there ways (e.g. change in model) so that the effects of the different optimizers become more clear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
