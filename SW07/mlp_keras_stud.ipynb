{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation, Parameter Initialisation, Batchnorm, Optimisers\n",
    "\n",
    "Create and compare different models (as described below).\n",
    "\n",
    "Inspect the results by using tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28**2)\n",
    "x_test = x_test.reshape(-1, 28**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersizes = [50,50,50,10]\n",
    "batchsize = 32 \n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "outdir = os.path.join(os.getcwd(), tensorboard_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* Default parameter initialisation of Keras: What is the default?\n",
    "   - glorot_uniform\n",
    "   - src:https://keras.io/api/layers/core_layers/dense/\n",
    "* Sigmoid activation (last layer always softmax)\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Now, create the baseline model. \n",
    "\n",
    "Possibly, add convenient naming to the layers so that you can more easily read the outputs in tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layersizes, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    activation -- string specifying the activation function for the hidden layers to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(28**2,), name=\"input_layer\")\n",
    "    x = inputs\n",
    "    \n",
    "    hidden_layer_nr = 1\n",
    "    for size in layersizes[:-1]:\n",
    "        x = tf.keras.layers.Dense(units=size, activation=activation, name=f\"hidden_layer_{hidden_layer_nr}\")(x)\n",
    "        hidden_layer_nr += 1\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=layersizes[-1], activation=\"softmax\", name=\"final_layer\")(x)\n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Use cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "hidden_layer_1 (Dense)       (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "hidden_layer_2 (Dense)       (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "hidden_layer_3 (Dense)       (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "final_layer (Dense)          (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 44,860\n",
      "Trainable params: 44,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 2.2164 - accuracy: 0.1767 - val_loss: 1.0005 - val_accuracy: 0.6769\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 1s 913us/step - loss: 0.8316 - accuracy: 0.7430 - val_loss: 0.4753 - val_accuracy: 0.8692\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 1s 909us/step - loss: 0.4622 - accuracy: 0.8713 - val_loss: 0.3539 - val_accuracy: 0.8995\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 1s 886us/step - loss: 0.3437 - accuracy: 0.9034 - val_loss: 0.2770 - val_accuracy: 0.9232\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 1s 873us/step - loss: 0.2870 - accuracy: 0.9196 - val_loss: 0.2334 - val_accuracy: 0.9372\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 1s 877us/step - loss: 0.2314 - accuracy: 0.9359 - val_loss: 0.2190 - val_accuracy: 0.9385\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 1s 888us/step - loss: 0.2026 - accuracy: 0.9434 - val_loss: 0.1891 - val_accuracy: 0.9466\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 1s 879us/step - loss: 0.1723 - accuracy: 0.9508 - val_loss: 0.1844 - val_accuracy: 0.9465\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 1s 874us/step - loss: 0.1536 - accuracy: 0.9553 - val_loss: 0.1676 - val_accuracy: 0.9503\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 1s 859us/step - loss: 0.1484 - accuracy: 0.9568 - val_loss: 0.1495 - val_accuracy: 0.9559\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 1s 873us/step - loss: 0.1314 - accuracy: 0.9628 - val_loss: 0.1486 - val_accuracy: 0.9577\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 1s 871us/step - loss: 0.1229 - accuracy: 0.9643 - val_loss: 0.1424 - val_accuracy: 0.9582\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 1s 870us/step - loss: 0.1079 - accuracy: 0.9684 - val_loss: 0.1344 - val_accuracy: 0.9602\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 1s 887us/step - loss: 0.1053 - accuracy: 0.9706 - val_loss: 0.1428 - val_accuracy: 0.9588\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 1s 892us/step - loss: 0.0992 - accuracy: 0.9720 - val_loss: 0.1321 - val_accuracy: 0.9636\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 1s 887us/step - loss: 0.0900 - accuracy: 0.9742 - val_loss: 0.1286 - val_accuracy: 0.9635\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 1s 878us/step - loss: 0.0841 - accuracy: 0.9758 - val_loss: 0.1248 - val_accuracy: 0.9639\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 1s 922us/step - loss: 0.0800 - accuracy: 0.9770 - val_loss: 0.1279 - val_accuracy: 0.9642\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 1s 888us/step - loss: 0.0727 - accuracy: 0.9792 - val_loss: 0.1355 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 1s 863us/step - loss: 0.0739 - accuracy: 0.9791 - val_loss: 0.1335 - val_accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "run_name = \"baseline\"\n",
    "rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "# start tensorboard on command line with tensorboard -logs <path to outdir> \n",
    "\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "model = baseline_model(layersizes, \"sigmoid\")\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batchsize, epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/acc1.png\" style=\"float:left\">\n",
    "<img src=\"img/loss1.png\">\n",
    "\n",
    "We observe that the validation loss of the network is converging and the model already achieves an validation accuracy of  96.45%. With more epochs the model could gain another percent of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialisation\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* __Parameter Initialisation: Compare GlorotNormal, Random Normal (mean 0, stdev 1), Zero, HeNormal__\n",
    "* __Sigmoid Activation (last layer always softmax): Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Hence, for each of the 4 initializers train and test a model sigmoid and relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_param_init(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(28**2,), name=\"input_layer\")\n",
    "    x = inputs\n",
    "    \n",
    "    hidden_layer_nr = 1\n",
    "    for size in layersizes[:-1]:\n",
    "        x = tf.keras.layers.Dense(units=size, activation=activation,\n",
    "                                  name=f\"hidden_layer_{hidden_layer_nr}\",\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=initializer)(x)\n",
    "        hidden_layer_nr += 1\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=layersizes[-1], activation=\"softmax\", name=\"final_layer\")(x)\n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run with the different settings.\n",
    "Don't forget to configure the proper tensorboard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer 'glorot_normal'\n",
      "latest val_loss: 0.11762388795614243\n",
      "latest val_accuracy: 0.9652500152587891\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer 'random_normal'\n",
      "latest val_loss: 0.16827039420604706\n",
      "latest val_accuracy: 0.9516666531562805\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer 'random_stand_normal'\n",
      "latest val_loss: 0.24702580273151398\n",
      "latest val_accuracy: 0.9246666431427002\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer 'zero'\n",
      "latest val_loss: 1.485146164894104\n",
      "latest val_accuracy: 0.4117499887943268\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer 'he_normal'\n",
      "latest val_loss: 0.11227522790431976\n",
      "latest val_accuracy: 0.968416690826416\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer 'glorot_normal'\n",
      "latest val_loss: 0.13733188807964325\n",
      "latest val_accuracy: 0.9692500233650208\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer 'random_normal'\n",
      "latest val_loss: 0.14658606052398682\n",
      "latest val_accuracy: 0.9702500104904175\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer 'random_stand_normal'\n",
      "latest val_loss: 2.3019139766693115\n",
      "latest val_accuracy: 0.10599999874830246\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer 'zero'\n",
      "latest val_loss: 2.302727699279785\n",
      "latest val_accuracy: 0.10599999874830246\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer 'he_normal'\n",
      "latest val_loss: 0.1717206984758377\n",
      "latest val_accuracy: 0.9667500257492065\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "activations = [\"sigmoid\", \"relu\"]\n",
    "\n",
    "# initializers\n",
    "glorot_norm = tf.keras.initializers.GlorotNormal()\n",
    "glorot_norm.name = 'glorot_normal'\n",
    "\n",
    "rand_norm = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05) # default params\n",
    "rand_norm.name = 'random_normal'\n",
    "\n",
    "rand_stand_norm = tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0) # default params\n",
    "rand_stand_norm.name = 'random_stand_normal'\n",
    "\n",
    "zero = tf.keras.initializers.Zeros()\n",
    "zero.name = 'zero'\n",
    "\n",
    "he_normal = tf.keras.initializers.HeNormal()\n",
    "he_normal.name = 'he_normal'\n",
    "\n",
    "initializers = [glorot_norm, rand_norm, rand_stand_norm, zero, he_normal]\n",
    "\n",
    "for act, init in itertools.product(activations, initializers):\n",
    "    print(40*\"-\")\n",
    "    print(f\"Activation {act}\")\n",
    "    print(40*\"-\")\n",
    "    \n",
    "    run_name = f\"act-{act}-init-{init.name}\"\n",
    "    rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "    model = model_param_init(layersizes, init, act)\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    print(f\"working on initializer '{init.name}'\")\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batchsize, epochs=epochs,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard_callback], verbose=False)\n",
    "\n",
    "    print(f\"latest val_loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"latest val_accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    print(40*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sigmoid\n",
    "<img src=\"img/acc-sigmoid.png\" style=\"float:left; width:400px\">\n",
    "<img src=\"img/loss-sigmoid.png\" style=\"width:400px\">\n",
    "<img src=\"img/legend-sigmoid.png\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for `glorot_normal`, `he_normal` and `random_normal` the model converges relatively fast. The later shows the slowest convergence speed, which could be due to the fact that the sigmoid activation function has two saturating regions and the `random_normal` initialization does not account for the number of inputs. Hence it is more probable that the logit value can get very large (or very small) and result in a vanishing gradient.\n",
    "\n",
    "<br>\n",
    "<div style=\"color:red\">\n",
    "    \n",
    "Maybe the difference from `random_normal` to the other two could also come from the fact that it has a much smaller standard deviation as we learned today. Knowing that we could also argument that using `random_normal` initialization the initial neuron activations are more likely to lie within the linear region of the sigmoid function. This would imply that the optimization process would start with a weaker (almost linear) model, as a superposition of linear function is a linear function as well. Therefore the magnitude of the change that needs to be applied to the weights is larger and hence requires more time. This argumentation would also explain why the val_accuracy is almost 0 after the first epoch.\n",
    "\n",
    "This argumentation would be supported if using a `random_normal` initialization with mean 0 and std 1 would converge faster. \n",
    "    \n",
    "TBD: Results?\n",
    "</div>\n",
    "\n",
    "The slowest convergence speed is observed with the `zero` weight initializer. From the chain rule we know that the gradient of the activation of the previous layer are calculated as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{A}[l-1]}=\\frac{\\partial L}{\\partial \\mathbf{Z}^{[l]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{A}^{[l-1]}}=\\left(\\mathbf{W}^{[1]}\\right)^{T} \\cdot \\frac{\\partial L}{\\partial \\mathbf{Z}^{[l]}}\n",
    "$$\n",
    "\n",
    "and will be backpropagated through multiplication. Hence the gradient will vanish already at the last layer in the first training iteration and will only be able to be propagated back properly, once the weights in the last layer get pushed away from zero. This obviously takes some time. This \"pushing away from 0\" can also be seen as the network trying to predict equal probability for all classes, as the extracted features at the last hidden layer are completely random ($\\sigma(0)=0.5$).\n",
    "\n",
    "Most probably the weights of all layers are non zero at around epoch 6 which then finally enabled the network to learn something meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu\n",
    "<img src=\"img/acc-relu.png\" style=\"float:left\">\n",
    "<img src=\"img/loss-relu.png\">\n",
    "<img src=\"img/legend-relu.png\">\n",
    "\n",
    "We observe that for `glorot_normal`, `he_normal` and `random_normal` the model converges relatively fast. The later does not differ from the former two as it was observed with the sigmoid activation function. This could be due to the fact that the relu activation function has only a single saturating region.\n",
    "\n",
    "The `zero` weight initializer does not work at all with the relu activation function. This is due to the fact that $\\text{relu}(0)=0$. Hence \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}}=\\frac{\\partial L}{\\partial \\mathbf{Z}[l]} \\cdot \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{W}^{[l]}}=\\frac{\\partial L}{\\partial \\mathbf{Z}^{[l]}}\\left(\\mathbf{A}^{[l-1]}\\right)^{T}\n",
    "$$\n",
    "\n",
    "will always be zero and so will the weights. The only thing that the model can learn is to predict equal probability for each class. Thus the maximal accuracy that is can reach is ~10%.\n",
    "\n",
    "<img src=\"img/acc-relu-zero.png\" style=\"float:left; width:400px\">\n",
    "<img src=\"img/loss-relu-zero.png\" style='width:400px'>\n",
    "<img src=\"img/legend-relu-zero.png\" style='width:700px'>\n",
    "\n",
    "The loss fluctuates very slightly because the gradients of the weights at the last layer are the only ones that will never be zero. Thus those weights will be adapted slightly in every iteration which then results in those small fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see more differences between `glorot_normal`, `he_normal` we could try to create a network with very \"thin\" layers  (small number of neurons), such that the constant factor of the two techniques would gain more weight as it is the only difference between the two.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_{\\text{Glorot}}=&\\sqrt{\\frac{2}{n_{\\text {inputs }}+n_{\\text {outputs }}}} \\\\\n",
    "\\sigma_{\\text{He}}=\\sqrt{2} &\\sqrt{\\frac{2}{n_{\\text {inpurs }}+n_{\\text {outputs }}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The expectation would be that `glorot_normal` works better for sigmoid and `he_normal` better for relu activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation\n",
    "\n",
    "* No regularisation\n",
    "* __Batch Norm__: with / without \n",
    "* __Parameter Initialisation: Random Normal (0,1), GlorotNormal__\n",
    "* __Activation: Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Run with/without batchnorm in combination with sigmoid or relu (with GlorotNormal).<br>\n",
    "Run with/without batchnorm in combination with GlorotNormal or RandomNormal (with sigmoid).<br>\n",
    "Hence run 8 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_batchnorm(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(28**2,), name=\"input_layer\")\n",
    "    x = inputs\n",
    "    \n",
    "    hidden_layer_nr = 1\n",
    "    for size in layersizes[:-1]:\n",
    "        x = tf.keras.layers.Dense(units=size, activation=None,\n",
    "                                  name=f\"hidden_layer_{hidden_layer_nr}\",\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(activation)(x)\n",
    "        hidden_layer_nr += 1\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=layersizes[-1], activation=\"softmax\", name=\"final_layer\")(x)  \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run the different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm True\n",
      "latest val_loss: 0.08843450993299484\n",
      "latest val_accuracy: 0.9731666445732117\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm True\n",
      "latest val_loss: 0.09289657324552536\n",
      "latest val_accuracy: 0.971916675567627\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm False\n",
      "latest val_loss: 0.11580841988325119\n",
      "latest val_accuracy: 0.9664166569709778\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm False\n",
      "latest val_loss: 0.14083606004714966\n",
      "latest val_accuracy: 0.9628333449363708\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm True\n",
      "latest val_loss: 0.08475083112716675\n",
      "latest val_accuracy: 0.9760833382606506\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm True\n",
      "latest val_loss: 0.07491609454154968\n",
      "latest val_accuracy: 0.9797499775886536\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm False\n",
      "latest val_loss: 0.178066223859787\n",
      "latest val_accuracy: 0.9633333086967468\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm False\n",
      "latest val_loss: 0.1483737826347351\n",
      "latest val_accuracy: 0.968916654586792\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "for act in [\"sigmoid\", \"relu\"]:\n",
    "    print(40*\"-\")\n",
    "    print(f\"Activation {act}\")\n",
    "    print(40*\"-\")\n",
    "    for batch_norm, init in itertools.product([True, False], [\"glorot_normal\", \"random_normal\"]):\n",
    "        run_name = f\"batchnorm-{batch_norm}-act-{act}-init-{init}\"\n",
    "        rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "        if batch_norm:\n",
    "            model = model_batchnorm(layersizes, init, act)\n",
    "        else:\n",
    "            model = model_param_init(layersizes, init, act)\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        print(f\"working on initializer {init} with batch_norm {batch_norm}\")\n",
    "        history = model.fit(x_train, y_train,\n",
    "                            batch_size=batchsize, epochs=epochs,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[tensorboard_callback], verbose=False)\n",
    "        \n",
    "        print(f\"latest val_loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"latest val_accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        print(40*\"-\")\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "<img src=\"img/acc-sigmoid-batchnorm.png\" style=\"float:left\">\n",
    "<img src=\"img/loss-sigmoid-batchnorm.png\">\n",
    "<img src=\"img/legend-sigmoid-batchnorm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe for both initializers that the model using batchnorm converges *much* faster and also to a solution with a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu\n",
    "\n",
    "<img src=\"img/acc-relu-batchnorm.png\" style=\"float:left\">\n",
    "<img src=\"img/loss-relu-batchnorm.png\">\n",
    "<img src=\"img/legend-relu-batchnorm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe for both initializers that the model using batchnorm converges *a little* faster and also to a solution with a higher accuracy. But in general when using the relu activation function the convergence is again much faster in contrast to the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "Thanks to the tensorboard log, we can analyze the distribution of the kernel weights and compare them for different models\n",
    "<img src=\"img/batchnorm-activation-hists.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper row contains the histogram of the kernel of the second hidden layer for different models **not** using batchnorm and the lower row contains the histrograms of the same layer of the same models but with batchnorm.\n",
    "\n",
    "We can see that using batch norm results in much more consistent weights distribution over the different models (notice x-axis ranges). Another interesting observation is that when using a `standard_normal` initialization, the initial standard deviation of the weights is much smaller compared to the `glorot_normal` initialization.\n",
    "\n",
    "We can also analyse a plot where the time is on the x-axis and the weights distribution on the y-axis.\n",
    "\n",
    "<img src=\"img/batchnorm-activation-timedist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe the weights distribution of the kernel of the third hidden layer without (left) and with (right) batchnorm. The main differences are the scale and rate of change (especially in the beginning) of the distribution. Again it is nicely visible that the batchnorm layers help to make the weights more consistent and thus enable a smoother /stable learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also argument that using batchnorm layers helps to make the loss surface smoother. In the plot above we can see that the rate of change in the weights distribution is smoother using batchnorm which implies that the gradient is smoother which can only be the case if the loss surface is as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the effects of batch norm more clear we could make the model deeper and observe the weights histograms in the deep layers. There the effect should be better visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "* No regularisation\n",
    "* No BatchNorm \n",
    "* Parameter Initialisation: GlorotNormal\n",
    "* Activation: ReLu\n",
    "* Optimizers: Compare \n",
    "    * SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop)\n",
    "    * RmsProp\n",
    "    * Momentum\n",
    "\n",
    "Create an according model and train it with the different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Optimizer SGD\n",
      "----------------------------------------\n",
      "working on optimizer SGD\n",
      "latest val_loss: 0.13547353446483612\n",
      "latest val_accuracy: 0.9730833172798157\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Optimizer RMS prop\n",
      "----------------------------------------\n",
      "working on optimizer RMS prop\n",
      "latest val_loss: 2.116881847381592\n",
      "latest val_accuracy: 0.48091667890548706\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Optimizer Momentum\n",
      "----------------------------------------\n",
      "working on optimizer Momentum\n",
      "latest val_loss: 0.13555559515953064\n",
      "latest val_accuracy: 0.9712499976158142\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Optimizer Adam\n",
      "----------------------------------------\n",
      "working on optimizer Adam\n",
      "latest val_loss: 0.11547625064849854\n",
      "latest val_accuracy: 0.9659166932106018\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "import itertools\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for opt in [\"SGD\", \"RMS prop\", \"Momentum\", 'Adam']:\n",
    "    print(40*\"-\")\n",
    "    print(f\"Optimizer {opt}\")\n",
    "    print(40*\"-\")\n",
    "    run_name = f\"optimizer-{opt}\"\n",
    "    rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "    model = model_param_init(layersizes, \"glorot_normal\", \"relu\")\n",
    "\n",
    "    if opt == \"SGD\":\n",
    "        optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif opt == \"RMS prop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(lr=0.1*learning_rate)\n",
    "    elif opt == \"Momentum\":\n",
    "        optimizer = tf.keras.optimizers.SGD(lr=0.1*learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif opt == 'Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=0.001*learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    print(f\"working on optimizer {opt}\")\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batchsize, epochs=epochs,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard_callback], verbose=False)\n",
    "\n",
    "    print(f\"latest val_loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"latest val_accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    print(40*\"-\")\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<img src=\"img/acc-opt.png\" style=\"float:left\">\n",
    "<img src=\"img/loss-opt.png\">\n",
    "<img src=\"img/legend-opt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RMS prop` shows the fastes convergence and `SGD` with momentum the most stable and best end result. For those two to convergce in the first place, a much smaller global learning rate was required. The reason that `RMS prop` overfitts more could be due to the fact that it can get stuck faster in a local minima where as `SGD` with momentum can escape thanks to the momentum term. In theory the `Adam` optimizer should solve this discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my point of view the differences of the optimizers is already nicely shown in this example. But for the differences to become more extreme, we could increase the number of trainable parameters of the model (increase neurons in layers and / or increase network depth)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
