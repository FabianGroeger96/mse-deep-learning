{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation, Parameter Initialisation, Batchnorm, Optimisers\n",
    "\n",
    "Create and compare different models (as described below).\n",
    "\n",
    "Inspect the results by using tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28**2)\n",
    "x_test = x_test.reshape(-1, 28**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersizes = [50,50,50,10]\n",
    "batchsize = 32 \n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "outdir = os.path.join(os.getcwd(), tensorboard_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* Default parameter initialisation of Keras: What is the default?\n",
    "   - glorot_uniform\n",
    "   - src:https://keras.io/api/layers/core_layers/dense/\n",
    "* Sigmoid activation (last layer always softmax)\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Now, create the baseline model. \n",
    "\n",
    "Possibly, add convenient naming to the layers so that you can more easily read the outputs in tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layersizes, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    activation -- string specifying the activation function for the hidden layers to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(28**2,), name=\"input_layer\")\n",
    "    x = inputs\n",
    "    \n",
    "    hidden_layer_nr = 1\n",
    "    for size in layersizes[:-1]:\n",
    "        x = tf.keras.layers.Dense(units=size, activation=activation, name=f\"hidden_layer_{hidden_layer_nr}\")(x)\n",
    "        hidden_layer_nr += 1\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=layersizes[-1], activation=\"softmax\", name=\"final_layer\")(x)\n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Use cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "hidden_layer_1 (Dense)       (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "hidden_layer_2 (Dense)       (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "hidden_layer_3 (Dense)       (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "final_layer (Dense)          (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 44,860\n",
      "Trainable params: 44,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1.9815 - accuracy: 0.2849 - val_loss: 1.1668 - val_accuracy: 0.6532\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7560 - accuracy: 0.7755 - val_loss: 0.5032 - val_accuracy: 0.8670\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4487 - accuracy: 0.8784 - val_loss: 0.3799 - val_accuracy: 0.8974\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3476 - accuracy: 0.9037 - val_loss: 0.3005 - val_accuracy: 0.9189\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2819 - accuracy: 0.9220 - val_loss: 0.2472 - val_accuracy: 0.9335\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2350 - accuracy: 0.9347 - val_loss: 0.2107 - val_accuracy: 0.9425\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2021 - accuracy: 0.9426 - val_loss: 0.1907 - val_accuracy: 0.9477\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9503 - val_loss: 0.1763 - val_accuracy: 0.9499\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1575 - accuracy: 0.9557 - val_loss: 0.1617 - val_accuracy: 0.9532\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1424 - accuracy: 0.9590 - val_loss: 0.1574 - val_accuracy: 0.9571\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1295 - accuracy: 0.9631 - val_loss: 0.1458 - val_accuracy: 0.9596\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1186 - accuracy: 0.9657 - val_loss: 0.1344 - val_accuracy: 0.9615\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1089 - accuracy: 0.9683 - val_loss: 0.1324 - val_accuracy: 0.9607\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1017 - accuracy: 0.9703 - val_loss: 0.1277 - val_accuracy: 0.9625\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0945 - accuracy: 0.9730 - val_loss: 0.1259 - val_accuracy: 0.9631\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0891 - accuracy: 0.9740 - val_loss: 0.1245 - val_accuracy: 0.9643\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0824 - accuracy: 0.9761 - val_loss: 0.1329 - val_accuracy: 0.9601\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0779 - accuracy: 0.9772 - val_loss: 0.1276 - val_accuracy: 0.9629\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0739 - accuracy: 0.9778 - val_loss: 0.1219 - val_accuracy: 0.9634\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0694 - accuracy: 0.9796 - val_loss: 0.1215 - val_accuracy: 0.9646\n"
     ]
    }
   ],
   "source": [
    "run_name = \"baseline\"\n",
    "rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "# start tensorboard on command line with tensorboard -logs <path to outdir> \n",
    "\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "model = baseline_model(layersizes, \"sigmoid\")\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batchsize, epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"acc1.png\" style=\"float:left\">\n",
    "<img src=\"loss1.png\">\n",
    "\n",
    "We observe that the validation loss of the network is converging and the model already achieves an validation accuracy of  96.45%. With more epochs the model could gain another percent of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialisation\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* __Parameter Initialisation: Compare GlorotNormal, Random Normal (mean 0, stdev 1), Zero, HeNormal__\n",
    "* __Sigmoid Activation (last layer always softmax): Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Hence, for each of the 4 initializers train and test a model sigmoid and relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_param_init(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(28**2,), name=\"input_layer\")\n",
    "    x = inputs\n",
    "    \n",
    "    hidden_layer_nr = 1\n",
    "    for size in layersizes[:-1]:\n",
    "        x = tf.keras.layers.Dense(units=size, activation=activation,\n",
    "                                  name=f\"hidden_layer_{hidden_layer_nr}\",\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=initializer)(x)\n",
    "        hidden_layer_nr += 1\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=layersizes[-1], activation=\"softmax\", name=\"final_layer\")(x)\n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run with the different settings.\n",
    "Don't forget to configure the proper tensorboard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal\n",
      "latest val_loss: 0.11276379227638245\n",
      "latest val_accuracy: 0.9644166827201843\n",
      "----------------------------------------\n",
      "working on initializer random_normal\n",
      "latest val_loss: 0.15437713265419006\n",
      "latest val_accuracy: 0.9574999809265137\n",
      "----------------------------------------\n",
      "working on initializer zero\n",
      "latest val_loss: 1.4241622686386108\n",
      "latest val_accuracy: 0.45133334398269653\n",
      "----------------------------------------\n",
      "working on initializer he_normal\n",
      "latest val_loss: 0.10860568284988403\n",
      "latest val_accuracy: 0.968916654586792\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal\n",
      "latest val_loss: 0.15023940801620483\n",
      "latest val_accuracy: 0.9699166417121887\n",
      "----------------------------------------\n",
      "working on initializer random_normal\n",
      "latest val_loss: 0.1365629881620407\n",
      "latest val_accuracy: 0.9712499976158142\n",
      "----------------------------------------\n",
      "working on initializer zero\n",
      "latest val_loss: 2.3019659519195557\n",
      "latest val_accuracy: 0.10599999874830246\n",
      "----------------------------------------\n",
      "working on initializer he_normal\n",
      "latest val_loss: 0.13829973340034485\n",
      "latest val_accuracy: 0.9692500233650208\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for act in [\"sigmoid\", \"relu\"]:\n",
    "    print(40*\"-\")\n",
    "    print(f\"Activation {act}\")\n",
    "    print(40*\"-\")\n",
    "    for init in [\"glorot_normal\", \"random_normal\", \"zero\", \"he_normal\"]:\n",
    "        run_name = f\"act-{act}-init-{init}\"\n",
    "        rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "        model = model_param_init(layersizes, init, act)\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        print(f\"working on initializer {init}\")\n",
    "        history = model.fsit(x_train, y_train,\n",
    "                            batch_size=batchsize, epochs=epochs,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[tensorboard_callback], verbose=False)\n",
    "        \n",
    "        print(f\"latest val_loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"latest val_accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        print(40*\"-\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sigmoid\n",
    "<img src=\"acc-sigmoid.png\" style=\"float:left\">\n",
    "<img src=\"loss-sigmoid.png\">\n",
    "<img src=\"legend-sigmoid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for `glorot_normal`, `he_normal` and `random_normal` the model converges relatively fast. The later shows the slowest convergence speed, which could be due to the fact that the sigmoid activation function has two saturating regions and the `random_normal` initialization does not account for the number of inputs. Hence it is more probable that the logit value can get very large (or very small) and result in a vanishing gradient.\n",
    "\n",
    "The slowest convergence speed is observed with the `zero` weight initializer. From the chain rule we know that the gradient of the activation of the previous layer are calculated as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{A}[l-1]}=\\frac{\\partial L}{\\partial \\mathbf{Z}^{[l]}} \\cdot \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{A}^{[l-1]}}=\\left(\\mathbf{W}^{[1]}\\right)^{T} \\cdot \\frac{\\partial L}{\\partial \\mathbf{Z}^{[l]}}\n",
    "$$\n",
    "\n",
    "and will be backpropagated through multiplication. Hence the gradient will vanish already at the last layer in the first training iteration and will only be able to be propagated back properly, once the weights in the last layer get pushed away from zero. This obviously takes some time. This \"pushing away from 0\" can also be seen as the network trying to predict equal probability for all classes, as the extracted features at the last hidden layer are completely random ($\\sigma(0)=0.5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu\n",
    "<img src=\"acc-relu.png\" style=\"float:left\">\n",
    "<img src=\"loss-relu.png\">\n",
    "<img src=\"legend-relu.png\">\n",
    "\n",
    "We observe that for `glorot_normal`, `he_normal` and `random_normal` the model converges relatively fast. The later does not differ from the former two as it was observed with the sigmoid activation function. This could be due to the fact that the relu activation function has only a single saturating region.\n",
    "\n",
    "The `zero` weight initializer does not work at all with the relu activation function. This is due to the fact that $\\text{relu}(0)=0$. Hence \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}}=\\frac{\\partial L}{\\partial \\mathbf{Z}[l]} \\cdot \\frac{\\partial \\mathbf{Z}^{[l]}}{\\partial \\mathbf{W}^{[l]}}=\\frac{\\partial L}{\\partial \\mathbf{Z}^{[l]}}\\left(\\mathbf{A}^{[l-1]}\\right)^{T}\n",
    "$$\n",
    "\n",
    "will always be zero and so will the weights. The only thing that the model can learn is to predict equal probability for each class. Thus the maximal accuracy that is can reach is ~10%.\n",
    "\n",
    "<img src=\"acc-relu-zero.png\" style=\"float:left\">\n",
    "<img src=\"loss-relu-zero.png\">\n",
    "<img src=\"legend-relu-zero.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see more differences between `glorot_normal`, `he_normal` we could try to create a network with very \"thin\" layers  (small number of neurons), such that the constant factor of the two techniques would gain more weight as it is the only difference between the two.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_{\\text{Glorot}}=&\\sqrt{\\frac{2}{n_{\\text {inputs }}+n_{\\text {outputs }}}} \\\\\n",
    "\\sigma_{\\text{He}}=\\sqrt{2} &\\sqrt{\\frac{2}{n_{\\text {inpurs }}+n_{\\text {outputs }}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The expectation would be that `glorot_normal` works better for sigmoid and `he_normal` better for relu activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation\n",
    "\n",
    "* No regularisation\n",
    "* __Batch Norm__: with / without \n",
    "* __Parameter Initialisation: Random Normal (0,1), GlorotNormal__\n",
    "* __Activation: Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Run with/without batchnorm in combination with sigmoid or relu (with GlorotNormal).<br>\n",
    "Run with/without batchnorm in combination with GlorotNormal or RandomNormal (with sigmoid).<br>\n",
    "Hence run 8 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_batchnorm(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(28**2,), name=\"input_layer\")\n",
    "    x = inputs\n",
    "    \n",
    "    hidden_layer_nr = 1\n",
    "    for size in layersizes[:-1]:\n",
    "        x = tf.keras.layers.Dense(units=size, activation=None,\n",
    "                                  name=f\"hidden_layer_{hidden_layer_nr}\",\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  bias_initializer=initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(activation)(x)\n",
    "        hidden_layer_nr += 1\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units=layersizes[-1], activation=\"softmax\", name=\"final_layer\")(x)  \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run the different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Activation sigmoid\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm True\n",
      "latest val_loss: 0.09597315639257431\n",
      "latest val_accuracy: 0.9724166393280029\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm True\n",
      "latest val_loss: 0.08772739768028259\n",
      "latest val_accuracy: 0.9749166369438171\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm False\n",
      "latest val_loss: 0.11894385516643524\n",
      "latest val_accuracy: 0.965666651725769\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm False\n",
      "latest val_loss: 0.1706104576587677\n",
      "latest val_accuracy: 0.953083336353302\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Activation relu\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm True\n",
      "latest val_loss: 0.08182129263877869\n",
      "latest val_accuracy: 0.9775833487510681\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm True\n",
      "latest val_loss: 0.08689206838607788\n",
      "latest val_accuracy: 0.9760000109672546\n",
      "----------------------------------------\n",
      "working on initializer glorot_normal with batch_norm False\n",
      "latest val_loss: 0.14565885066986084\n",
      "latest val_accuracy: 0.9705833196640015\n",
      "----------------------------------------\n",
      "working on initializer random_normal with batch_norm False\n",
      "latest val_loss: 0.1373707354068756\n",
      "latest val_accuracy: 0.9698333144187927\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "import itertools\n",
    "\n",
    "for act in [\"sigmoid\", \"relu\"]:\n",
    "    print(40*\"-\")\n",
    "    print(f\"Activation {act}\")\n",
    "    print(40*\"-\")\n",
    "    for batch_norm, init in itertools.product([True, False], [\"glorot_normal\", \"random_normal\"]):\n",
    "        run_name = f\"batchnorm-{batch_norm}-act-{act}-init-{init}\"\n",
    "        rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "        if batch_norm:\n",
    "            model = model_batchnorm(layersizes, init, act)\n",
    "        else:\n",
    "            model = model_param_init(layersizes, init, act)\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        print(f\"working on initializer {init} with batch_norm {batch_norm}\")\n",
    "        history = model.fit(x_train, y_train,\n",
    "                            batch_size=batchsize, epochs=epochs,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[tensorboard_callback], verbose=False)\n",
    "        \n",
    "        print(f\"latest val_loss: {history.history['val_loss'][-1]}\")\n",
    "        print(f\"latest val_accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "        print(40*\"-\")\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "<img src=\"acc-sigmoid-batchnorm.png\" style=\"float:left\">\n",
    "<img src=\"loss-sigmoid-batchnorm.png\">\n",
    "<img src=\"legend-sigmoid-batchnorm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe for both initializers that the model using batchnorm converges *much* faster and also to a solution with a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu\n",
    "\n",
    "<img src=\"acc-relu-batchnorm.png\" style=\"float:left\">\n",
    "<img src=\"loss-relu-batchnorm.png\">\n",
    "<img src=\"legend-relu-batchnorm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again observe for both initializers that the model using batchnorm converges *a little* faster and also to a solution with a higher accuracy. But in general when using the relu activation function the convergence is again much faster in contrast to the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "Thanks to the tensorboard log, we can analyze the distribution of the kernel weights and compare them for different models\n",
    "<img src=\"batchnorm-activation-hists.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper row contains the histogram of the kernel of the second hidden layer for different models **not** using batchnorm and the lower row contains the histrograms of the same layer of the same models but with batchnorm.\n",
    "\n",
    "We can see that using batch norm results in much more consistent weights distribution over the different models (notice x-axis ranges). Another interesting observation is that when using a `standard_normal` initialization, the initial standard deviation of the weights is much smaller compared to the `glorot_normal` initialization.\n",
    "\n",
    "We can also analyse a plot where the time is on the x-axis and the weights distribution on the y-axis.\n",
    "\n",
    "<img src=\"batchnorm-activation-timedist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe the weights distribution of the kernel of the third hidden layer without (left) and with (right) batchnorm. The main differences are the scale and rate of change (especially in the beginning) of the distribution. Again it is nicely visible that the batchnorm layers help to make the weights more consistent and thus enable a smoother /stable learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the effects of batch norm more clear we could make the model deeper and observe the weights histograms in the deep layers. There the effect should be better visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "* No regularisation\n",
    "* No BatchNorm \n",
    "* Parameter Initialisation: GlorotNormal\n",
    "* Activation: ReLu\n",
    "* Optimizers: Compare \n",
    "    * SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop)\n",
    "    * RmsProp\n",
    "    * Momentum\n",
    "\n",
    "Create an according model and train it with the different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Optimizer SGD\n",
      "----------------------------------------\n",
      "working on optimizer SGD\n",
      "latest val_loss: 0.2698284387588501\n",
      "latest val_accuracy: 0.9228333234786987\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Optimizer RMS prop\n",
      "----------------------------------------\n",
      "working on optimizer RMS prop\n",
      "latest val_loss: 0.24551866948604584\n",
      "latest val_accuracy: 0.9683333039283752\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Optimizer Momentum\n",
      "----------------------------------------\n",
      "working on optimizer Momentum\n",
      "latest val_loss: 0.11718112230300903\n",
      "latest val_accuracy: 0.968416690826416\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "import itertools\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for opt in [\"SGD\", \"RMS prop\", \"Momentum\"]:\n",
    "    print(40*\"-\")\n",
    "    print(f\"Optimizer {opt}\")\n",
    "    print(40*\"-\")\n",
    "    run_name = f\"optimizer-{opt}\"\n",
    "    rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "\n",
    "    model = model_param_init(layersizes, \"glorot_normal\", \"relu\")\n",
    "\n",
    "    if opt == \"SGD\":\n",
    "        optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif opt == \"RMS prop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    elif opt == \"Momentum\":\n",
    "        optimizer = tf.keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    print(f\"working on optimizer {opt}\")\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batchsize, epochs=epochs,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard_callback], verbose=False)\n",
    "\n",
    "    print(f\"latest val_loss: {history.history['val_loss'][-1]}\")\n",
    "    print(f\"latest val_accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "    print(40*\"-\")\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<img src=\"acc-opt.png\" style=\"float:left\">\n",
    "<img src=\"loss-opt.png\">\n",
    "<img src=\"legend-opt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RMS prop` shows the fastes convergence and `SGD` with momentum the most stable and best end result. For those two to convergce in the first place, a much smaller global learning rate was required. The reason that `RMS prop` overfitts more could be due to the fact that it can get stuck faster in a local minima where as `SGD` with momentum can escape thanks to the momentum term. In theory the `Adam` optimizer should solve this discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my point of view the differences of the optimizers is already nicely shown in this example. But for the differences to become more extreme, we could increase the number of trainable parameters of the model (increase neurons in layers and / or increase network depth)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
