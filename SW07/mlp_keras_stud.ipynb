{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation, Parameter Initialisation, Batchnorm, Optimisers\n",
    "\n",
    "Create and compare different models (as described below).\n",
    "\n",
    "Inspect the results by using tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersizes = [50,50,50,10]\n",
    "batchsize = 32 \n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "tensorboard_folder = \"tb_logs_keras\"\n",
    "outdir = os.path.join(os.getcwd(), tensorboard_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* Default parameter initialisation of Keras: What is the default?\n",
    "* Sigmoid activation (last layer always softmax)\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Now, create the baseline model. \n",
    "\n",
    "Possibly, add convenient naming to the layers so that you can more easily read the outputs in tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layersizes, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    activation -- string specifying the activation function for the hidden layers to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Use cross entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 1.7995 - accuracy: 0.3763 - val_loss: 0.9199 - val_accuracy: 0.7487\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.6064 - accuracy: 0.8318 - val_loss: 0.4348 - val_accuracy: 0.8798\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 0.3615 - accuracy: 0.9006 - val_loss: 0.3003 - val_accuracy: 0.9162\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 127us/sample - loss: 0.2709 - accuracy: 0.9248 - val_loss: 0.2350 - val_accuracy: 0.9352\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.2194 - accuracy: 0.9377 - val_loss: 0.2122 - val_accuracy: 0.9414\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1855 - accuracy: 0.9462 - val_loss: 0.1776 - val_accuracy: 0.9486\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.1599 - accuracy: 0.9541 - val_loss: 0.1751 - val_accuracy: 0.9507\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.1407 - accuracy: 0.9585 - val_loss: 0.1537 - val_accuracy: 0.9550\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.1270 - accuracy: 0.9629 - val_loss: 0.1401 - val_accuracy: 0.9597\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.1154 - accuracy: 0.9664 - val_loss: 0.1343 - val_accuracy: 0.9604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb242fae950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name = \"baseline\"\n",
    "rundir = os.path.join(outdir, run_name)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=rundir, histogram_freq=1, profile_batch=0)\n",
    "# start tensorboard on command line with tensorboard -logs <path to outdir> \n",
    "\n",
    "\n",
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "Provide here suitable plots and comments:\n",
    "\n",
    "* Learning curves: train / test accuracy and loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialisation\n",
    "\n",
    "* No regularisation\n",
    "* No Batch Norm\n",
    "* __Parameter Initialisation: Compare GlorotNormal, Random Normal (mean 0, stdev 1), Zero, HeNormal__\n",
    "* __Sigmoid Activation (last layer always softmax): Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Hence, for each of the 4 initializers train and test a model sigmoid and relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_param_init(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run with the different settings.\n",
    "Don't forget to configure the proper tensorboard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s 177us/sample - loss: 0.7767 - accuracy: 0.7752 - val_loss: 0.3102 - val_accuracy: 0.9117\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 176us/sample - loss: 0.2555 - accuracy: 0.9260 - val_loss: 0.2086 - val_accuracy: 0.9386\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.1866 - accuracy: 0.9450 - val_loss: 0.1688 - val_accuracy: 0.9491\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.1528 - accuracy: 0.9552 - val_loss: 0.1464 - val_accuracy: 0.9581\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.1316 - accuracy: 0.9615 - val_loss: 0.1370 - val_accuracy: 0.9616\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.1178 - accuracy: 0.9654 - val_loss: 0.1307 - val_accuracy: 0.9617\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.1078 - accuracy: 0.9690 - val_loss: 0.1253 - val_accuracy: 0.9639\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0992 - accuracy: 0.9711 - val_loss: 0.1290 - val_accuracy: 0.9635\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0928 - accuracy: 0.9732 - val_loss: 0.1217 - val_accuracy: 0.9643\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0870 - accuracy: 0.9744 - val_loss: 0.1176 - val_accuracy: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f88faff17d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "Provide here suitable plots and comments:\n",
    "\n",
    "* Comparison of the different learning curves: \n",
    "    * train accuracy vs epochs for different models\n",
    "    * train loss vs epochs for different models\n",
    "    * test accuracy vs epochs for different models\n",
    "    * test loss vs epochs for different models\n",
    "    \n",
    "Interpret the result and report your findings: Is it consistent with what you have learned in the lecture?\n",
    "\n",
    "Are there ways (e.g. change in model) so that the effects of parameter initialisation become more clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation\n",
    "\n",
    "* No regularisation\n",
    "* __Batch Norm__: with / without \n",
    "* __Parameter Initialisation: Random Normal (0,1), GlorotNormal__\n",
    "* __Activation: Compare Sigmoid, ReLu__\n",
    "* SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop).\n",
    "\n",
    "Run with/without batchnorm in combination with sigmoid or relu (with GlorotNormal).<br>\n",
    "Run with/without batchnorm in combination with GlorotNormal or RandomNormal (with sigmoid).<br>\n",
    "Hence run 8 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_batchnorm(layersizes, initializer, activation):\n",
    "    \"\"\"\n",
    "    Provides an MLP model (using Sequential) with given layersizes. The last layer is a softmax layer.\n",
    "    As activation function use sigmoid.\n",
    "        \n",
    "    Arguments:\n",
    "    layersizes -- list of integers with the number of hidden units per layer. The last element is for MNIST 10.\n",
    "    initializer -- weight initializer\n",
    "    activation -- string specifying the activation function to be used.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### STOP YOUR CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model\n",
    "\n",
    "Run the different variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s 177us/sample - loss: 0.7767 - accuracy: 0.7752 - val_loss: 0.3102 - val_accuracy: 0.9117\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 176us/sample - loss: 0.2555 - accuracy: 0.9260 - val_loss: 0.2086 - val_accuracy: 0.9386\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.1866 - accuracy: 0.9450 - val_loss: 0.1688 - val_accuracy: 0.9491\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 184us/sample - loss: 0.1528 - accuracy: 0.9552 - val_loss: 0.1464 - val_accuracy: 0.9581\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s 187us/sample - loss: 0.1316 - accuracy: 0.9615 - val_loss: 0.1370 - val_accuracy: 0.9616\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.1178 - accuracy: 0.9654 - val_loss: 0.1307 - val_accuracy: 0.9617\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.1078 - accuracy: 0.9690 - val_loss: 0.1253 - val_accuracy: 0.9639\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0992 - accuracy: 0.9711 - val_loss: 0.1290 - val_accuracy: 0.9635\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0928 - accuracy: 0.9732 - val_loss: 0.1217 - val_accuracy: 0.9643\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0870 - accuracy: 0.9744 - val_loss: 0.1176 - val_accuracy: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f88faff17d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STOP YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "Provide here suitable plots and comments:\n",
    "\n",
    "* Comparison of the different learning curves: \n",
    "    * train accuracy vs epochs for different models\n",
    "    * train loss vs epochs for different models\n",
    "    * test accuracy vs epochs for different models\n",
    "    * test loss vs epochs for different models\n",
    "    \n",
    "* Inspect the histograms of the activations and compare them for the different models.\n",
    "\n",
    "* Find the max learning rate for the model with and without Batch Norm. \n",
    "\n",
    "Interpret the result and report your findings: Is it consistent with what you have learned in the lecture?\n",
    "\n",
    "Are there ways (e.g. change in model) so that the effects of batch norm become more clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "* No regularisation\n",
    "* No BatchNorm \n",
    "* Parameter Initialisation: GlorotNormal\n",
    "* Activation: ReLu\n",
    "* Optimizers: Compare \n",
    "    * SGD with given batchsize and learning rate, no accelerators (no momentum nor RMS prop)\n",
    "    * RmsProp\n",
    "    * Momentum\n",
    "\n",
    "Create an according model and train it with the different optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTPUTs\n",
    "\n",
    "TODO: \n",
    "* Comparison of the different learning curves: \n",
    "    * train accuracy vs epochs for optimizers\n",
    "    * train loss vs epochs for optimizers\n",
    "    * test accuracy vs epochs for optimizers\n",
    "    * test loss vs epochs for optimizers\n",
    "    \n",
    "Interpret the result and report your findings: Is it consistent with what you have learned in the lecture?\n",
    "\n",
    "Are there ways (e.g. change in model) so that the effects of the different optimizers become more clear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
