{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - CIFAR10 - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import TF and get its version.\n",
    "import tensorflow as tf\n",
    "tf_version = tf.__version__\n",
    "\n",
    "# Check if version >=2.0.0 is used\n",
    "if not tf_version.startswith('2.'):\n",
    "    print('\\033[91m' + 'WARNING: TensorFlow >= 2.0.0 will be used in this course.\\nYour version is {}'.format(tf_version) + '.\\033[0m')\n",
    "else:\n",
    "    print('\\033[92m' + 'OK: TensorFlow >= 2.0.0' + '.\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras import utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot some images from CIFAR and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(X,y):\n",
    "    plt.figure(1, figsize=(12,12))\n",
    "    k = 0\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            while y[k] != i: k += 1\n",
    "            plt.subplot2grid((10,10),(i,j))\n",
    "            plt.imshow(X[k])\n",
    "            plt.axis('off')\n",
    "            k += 1\n",
    "    plt.show()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "show_imgs(X_test, y_test)\n",
    "print('training input shape : ', X_train.shape)\n",
    "print('training output shape: ', y_train.shape)\n",
    "print('testing input shape  : ', X_test.shape)\n",
    "print('testing output shape : ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "After loading and splitting the data, we need to preprocess them by reshaping them into the shape the network expects and scaling them so that all values are in the \\[0, 1\\] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values of the network are supposed to be 1-hot targets. Now the `y_train` is an array with scalar values as in `[5 0 4 1 ...]` and it should be a 1-hot array `Y_train` as in : \n",
    "\n",
    "`[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]...]`\n",
    " \n",
    "Note the change of capital letter in the `Y_train` to denote, per convention, an array with multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)\n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network\n",
    "The neural network will be a CNN. Follow the structure given in the exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn = # ... TO COMPLETE \n",
    "# ...\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train the network\n",
    "\n",
    "When compiling the model, we need to specify the loss function, the optimizer and the metrics we want to track during the training. In Keras, we need to call the methods `compile()` and `fit()`. We will train through E epochs, using batches of size B, as specified in the exercise 1.\n",
    "\n",
    "- The `categorical_crossentropy` loss is relevant for multiclass, single-label classification problem. Categorical is used because there are 10 classes to predict from. If there were 2 classes, we would have used `binary_crossentropy`.\n",
    "- The `adam` optimizer is an improvement over SGD(Stochastic Gradient Descent). The optimizer is defining the update rule for the weights of the neurons during backpropagation gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(# ... TO COMPLETE)\n",
    "log = cnn.fit(# ... TO COMPLETE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network\n",
    "\n",
    "We can do this at three levels: (1) plot of the loss during the training phase, (2) overall accuracy evaluation on test set and (3) per class evaluation with confusion matrix on test set.\n",
    "\n",
    "### Loss and accuracy evolution during training\n",
    "This can be done first looking at the history of the training (output of the `fit()` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Testing acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "We can compute the overall performance on test set calling the `evaluate()` function on the model. The function returns the loss and the metrics used to compile the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "We can call the `predict_classes()` function to get the predicted classes. The output of this function is an array with the predicted class labels as in `[5 0 4 1 ...]`. The output array of ground truth `y_test` and the predicted classes can then be fed to the `confusion_matrix()` function of [sklearn metrics package](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn.predict_classes(X_test, verbose=0)\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Overfitting can be caused by having networks with too many parameters that are trained on too few samples. Through training, the model learns *by hart* and generalizes poorly.\n",
    "\n",
    "**Data augmentation** takes the approach of generating artificially more training data from existing training samples. For images, data augmentation is performed via a number of random transformations that yield believable-looking images. The goal is that at training time, the model will not see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "In Keras, this can be done by configuring a number of random transformations to be performed on the images read by the ```ImageDataGenerator``` instance.\n",
    "\n",
    "- rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "- width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
    "- shear_range is for randomly applying shearing transformations.\n",
    "- zoom_range is for randomly zooming inside pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the data augmentation pipelines, one for train set, one for test set\n",
    "# ... TO COMPLETE\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to reset your network (by defining it again). You need then to compile the network and train it. The call to the `fit()` function has to be replaced by a call to `fit_generator()` and using the data flow defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compile and train the network\n",
    "# ... TO COMPLETE\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Testing acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of results\n",
    "\n",
    "Describe your data augmentation strategies here.\n",
    "\n",
    "| CNN | Architecture description | Acc. train | Acc. test |\n",
    "|-----|--------------------------|------------|-----------|\n",
    "|  No DA | CONV(32F,same)-RELU-CONV(32F,same)-RELU-MAXP(2)-CONV(32F,same)-RELU-MAXP(2)-DENSE | ...  | ... |\n",
    "|  With DA v1 | CONV(32F,same)-RELU-CONV(32F,same)-RELU-MAXP(2)-CONV(32F,same)-RELU-MAXP(2)-DENSE | ...  | ... |\n",
    "|  With DA v2 | CONV(32F,same)-RELU-CONV(32F,same)-RELU-MAXP(2)-CONV(32F,same)-RELU-MAXP(2)-DENSE | ...  | ... |\n",
    "| ... | | | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
