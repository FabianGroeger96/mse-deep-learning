{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) \n",
    "*Explain the notion of hierarchical features with CNNs.*\n",
    "\n",
    "With the consecutive stacking of convolutional and max pooling layers the local receptive field of the neurons (activation maps) increases. This enables the network to learn lower level features (corresponding to a small neighborhood) in the early layers and higher level features (corresponding to a larger neighborhood) in the later layers as a combination of the lower level features.\n",
    "\n",
    "## (b)  \n",
    "*Explain 2 strategies to visualise the modelling taking place in CNNs.*\n",
    "\n",
    "We can visualize the activation maps of the individual neurons in a convolutional layer and observe what input they require to get activated. Another option would be to start with a randomly generated input and change it using gradient descent in order to maximize the output of a specific neuron and analyze the resulting image. \n",
    "\n",
    "## (c)  \n",
    "*What do we try to fight when using data augmentation ?*\n",
    "\n",
    "We try to avoid overfitting to the training data. By applying data augmentation we hope to find a more generalized model.\n",
    "\n",
    "## (d)  \n",
    "*What are the implementation strategies for data augmentation ?*\n",
    "\n",
    "- Pre-augmentation: The augmentation of the data happens once as a preprocessing step, hence the resulting dataset has again a fixed size.\n",
    "- Online augmentation: The augmentation is a part of the training pipeline and is applied on the fly to the dataset. Hence we no longer have a fixed size for the dataset, as we randomly select training samples and augment them at each training step.\n",
    "\n",
    "\n",
    "## (e)  \n",
    "*Explain the main differences for the deep architectures seen in class : AlexNet, VGGNet,GoogLeNet, ResNet. What were their intuitions when putting together such architectures?*\n",
    "\n",
    " - **AlexNet**: Uses some large kernel sizes and increases the number of filters with depth (hierarchy of features). Model has a lot of parameters\n",
    " - **VGGNet**: Only usage of small kernel sizes ($3\\times 3$) is sufficient and requires less parameters. Model mainly consists of multiple modules which contain $3\\times 3$ convolutions folowed by a max pooling. \n",
    " - **GoogLeNet**: Employs usage parallel convolutions with different kernel sizes / receptive fields per module so the network can learn which one or which combination of kernels is the optimal one. No dense layer anymore after flattening the output. Has 12 times less parameters than AlexNet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
