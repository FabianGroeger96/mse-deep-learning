{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)\n",
    "\n",
    "The softmaxlayer can convert the calculated logit values to a probability distribution. It is typically used as an activation function for the last layer in a network that tries to solve a classification task. The softmax is preferred over the sigmoid activation function when the problem formulation makes sure that a single input can only belog to one of the possible classes. \n",
    "\n",
    "## (b)\n",
    "- Cost function is still decreasing\n",
    "- Cost function show very large fluctuations\n",
    "\n",
    "\n",
    "## (c)\n",
    "I assume that a single layer perceptron is a network without any hiddenlayer.\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Amout of training data\n",
    "- Number of epochs\n",
    "\n",
    "\n",
    "## (d)\n",
    "A linear combination of linear functions will result again in a linear function. Hence only linear functions can be represented with only linear activation functions. \n",
    "\n",
    "\n",
    "## (e)\n",
    "Given a feedforward network with at least one hidden layer with a non-linear activation function can approximate any type of function if the capacity is high enough and the parameters are choosen *correctly*.\n",
    "\n",
    "\n",
    "## (f)\n",
    "Given a one dimensional function\n",
    "\n",
    "$$\n",
    "f(x) \\mapsto \\mathbb R, x\\in \\mathbb R\n",
    "$$\n",
    "\n",
    "We can superimpose 2 neurons of the hidden layer (of wich at least one is required) to create a simple step function\n",
    "\n",
    "$$\n",
    "\\phi_{[a, b]}(x)=\\sigma(s(x-a))-\\sigma(s(x-b))\n",
    "$$\n",
    "\n",
    "for an arbitrary intervall $I=[a,b]$ and $s$ fixed. Such stepfunctions can then be combined to approximate any arbitrary function.\n",
    "\n",
    "## (g)\n",
    "- To improve the approximation of the function, more step functions / neurons are required. With high-dimensional input this leads to exponentially growing number of neurons.\n",
    "- With a growing number of neurons, more data samples are required to learn the parameters correctly. This gets even worse with growing dimensionality in the input (image, audio etc.) > Curse of dimensionality.\n",
    "- The theorem does not provide a method to efficiently laern the parameters from available and limited data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
