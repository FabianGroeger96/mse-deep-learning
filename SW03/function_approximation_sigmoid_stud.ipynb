{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem - Gradient Descent Optimisation\n",
    "\n",
    "Here we study the possibility to approximate arbitrary functions $f: [0,1]\\rightarrow \\mathbb{R}$ by using MLPs with a single hidden layer ($n_1$ hidden units). Since this is a regression problem we use the MSE cost: \n",
    "\n",
    "The MSE cost for a neural net with a 1d input $x$, a single hidden layer with $n$ units and a linear output layer is given by\n",
    "\n",
    "$J_{\\rm MSE}(\\mathbf{\\theta}) = \\frac{1}{2m}\\sum_{i=1}^m \\left(y^{(i)} - (\\sum_{k=1}^{n_1} w_{2,k}\\sigma(w_{1,k}\\cdot x^{(i)} +b_{1,k})+b_2)\\right)^2$\n",
    "\n",
    "where we have used the sigmoid ('logit') function $\\sigma$ as activation function.\n",
    "\n",
    "Finally, we will use mini-batch-gradient descent to minimize MSE cost.\n",
    "\n",
    "The dataset is given by suitable $x$-values (in the interval $[0,1]$) and associated function values $f(x)$, i.e. $\\{(x^{(i)},y^{(i)}\\,=\\,f(x^{(i)}))\\, |\\, i=1,\\dots,m\\}$. The data for the training will be generated on the fly.\n",
    "\n",
    "Goals:\n",
    "* Learn how a given function can be represented with a single layer MLP.\n",
    "* Understand that, in principle, it can be learned from sample data.\n",
    "* Understand that the optimization based on plain gradient is not always straightforward. \n",
    "* Experience that the choice of the hyper-parameters number of hidden units, batchsize, learning rate is tricky. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Calculation of the Formulas\n",
    "\n",
    "Compute the formulas for gradient descent for this problem, i.e. compute the derivatives w.r.t. parameters $w_{1,k},w_{2,k},b_{1,k},b_2$ and formulate the according update rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "J_{\\rm MSE}\\left(\\mathbf{\\theta}\\right) &= \\frac{1}{2m}\\sum_{i=1}^m \\left(y^{\\left(i\\right)} - \\left( \\sum_{k=i}^n w_{2k}\\sigma\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)+b_2\\right)\\right)^2 \\\\ \n",
    "&= \\frac{1}{2m}\\left\\lVert\\left(Y - \\left( W_2\\sigma\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)+b_2\\right)\\right)\\right\\rVert_2^2 \\\\ \\\\ \\\\\n",
    "\\frac{\\partial}{\\partial b_2}J_{\\rm MSE}\\left(\\mathbf{\\theta}\\right) &= \\frac{1}{m}\\sum_{i=1}^m \\left(y^{\\left(i\\right)} - \\left( \\sum_{k=i}^n w_{2k}\\sigma\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)+b_2\\right)\\right) \\cdot \\left(-1\\right)=  \\frac{1}{m}\\sum_{i=1}^m \\delta_2^{\\left(i\\right)} \\cdot \\left(-1\\right) \\\\\n",
    "&= \\frac{1}{m}\\left(Y - \\left( W_2\\sigma\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)+b_2\\right)\\right) \\cdot \\left(-\\overrightarrow{1}^\\top\\right)=  \\frac{1}{m} \\Delta_2\\cdot \\left(-\\overrightarrow{1}^\\top\\right)\\\\ \\\\\n",
    "\\frac{\\partial}{\\partial w_{2k}}J_{\\rm MSE}\\left(\\mathbf{\\theta}\\right) &= \\frac{1}{m}\\sum_{i=1}^m \\left(y^{\\left(i\\right)} - \\left( \\sum_{k=i}^n w_{2k}\\sigma\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)+b_2\\right)\\right) \\cdot \\left(-\\sigma\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)\\right)= \\frac{1}{m}\\sum_{i=1}^m \\delta_2^{\\left(i\\right)} \\cdot \\left(-\\sigma\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)\\right) \\\\\n",
    " &= \\frac{1}{m}\\left(Y- \\left( W_2\\sigma\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)+b_2\\right)\\right) \\cdot \\left(-\\sigma\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)\\right)^\\top= \\frac{1}{m}\\Delta_2 \\cdot \\left(-\\sigma\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)\\right)^\\top \\\\ \\\\\n",
    "\\text{ with }\\delta_2^{(i)}&=y^{\\left(i\\right)} - \\left( \\sum_{k=i}^n w_{2k}\\sigma\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)+b_2\\right) \\\\\n",
    "\\Delta_2&=Y- \\left( W_2\\sigma\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)+b_2\\right)\\\\ \\\\ \\\\\n",
    "\\frac{\\partial}{\\partial b_{1k}}J_{\\rm MSE}\\left(\\mathbf{\\theta}\\right) &= \\frac{1}{m}\\sum_{i=1}^m \\delta_2^{\\left(i\\right)} \\cdot \\left(\\left(- w_{2k}\\sigma^\\prime\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)\\right)\\right)\\cdot 1 =  \\frac{1}{m}\\sum_{i=1}^m \\delta_2^{\\left(i\\right)} \\cdot \\delta_1^{\\left(i\\right)} \\cdot 1 \\\\ \n",
    "&= \\frac{1}{m} \\Delta_2 \\cdot \\left(-W_2^\\top\\circ\\sigma^\\prime\\left(W_1\\cdot X+\\overrightarrow{b_{1}}\\right)\\circ 1\\right)^\\top =  \\frac{1}{m} \\Delta_2 \\cdot (\\Delta_1\\circ 1)^\\top \\\\ \\\\\n",
    "\\frac{\\partial}{\\partial w_{1k}}J_{\\rm MSE}\\left(\\mathbf{\\theta}\\right) &= \\frac{1}{m}\\sum_{i=1}^m \\delta_2^{\\left(i\\right)} \\cdot \\left(\\left(- w_{2k}\\sigma^\\prime\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right)\\right)\\right)\\cdot x^{\\left(i\\right)} =  \\frac{1}{m}\\sum_{i=1}^m \\delta_2^{\\left(i\\right)} \\cdot \\delta_1^{\\left(i\\right)} \\cdot x^{\\left(i\\right)} \\\\\n",
    "&= \\frac{1}{m}\\Delta_2 \\cdot \\left(\\left(-W_2^\\top\\circ\\sigma^\\prime\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)\\right)\\circ X\\right)^\\top =  \\frac{1}{m} \\Delta_2 \\cdot \\left(\\Delta_1 \\circ X\\right)^\\top \\\\ \\\\\n",
    "\\text{ with }\\delta_1^{(i)}&=- w_{2k}\\sigma^\\prime\\left(w_{1k}\\cdot x^{\\left(i\\right)} +b_{1k}\\right) \\\\\n",
    "\\Delta_1&=-W_2^\\top\\circ\\sigma^\\prime\\left(W_1\\cdot X +\\overrightarrow{b_{1}}\\right)\n",
    "\\end{aligned} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\circ$ denotes the elementwise multiplication. Hence we can formulate the update rules as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "b_2^{(t+1)}&=b_2^{(t)}-\\alpha \\cdot \\frac{1}{m} \\Delta_2 \\cdot \\left(-\\overrightarrow{1}^\\top\\right) \\\\ \\\\\n",
    "W_2^{(t+1)}&=W_2^{(t)}-\\alpha \\cdot \\frac{1}{m}\\Delta_2 \\cdot \\left(-\\sigma\\left(W_1\\cdot x^{\\left(i\\right)} +\\overrightarrow{b_{1}}\\right)\\right)^\\top \\\\ \\\\ \\\\\n",
    "\\overrightarrow{b_1}^{(t+1)}&=\\overrightarrow{b_1}^{(t)}-\\alpha \\cdot \\frac{1}{m} \\Delta_2 \\cdot (\\Delta_1 \\circ 1)^\\top \\\\ \\\\\n",
    "W_{1}^{(t+1)}&=W_{1}^{(t)}-\\alpha \\cdot \\frac{1}{m} \\Delta_2 \\cdot \\left(\\Delta_1 \\circ X\\right)^\\top \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $t$ denotes the iteration step index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Implementation\n",
    "\n",
    "In this part, you need to implement and train the model:\n",
    "    \n",
    "1. Implement the cells below for the functions `predict`, `cost`, `gradient`, `train`.\n",
    "2. Generate the training data - by assuming a function on the unit interval $[0,1]$. Here, we provide two families of functions:\n",
    "    * Beta distribution function: $b_{\\alpha,\\beta}(x)=x^\\alpha\\cdot(1-x)^\\beta$\n",
    "    * Sine function: $sin_\\omega(x)=\\sin(2\\pi\\omega\\cdot x)$\n",
    "You can also add some moderate amount of noise to the data in the form $y = f(x)+\\sigma_Y * \\epsilon$ where $\\epsilon$ are standard normally distributed random numbers.\n",
    "3. Normalise the data (both $x$- and $y$ values.\n",
    "4. Train the model and convince yourself that the functional relationship could be learned. Can it be reliably learned? - Try several trainings in a row (starting from different random initial weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_function(x,y):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('x')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_function(x,y1,y2, label1='', label2=''):\n",
    "    plt.plot(x, y1, label=label1)\n",
    "    plt.xlabel('x')\n",
    "    plt.plot(x, y2, label=label2)\n",
    "    if label1 and label2:\n",
    "        plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,W1,b1,W2,b2):\n",
    "    \"\"\"\n",
    "    Computes the output for the single hidden layer network (n1 units) with 1d input and 1d output.\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weights of the hidden layer with shape (n1,1)\n",
    "    b1 -- biases of the hidden units with shape (n1,1)\n",
    "    W2 -- weights of the output layer with shape (1,n1)\n",
    "    b2 -- bias of the output\n",
    "    X  -- input data with m samples and shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- Output from the network of shape (1,m) \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    hidden_layer = sigmoid(W1.dot(X) + b1)\n",
    "    A2 = W2.dot(hidden_layer) + b2\n",
    "        \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([0.4,0.2,-0.4]).reshape(3,1) # n1 = 3\n",
    "b1 = np.array([0.1,0.1,0.1]).reshape(3,1)\n",
    "W2 = np.array([1,2,1]).reshape(1,3)\n",
    "b2 = -1\n",
    "X = np.linspace(-1,1,5).reshape((1,5))\n",
    "Ypred = predict(X,W1,b1,W2,b2)\n",
    "Yexp = np.array([0.99805844, 1.04946333, 1.09991675, 1.14913132, 1.19690185]).reshape(1,5)\n",
    "np.testing.assert_array_almost_equal(Ypred,Yexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y,Ypred):\n",
    "    \"\"\"\n",
    "    Computes the MSE cost for given ground truth Y and predicted Ypred\n",
    "    Uses the predict function defined above.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- ground truth output with shape (1,m) \n",
    "    Ypred -- predicted output with shape (1,m) \n",
    "    \n",
    "    Returns:\n",
    "    cost -- the MSE cost divided by 2.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = 1 / (2*m) * np.linalg.norm((Y-Ypred))**2\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([4,5,6]).reshape(3,1)\n",
    "W2 = np.array([1,2,3]).reshape(1,3)\n",
    "b1 = np.array([1,1,1]).reshape(3,1)\n",
    "b2 = 2\n",
    "X = np.linspace(-1,1,5).reshape(1,5)\n",
    "Ypred = predict(X,W1,b1,W2,b2)\n",
    "Y = 2.0*np.ones(5).reshape(1,5)\n",
    "c = cost(Y,Ypred)\n",
    "cexp = 9.01669099\n",
    "np.testing.assert_almost_equal(c,cexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W1,b1,W2,b2,X,Y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the MSE cost for a single hidden layer network with 1d input and 1d output.\n",
    "    The parts of the gradient associated with the weights array and bias array for the hidden layer, \n",
    "    the weights array and the bias for the output layer are provided as separate numpy arrays of according \n",
    "    dimension. \n",
    "    \n",
    "    Arguments:    \n",
    "    W1 -- weights of hidden layer with shape (n1,1)\n",
    "    b1  -- biases of hidden layer with shape (n1,1)\n",
    "    W2 -- weights of output layer with shape (1,n1)\n",
    "    b2  -- biases of output layer\n",
    "    X  -- input data with shape (1,m)\n",
    "    Y  -- labels with shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    gradient -- dictionary with the gradients w.r.t. W1, W2, b1, b2 and according keys \n",
    "                'dW1' with shape (n1,1)\n",
    "                'db1' with shape (n1,1)\n",
    "                'dW2' with shape (1,n1)\n",
    "                'db2' a scalar\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    m = Y.shape[1]\n",
    "    n1 = W1.shape[0]\n",
    "    \n",
    "    \n",
    "    hidden_layer = sigmoid(W1.dot(X) + b1)\n",
    "    \n",
    "    delta_1 = - (W2.T * (hidden_layer * (1 - hidden_layer)))\n",
    "    \n",
    "    delta_2 = Y - (W2.dot(hidden_layer) + b2)\n",
    "    \n",
    "    db2 = 1 / m * (delta_2.dot(-np.ones((1, m)).T))\n",
    "    \n",
    "    dW2 = 1 / m * (delta_2.dot(-hidden_layer.T))\n",
    "    \n",
    "    db1 = 1 / m * (delta_2.dot((delta_1 * 1).T)).T\n",
    "    \n",
    "    dW1 = 1 / m * (delta_2.dot((delta_1 * X).T)).T\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return {'dW1':dW1, 'dW2':dW2, 'db1':db1, 'db2':db2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([4,5,6]).reshape(3,1)\n",
    "W2 = np.array([1,2,3]).reshape(1,3)\n",
    "b1 = np.array([1,1,1]).reshape(3,1)\n",
    "b2 = 2\n",
    "X = np.array([1,2,3,4,5,6,7]).reshape((1,7))\n",
    "Y = np.array([2,2,2,2,2,2,2]).reshape((1,7))\n",
    "gradJ = gradient(W1,b1,W2,b2,X,Y)\n",
    "dW1exp = np.array([0.00590214,0.00427602,0.00234663]).reshape(W1.shape)\n",
    "db1exp = np.array([0.00579241,0.004247,0.00234079]).reshape(b1.shape)\n",
    "dW2exp = np.array([5.99209251,5.99579451,5.99714226]).reshape(W2.shape)\n",
    "db2exp = 5.99792323\n",
    "np.testing.assert_almost_equal(gradJ['db2'],db2exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['dW2'],dW2exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['db1'],db1exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['dW1'],dW1exp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,n1,nepochs,batchsize=32,learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Performs the training by using MBGD for a MLP with a single hidden layer (n1 units) and 1d input and output layer.\n",
    "    \n",
    "    It starts with initializing the parameters:\n",
    "    * the weights and the biases for the hidden units : W1,b1 of shape (n1,1) \n",
    "    * the weights and the bias for the output layer: W2 of shape (1,n1) and scalar b2 \n",
    "\n",
    "    Then, it loops over the epochs and per epoch over the mini-batches. The number of batches is determined from the \n",
    "    batchsize.\n",
    "    \"\"\"\n",
    "    # initialize weights\n",
    "    W1 = np.random.uniform(-1,1,n1).reshape(n1,1) / (2*np.sqrt(n1))\n",
    "    b1 = np.zeros((n1,1), dtype=float)\n",
    "    W2 = np.random.uniform(-1,1,n1).reshape(1,n1) / (2*np.sqrt(n1))\n",
    "    b2 = 0.0\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    mb = int(m/batchsize)\n",
    "    indices = np.arange(m)\n",
    "    \n",
    "    # remember the epoch id and cost after each epoch for constructing the learning curve at the end\n",
    "    costs = [] \n",
    "    epochs = []\n",
    "\n",
    "    # Initial cost value:\n",
    "    epochs.append(0)\n",
    "    Ypred = predict(X,W1,b1,W2,b2)\n",
    "    costs.append(cost(Y,Ypred)) \n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(nepochs):\n",
    "        ### START YOUR CODE ###\n",
    "\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in range(mb):\n",
    "            batch = np.random.choice(indices, size=batchsize)\n",
    "            Xbatch = X[:, batch]\n",
    "            ybatch = Y[:, batch]\n",
    "        \n",
    "            grad = gradient(W1,b1,W2,b2,X,Y)\n",
    "            \n",
    "            W1 -= learning_rate * grad[\"dW1\"]\n",
    "            b1 -= learning_rate * grad[\"db1\"]\n",
    "            W2 -= learning_rate * grad[\"dW2\"]\n",
    "            b2 -= learning_rate * grad[\"db2\"]\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        epochs.append(epoch+1)\n",
    "        Ypred = predict(X,W1,b1,W2,b2)\n",
    "        costs.append(cost(Y,Ypred))         \n",
    "    \n",
    "    print(costs[-1])    \n",
    "    params = {'W1':W1, 'W2':W2,'b1':b1,'b2':b2}    \n",
    "    return params, np.array(epochs), np.array(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the (Training) Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_fct(x,alpha,beta):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x - input array\n",
    "    alpha, beta -- larger values lead to more pronounced peaks\n",
    "    \"\"\"\n",
    "    c = alpha/(alpha+beta)\n",
    "    norm = c**alpha*(1-c)**beta\n",
    "    return x**alpha*(1-x)**beta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_fct(x,omega):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x -- input array\n",
    "    omega -- frequency that defines the integer number of cycles within the unit interval\n",
    "    \"\"\"\n",
    "    return np.sin(x*2*np.pi*omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(m, func, random=True, vargs=None, sigmaY=0.0):\n",
    "    \"\"\"\n",
    "    Generates m (x,y=f(x))-samples by either generating random x-values in the unit interval (random=True) or by \n",
    "    generating a grid of such values. Then the y values (used as labels below) are created from the function object \n",
    "    `func`.\n",
    "    Parameter needed to define the function `func` can be passed as vargs-dict. \n",
    "    \"\"\"\n",
    "    if random:\n",
    "        x = np.random.rand(1,m)\n",
    "    else:\n",
    "        x = np.linspace(0,1,m).reshape(1,m)\n",
    "    y = func(x,**vargs) + sigmaY*np.random.randn(*(1,m))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Input and Output\n",
    "\n",
    "It turns out that it is important to normalize the input and the output data here.\n",
    "Remember the mean and stdev computed for the training data so that you can also apply it to the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mu=None, stdev=None):\n",
    "    \"\"\"\n",
    "    Normalizes the data by using z-normalization. If mu and stdev are NOT specified, mean and stedev are \n",
    "    computed from the given samples.   \n",
    "\n",
    "    Returns:\n",
    "    X1 -- normalized data (array of the same shape as input)\n",
    "    mu -- mean\n",
    "    stdev -- standard deviation\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    if not mu:\n",
    "        mu = np.mean(X)\n",
    "    if not stdev:\n",
    "        stdev = np.std(X)\n",
    "    X1 = (X-mu)/stdev\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return X1,mu,stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_normalize(X1, mu, stdev):\n",
    "    \"\"\"\n",
    "    Invert the normalization. This is needed to bring the predicted values back to the original scale.\n",
    "\n",
    "    Returns:\n",
    "    X -- unnormalized data (array of the same shape as input X1)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    X = X1 * stdev + mu\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Training\n",
    "\n",
    "Includes generating and normalizing the data and training. Test data can be generated as non-random.<br>\n",
    "Make sure that you do the test performance on the right scales (of both x-values and y-values)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrain = 1000\n",
    "#func = beta_fct\n",
    "#vargs={'alpha':2.0,'beta':2.0}\n",
    "func = sin_fct\n",
    "vargs={'omega':1.0}\n",
    "X,Y = generate_inputs(mtrain,func,vargs=vargs, random=False, sigmaY=0.0)\n",
    "X1, muX, stdevX = normalize(X)\n",
    "Y1, muY, stdevY = normalize(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X1[0,:],Y1[0,:],'+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "nepochs = 5000 # number of epochs\n",
    "batchsize = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "params, epochs, costs = train(X1, Y1, n_hidden, nepochs, batchsize=batchsize,learning_rate=learning_rate)\n",
    "plt.semilogy(epochs,costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Compute the predicted values on the test set and compute the MSE cost.\n",
    "Prepare a (x,y)-plot with the ground truth test values and the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(0,1,0.05).reshape(1,int(1/0.05))\n",
    "yy = func(xx, **vargs)\n",
    "plt.plot(xx[0,:],yy[0,:],'r--')\n",
    "\n",
    "xx0,_,_ = normalize(xx, muX, stdevX)\n",
    "yypred0 = predict(xx0, params['W1'],params['b1'],params['W2'],params['b2'])\n",
    "yypred = inv_normalize(yypred0, muY, stdevY)\n",
    "plt.plot(xx[0,:],yypred[0,:],'g-')\n",
    "\n",
    "#plt.plot(X[0,:],Y[0,:],'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
