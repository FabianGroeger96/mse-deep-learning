{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification\n",
    "\n",
    "The goal of this notebook is to implement a model consisting of just a softmax layer for classifying MNIST images, hence to perform a multiclass classification task. The notation follows the convention introduced for PW02:\n",
    "\n",
    "<code>m</code>: Number of samples <br>\n",
    "<code>n</code>: Number of features\n",
    "\n",
    "Please follow the instructions in the cells below to conduct the following tasks:\n",
    "\n",
    "1. Implement the functions to prepare the data (very similar to PW02).\n",
    "2. Implement softmax, the cross entropy cost for multiclass and its gradient.\n",
    "3. Implement the `optimize` function (with given function signature) by using the classes `Metrics` and `MiniBatches`.  \n",
    "4. Run several trainings with different hyper-parameter settings and determine your favorite setting (1b).\n",
    "5. Compute the Error Rates for the individual Digits (1c)\n",
    "6. Analyze misclassified images with worst score (1d)\n",
    "8. Plot the weights as images (1e)\n",
    "7. Analyze different weights initialisation strategies (1f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Data\n",
    "\n",
    "Some preparatory steps to be applied before training:\n",
    "* Imports: numpy and matplotlib\n",
    "* Loading the data (same as for PW of previous week) \n",
    "* Some plot utilities\n",
    "* Splitting the dataset into train and test\n",
    "* Data Standarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ### \n",
    "data_home = \"./\"\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def load_mnist(data_home):\n",
    "    \"\"\"\n",
    "    Loads the mnist dataset, prints the shape of the dataset and \n",
    "    returns the array with the images, the array with associated labels \n",
    "    and the shape of the images.     \n",
    "    Parameters: \n",
    "    data_home -- Absolute path to the DATA_HOME  \n",
    "    \n",
    "    Returns:\n",
    "    x -- array with images of shape (784,m) where m is the number of images\n",
    "    y -- array with associated labels with shape (1,m) where m is the number of images\n",
    "    shape -- (28,28)\n",
    "    \"\"\"\n",
    "    mnist = fetch_openml(name='mnist_784', version=1, cache=True, data_home=data_home)\n",
    "    x, y = mnist['data'].T, np.array(mnist['target'], dtype='int').T\n",
    "    m = x.shape[1]\n",
    "    y = y.reshape(1,m)\n",
    "    print(\"Loaded MNIST original:\")\n",
    "    print(\"Image Data Shape\" , x.shape)\n",
    "    print(\"Label Data Shape\", y.shape)\n",
    "    return x,y,(28,28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img, label, shape):\n",
    "    \"\"\"\n",
    "    Plot the x array by reshaping it into a square array of given shape\n",
    "    and print the label.\n",
    "    \n",
    "    Parameters:\n",
    "    img -- array with the intensities to be plotted of shape (shape[0]*shape[1])\n",
    "    label -- label \n",
    "    shape -- 2d tuple with the dimensions of the image to be plotted.\n",
    "    \"\"\"\n",
    "    plt.imshow(np.reshape(img, shape), cmap=plt.cm.gray)\n",
    "    plt.title(\"Label %i\"%label)\n",
    "\n",
    "\n",
    "def plot_digits(x,y,selection,shape, cols=5):\n",
    "    \"\"\"\n",
    "    Plots the digits in a mosaic with given number of columns.\n",
    "\n",
    "    Arguments:\n",
    "    x -- array of images of size (n,m)\n",
    "    y -- array of labels of size (1,m)\n",
    "    selection -- list of selection of samples to be plotted\n",
    "    shape -- shape of the images (a 2d tuple)\n",
    "    selected_digits -- tuple with the two selected digits (the first associated with label 1, the second with label 0)\n",
    "    \"\"\"\n",
    "    if len(selection)==0:\n",
    "        print(\"No images in the selection!\")\n",
    "        return\n",
    "    cols = min(cols, len(selection))\n",
    "    rows = len(selection)/cols+1\n",
    "    plt.figure(figsize=(20,4*rows))\n",
    "    for index, (image, label) in enumerate(zip(x.T[selection,:], y.T[selection,:])):\n",
    "        plt.subplot(rows, cols, index+1)\n",
    "        plt.imshow(np.reshape(image, shape), cmap=plt.cm.gray)\n",
    "        plt.title('Sample %i\\n Label %i\\n' % (selection[index],label), fontsize = 12)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and reshape as specified\n",
    "\n",
    "Split the data into training set and test set.\n",
    "We use the scikit-learn function 'train_test_split' with 20\\% test data.\n",
    "\n",
    "Furthermore, we reshape input data x to (n,m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_train_test(x, y, test_size=0.20):\n",
    "    \"\"\"\n",
    "    Split the dataset consisting of an array of images (shape (n, m)) and an array of labels (shape (1, m))\n",
    "    into train and test set.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- Array of images of shape (n,m) where m is the number of samples\n",
    "    y -- Array of labels of shape (1, m) where m is the number of samples\n",
    "    test_size -- fraction of samples to reserve as test sample\n",
    "    \n",
    "    Returns:\n",
    "    x_train -- np.ndarray of images of shape (n,m1) used for training\n",
    "    y_train -- np.ndarray of labels of shape (1,m1) used for training\n",
    "    x_test -- np.ndarray of images of shape (n,m2) used for testing\n",
    "    y_test -- np.ndarray of labels of shape (1,m2) used for testing\n",
    "    \"\"\"\n",
    "    # split \n",
    "    # train_test_split() expects x, y in shapes (m, *), (m, *) \n",
    "    \n",
    "    out = train_test_split(x.T, y.T, test_size=0.20, random_state=1)\n",
    "    \n",
    "    # transpose back the output obtained from the train_test_split-function\n",
    "    x_train, x_test, y_train, y_test = (x.T for x in out)\n",
    "    \n",
    "    print(\"Shape training set: \", x_train.shape, y_train.shape)\n",
    "    print(\"Shape test set:     \", x_test.shape, y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Normalisation\n",
    "\n",
    "Normalize the data - apply min/max normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(x_train, x_test):\n",
    "    \"\"\"\n",
    "    Normalizes pixel values using min-max normalization, min and max are calculated globally over all features n\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    x_train -- Array of training samples of shape (n,m1) where n,m1 are the number of features and samples, respectively.  \n",
    "    x_test -- Array of test samples of shape (n,m2) where n,m2 are the number of features and samples, respectively. \n",
    "    \n",
    "    Returns:\n",
    "    The arrays with the normalized train and test samples.  \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###  \n",
    "    \n",
    "    maxx = x_train.max()\n",
    "    minn = x_train.min()\n",
    "    \n",
    "    x_train = 2 * (x_train - minn) / (maxx - minn) - 1\n",
    "    x_test = 2 * (x_test - minn) / (maxx - minn) - 1\n",
    "    \n",
    "    ### END YOUR CODE ###     \n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_unittest = np.array([[10, 5], [-5, -10]])\n",
    "x_test_unittest = np.array([[10, 1], [0, -10]])\n",
    "actual_train, actual_test = normalize(x_train_unittest, x_test_unittest)\n",
    "expected_train, expected_test = np.array([[1, 0.5], [-0.5, -1]]), np.array([[1, 0.1], [0, -1]])\n",
    "np.testing.assert_almost_equal(expected_train, actual_train)\n",
    "np.testing.assert_almost_equal(expected_test, actual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Implement the softmax function - actually, the softmax layer with given weights-matrix $W$ and bias-vector $b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Compute the per class probabilities for all the m samples by using a softmax layer with parameters (W, b).\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array with shape (ny, nx) (with ny=10 for MNIST).\n",
    "    b -- biases, a numpy array with shape (ny,1)\n",
    "    X -- input data of size (nx,m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- a numpy array of shape (ny,m) with the prediction probabilities for the digits.\n",
    "    ''' \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    z = W.dot(X) + b\n",
    "    \n",
    "    z_exp = np.exp(z)\n",
    "    \n",
    "    return z_exp / z_exp.sum(axis=0)\n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([2, 3]).reshape(2,1)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([0.01587624,0.86681333,0.11731043]).reshape(A.shape)\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), 1.0, decimal=8)\n",
    "\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "Aexp = np.array([[0.46831053, 0.01321289, 0.21194156, 0.01321289],\n",
    " [0.46831053, 0.26538793, 0.57611688, 0.26538793],\n",
    " [0.06337894, 0.72139918, 0.21194156, 0.72139918]]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(A,Aexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(A, axis=0), np.ones(4,dtype='float'), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Cross Entropy)\n",
    "\n",
    "Implement the cross entropy cost function for the multi-class setting. \n",
    "\n",
    "For later use implement a function to create one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost(Ypred, Y, eps=1.0e-12):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy cost function for given predicted values (Ypred) and labels (Y).\n",
    "    \n",
    "    Parameters:\n",
    "    Ypred -- prediction from softmax, a numpy array of shape (ny,m)\n",
    "    Y -- ground truth labels, a numpy array with shape (1,m) containing digits 0,1,...,9. \n",
    "    \n",
    "    Returns:\n",
    "    Cross Entropy Cost\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###  \n",
    "    \n",
    "    Ypred = np.clip(Ypred, eps, 1-eps)\n",
    "    \n",
    "    Y = onehot(Y, Ypred.shape[0])\n",
    "        \n",
    "    # From my point of view the CE should be summed over all predicted classes, not averaged\n",
    "    J = -(Y*np.log(Ypred)+(1-Y)*np.log(1-Ypred)).mean(axis=0)\n",
    "    \n",
    "    return J.mean()\n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y,n):\n",
    "    \"\"\"\n",
    "    Constructs a one-hot-vector from a given array of labels (shape (1,m), containing numbers 0,1,...,n-1) \n",
    "    and the number of classes n.\n",
    "    The resulting array has shape (n,m) and in row j and column i a '1' if the i-th sample has label 'j'. \n",
    "    \n",
    "    Parameters:\n",
    "    y -- labels, numpy array of shape (1,m) \n",
    "    n -- number of labels\n",
    "\n",
    "    Returns:\n",
    "    On-hot-encoded vector of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    m = y.reshape(-1).shape[0]\n",
    "    result = np.zeros((n, m))\n",
    "    result[y.reshape(-1), np.arange(0, m, 1)] = 1\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST Cross Entropy Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([1])\n",
    "Ypred = np.array([0.04742587,0.95257413]).reshape(2,1)\n",
    "J = cost(Ypred,Y)\n",
    "Jexp = 0.04858735\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)\n",
    "\n",
    "Y = np.array([1,1,1,0])\n",
    "Ypred = np.array([[1.79862100e-02, 6.69285092e-03, 4.74258732e-02, 9.99088949e-01],\n",
    "                  [9.82013790e-01, 9.93307149e-01, 9.52574127e-01, 9.11051194e-04]])\n",
    "Jexp = 0.01859102\n",
    "J = cost(Ypred,Y)\n",
    "np.testing.assert_almost_equal(J,Jexp,decimal=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test one-hot vector implementation ##\n",
    "Y = np.array([1,3,0]).reshape(1,3)\n",
    "onehot_comp = onehot(Y,4)\n",
    "onehot_exp = np.array([[0,0,1],[1,0,0],[0,0,0],[0,1,0]]).reshape(4,3)\n",
    "np.testing.assert_almost_equal(onehot_exp,onehot_comp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rules for the Parameters\n",
    "\n",
    "Implement the (estimate of) gradient of the cost function (cross entropy implemented above) with respect to the parameters of the softmax layer.\n",
    "The contributions from the different samples given in $X$ and $Y$ should be averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, Y, A):\n",
    "    \"\"\"\n",
    "    Computes the components of the gradient w.r.t. weights and bias - by using the cross entropy cost. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of size (nx,m)\n",
    "    Y -- output labels - a numpy array with shape (1,m).\n",
    "    A -- predicted scores (as output of softmax) - a numpy array with shape (ny,m) \n",
    "    \n",
    "    Returns:\n",
    "    gradJ -- dictionary with the gradient w.r.t. W (key \"dW\" with shape (ny,nx)) and w.r.t. b (key \"db\" with shape (ny,1))\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "\n",
    "    Y = onehot(Y, A.shape[0])\n",
    "    d = -(Y - A)\n",
    "    \n",
    "    m = A.shape[1]\n",
    "    \n",
    "    dw = 1/m * d.dot(X.T)\n",
    "    db = 1/m * d.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return {\"dW\": dw, \"db\": db}\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Calculation of the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1,-1],[0,1],[-1,1]]).reshape(3,2)\n",
    "b = np.array([0,0,0]).reshape(3,1)\n",
    "X = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4)\n",
    "A = predict(W,b,X)\n",
    "\n",
    "Y = np.array([1,1,1,1]).reshape(1,4)\n",
    "\n",
    "gradJ = gradient(X,Y,A)\n",
    "dW = gradJ['dW']\n",
    "db = gradJ['db']\n",
    "dWexp = np.array([[ 0.28053421,0.17666947],\n",
    "                  [-0.00450948,-0.60619918],\n",
    "                  [-0.27602473,0.42952972]]).reshape(3,2)\n",
    "dbexp = np.array([0.17666947,-0.60619918,0.42952972]).reshape(3,1)\n",
    "np.testing.assert_array_almost_equal(dW,dWexp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(db,dbexp, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for measuring the performance of the algorithm\n",
    "\n",
    "As metrics we compute the error rate as number of wrong predictions divided by the number of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def error_rate(Ypred, Y):\n",
    "    \"\"\"\n",
    "    Compute the error rate defined as the fraction of misclassified samples.\n",
    "    \n",
    "    Arguments:\n",
    "    Ypred -- Predicted label, a numpy array of size (1,m)\n",
    "    Y -- ground truth labels, a numpy array with shape (1,m)\n",
    "\n",
    "    Returns:\n",
    "    error_rate \n",
    "    \"\"\"\n",
    "    Ypredargmax = np.argmax(Ypred, axis=0)\n",
    "    return np.mean(Y != Ypredargmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize (Learn)\n",
    "\n",
    "As in PW02, we first provide the metrics class that is used for tracking progress during the training (by collecting suitable quantities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    Allows to collect statistics (such as classification error or cost) that are of interest over the course of training\n",
    "    and for creating learning curves that are a useful tool for analyzing the quality of the learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cost, smooth=False):\n",
    "        \"\"\"\n",
    "        Constructor for a metrics object. \n",
    "        Initializes all the statistics to track in form of python lists.\n",
    "        \n",
    "        Parameters:\n",
    "        cost -- cost function to use (a python function)\n",
    "        smooth -- if set to true updates learning curve after each training step and also provides learning curves \n",
    "        smoothed over the epoch  \n",
    "        \"\"\"\n",
    "        self.epochs = []\n",
    "        self.smooth = smooth\n",
    "        self.train_costs_last = []\n",
    "        self.test_costs_last = []\n",
    "        self.train_errors_last = []\n",
    "        self.test_errors_last = []\n",
    "        self.stepsize_w_last = []\n",
    "        self.stepsize_b_last = []\n",
    "        if self.smooth:\n",
    "            self.train_costs_smoothed = []\n",
    "            self.test_costs_smoothed = []\n",
    "            self.train_errors_smoothed = []\n",
    "            self.test_errors_smoothed = []\n",
    "            self.stepsize_w_smoothed = []\n",
    "            self.stepsize_b_smoothed = []\n",
    "\n",
    "        self.cost_function = cost\n",
    "        self.init_epoch()\n",
    "\n",
    "            \n",
    "    def init_epoch(self):\n",
    "        self.train_costs_epoch = []\n",
    "        self.test_costs_epoch = []\n",
    "        self.train_errors_epoch = []\n",
    "        self.test_errors_epoch = []\n",
    "        self.stepsize_w_epoch = []\n",
    "        self.stepsize_b_epoch = []\n",
    "        \n",
    "        \n",
    "    def update_epoch(self, epoch):\n",
    "        \"\"\"\n",
    "        Computes the average of the metrics over the epoch and adds the result to the per epoch history\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- the epoch to add to the per epoch cache\n",
    "        \"\"\"\n",
    "        self.epochs.append(epoch)\n",
    "        if self.smooth:\n",
    "            self.train_costs_smoothed.append(np.mean(self.train_costs_epoch))\n",
    "            self.test_costs_smoothed.append(np.mean(self.test_costs_epoch))\n",
    "            self.train_errors_smoothed.append(np.mean(self.train_errors_epoch))\n",
    "            self.test_errors_smoothed.append(np.mean(self.test_errors_epoch))\n",
    "            self.stepsize_w_smoothed.append(np.mean(self.stepsize_w_epoch))\n",
    "            self.stepsize_b_smoothed.append(np.mean(self.stepsize_b_epoch))                    \n",
    "\n",
    "        self.train_costs_last.append(self.train_costs_epoch[-1])\n",
    "        self.test_costs_last.append(self.test_costs_epoch[-1])\n",
    "        self.train_errors_last.append(self.train_errors_epoch[-1])\n",
    "        self.test_errors_last.append(self.test_errors_epoch[-1])\n",
    "        self.stepsize_w_last.append(self.stepsize_w_epoch[-1])\n",
    "        self.stepsize_b_last.append(self.stepsize_b_epoch[-1])                    \n",
    "        \n",
    "        self.init_epoch()\n",
    "    \n",
    "        \n",
    "    def update_iteration(self, ypred_train, y_train, ypred_test, y_test, dw, db):\n",
    "        \"\"\"\n",
    "        Allows to update the statistics to be tracked for a new epoch.\n",
    "        The cost is computed by using the function object passed to the constructor.\n",
    "        \n",
    "        Parameters:\n",
    "        epoch -- Epoch\n",
    "        ypred_train -- predicted values on the training samples, a numpy array of shape (1,m1)\n",
    "        y_train -- ground truth labels associated with the training samples, a numpy array of shape (1,m1)\n",
    "        ypred_test -- predicted values on the test samples, a numpy array of shape (1,m2)\n",
    "        y_test -- ground truth labels associated with the test samples, a numpy array of shape (1,m2)\n",
    "        dw -- some lenght measure for the gradient w.r.t. the weights, a numpy array of shape (1,n)\n",
    "        db -- gradient w.r.t. the bias, a scalar\n",
    "        \"\"\"\n",
    "        Jtrain = self.cost_function(ypred_train, y_train)\n",
    "        Jtest = self.cost_function(ypred_test, y_test)\n",
    "        train_error = error_rate(ypred_train, y_train)\n",
    "        test_error = error_rate(ypred_test, y_test)\n",
    "\n",
    "        self.train_costs_epoch.append(Jtrain)\n",
    "        self.test_costs_epoch.append(Jtest)\n",
    "        self.train_errors_epoch.append(train_error)\n",
    "        self.test_errors_epoch.append(test_error)\n",
    "        self.stepsize_w_epoch.append(dw)\n",
    "        self.stepsize_b_epoch.append(db)\n",
    "        \n",
    "        \n",
    "    def print_latest_errors(self):\n",
    "        print (\"Train/test error after epoch %i: %f, %f\" %(self.epochs[-1], self.train_errors_last[-1], self.test_errors_last[-1]))\n",
    "\n",
    "    def print_latest_costs(self):\n",
    "        print (\"Train/test cost after epoch %i: %f, %f\" %(self.epochs[-1], self.train_costs_last[-1], self.test_costs_last[-1]))\n",
    "\n",
    "    def plot_cost_curves(self, ymin=None, ymax=None, smooth=True, logy=True, show=True):\n",
    "        minvalue = 1e-5\n",
    "        if logy:\n",
    "            plt.semilogy(self.epochs, self.train_costs_last, \"b-\", label=\"train\")\n",
    "            plt.semilogy(self.epochs, self.test_costs_last, \"r-\", label=\"test\")\n",
    "            if self.smooth:\n",
    "                plt.semilogy(self.epochs, self.train_costs_smoothed, \"b--\", label=\"train_smoothed\")\n",
    "                plt.semilogy(self.epochs, self.test_costs_smoothed, \"r--\", label=\"test_smoothed\")\n",
    "        else:\n",
    "            plt.plot(self.epochs, self.train_costs_last, \"b-\", label=\"train\")\n",
    "            plt.plot(self.epochs, self.test_costs_last, \"r-\", label=\"test\")   \n",
    "            minvalue = 0.0\n",
    "            if self.smooth:\n",
    "                plt.plot(self.epochs, self.train_costs_smoothed, \"b--\", label=\"train_smoothed\")\n",
    "                plt.plot(self.epochs, self.test_costs_smoothed, \"r--\", label=\"test_smoothed\")\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.train_costs_last)),max(1e-5,np.min(self.test_costs_last))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.train_costs_last),np.max(self.test_costs_last)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        if show:            \n",
    "            plt.show()\n",
    "    \n",
    "    def plot_error_curves(self, ymin=None, ymax=None, smooth=True, logy=True, show=True):\n",
    "        minvalue = 1e-5\n",
    "        if logy:\n",
    "            plt.semilogy(self.epochs, self.train_errors_last, \"b-\", label=\"train\")\n",
    "            plt.semilogy(self.epochs, self.test_errors_last, \"r-\", label=\"test\")\n",
    "            if self.smooth:\n",
    "                plt.semilogy(self.epochs, self.train_errors_smoothed, \"b--\", label=\"train_smoothed\")\n",
    "                plt.semilogy(self.epochs, self.test_errors_smoothed, \"r--\", label=\"test_smoothed\")\n",
    "        else: \n",
    "            plt.plot(self.epochs, self.train_errors_last, \"b-\", label=\"train\")\n",
    "            plt.plot(self.epochs, self.test_errors_last, \"r-\", label=\"test\")\n",
    "            minvalue = 0.0\n",
    "            if self.smooth:\n",
    "                plt.plot(self.epochs, self.train_errors_smoothed, \"b--\", label=\"train_smoothed\")\n",
    "                plt.plot(self.epochs, self.test_errors_smoothed, \"r--\", label=\"test_smoothed\")\n",
    "        plt.ylabel('Errors')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.train_errors_last)),max(1e-5,np.min(self.test_errors_last))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.train_errors_last),np.max(self.test_errors_last)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        if show:            \n",
    "            plt.show()\n",
    "\n",
    "    def plot_stepsize_curves(self, ymin=None, ymax=None, smooth=True, show=True):\n",
    "        plt.semilogy(self.epochs, self.stepsize_w_last, label=\"dw\")\n",
    "        plt.semilogy(self.epochs, self.stepsize_b_last, label=\"db\")\n",
    "        if self.smooth and smooth:\n",
    "            plt.semilogy(self.epochs, self.stepsize_w_smoothed, label=\"dw--\")\n",
    "            plt.semilogy(self.epochs, self.stepsize_b_smoothed, label=\"db--\")\n",
    "        plt.ylabel('Step Sizes (dw,db)')\n",
    "        plt.xlabel('Epochs')\n",
    "        xmax = self.epochs[-1]\n",
    "        if not ymin:\n",
    "            ymin = min(max(1e-5,np.min(self.stepsize_w_last)),max(1e-5,np.min(self.stepsize_b_last))) * 0.8\n",
    "        if not ymax:\n",
    "            ymax = max(np.max(self.stepsize_w_last),np.max(self.stepsize_b_last)) * 1.2\n",
    "        plt.axis([0,xmax,ymin,ymax])\n",
    "        plt.legend()\n",
    "        if show:            \n",
    "            plt.show()    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we provide a utility class that allows to create mini-batches from the given dataset $(X,Y)$ with given batch size and initially shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches():\n",
    "    \"\"\"\n",
    "    Is initialized (constructed) with features x of shape (nx,m) and the labels y of shape (1,m).\n",
    "    Is reshuffled at construction time.\n",
    "    \n",
    "    Then, a next minibatch (MBX,MBY) with MBX of shape (nx,batchsize) and MBY of shape (1,batchsize) is provided by calling next() on the object. \n",
    "    \"\"\"    \n",
    "    def __init__(self, x, y, batchsize):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        m = x.shape[1]\n",
    "        if not batchsize:\n",
    "            self.batchsize = m\n",
    "        else:\n",
    "            self.batchsize = batchsize\n",
    "        self.n = x.shape[0]\n",
    "        self.mb = int(m/batchsize)\n",
    "        self.indices = np.arange(m)\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.ib = 0\n",
    "        \n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "        \n",
    "    def next(self):\n",
    "        it = self.indices[self.ib*self.batchsize:(self.ib+1)*self.batchsize]\n",
    "        xbatch = self.x[:,it].reshape(self.n,self.batchsize)\n",
    "        ybatch = self.y[:,it].reshape(1,self.batchsize)\n",
    "        self.ib += 1\n",
    "        return xbatch, ybatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation\n",
    "\n",
    "This function should implement the training loop - adopting mini-batch gradient descent with arbitrary batch size. Implement the given function signature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def optimize(W, b, x_train, y_train, x_test, y_test, nepochs, alpha, batchsize=32, debug=False):\n",
    "    \"\"\"\n",
    "    This function optimizes W and b by running (mini-batch) gradient descent. It starts with the given \n",
    "    weights as initial values and then iteratively updates the parameters for nepochs number of times.\n",
    "    It returns the trained parameters as dictionary (keys \"W\" and \"b\") and various quantities \n",
    "    collected during learning in form of a Metrics object. The data (x_train, etc.) is assumed to contain \n",
    "    m1 training and m2 test samples.\n",
    "    \n",
    "    Arguments:\n",
    "    W -- weights, a numpy array of size (ny,nx)\n",
    "    b -- biases, a numpy array with shape (ny,1) (with ny=10 for MNIST).\n",
    "    x_train -- input data for training of shape (nx,m1)\n",
    "    y_train -- ground-truth labels - a numpy array with shape (1,m1)\n",
    "    x_test -- input data for training of shape (nx,m2)\n",
    "    y_test -- ground-truth labels - a numpy array with shape (1,m2)\n",
    "    nepochs -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    batchsize -- batch size, defaults to 32\n",
    "    debug -- if true prints training and test error values after each epoch. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the (final) weights w and bias b\n",
    "    metrics -- contain the information about the learning curves\n",
    "    \"\"\" \n",
    "    metrics = Metrics(cost = cost)\n",
    "\n",
    "    m = x_train.shape[1] # number of samples\n",
    "    nx = x_train.shape[0] # number of input features\n",
    "    mb = int(m/batchsize) # number of mini-batches\n",
    "    print(\"Optimisation with batchsize %i and %i number of batches per epoch.\"%(batchsize,mb))\n",
    "    \n",
    "    # compute and set the initial values for the metrics curves\n",
    "    ypred_train = predict(W,b,x_train)    \n",
    "    ypred_test = predict(W,b,x_test)    \n",
    "    metrics.update_iteration(ypred_train, y_train, ypred_test, y_test, 0, 0)\n",
    "    metrics.update_epoch(0)\n",
    "    \n",
    "    # Loop over the epochs    \n",
    "    for i in range(nepochs):\n",
    "                \n",
    "        # prepare shuffled mini-batches for this epoch\n",
    "        batches = MiniBatches(x_train, y_train, batchsize)\n",
    "        \n",
    "        ### START YOUR CODE ###   \n",
    "        \n",
    "        for j in range(mb):\n",
    "            xbatch, ybatch = batches.next()\n",
    "            \n",
    "            ypred_batch = predict(W,b,xbatch)\n",
    "\n",
    "            grad = gradient(xbatch, ybatch, ypred_batch)\n",
    "\n",
    "            W -= alpha * grad[\"dW\"]\n",
    "            b -= alpha * grad[\"db\"]\n",
    "\n",
    "        ypred_train = predict(W,b,x_train)    \n",
    "        ypred_test = predict(W,b,x_test)\n",
    "        grad = gradient(x_train, y_train, ypred_train)\n",
    "        metrics.update_iteration(ypred_train, y_train, ypred_test, y_test,\n",
    "                                 np.linalg.norm(grad[\"dW\"]), np.linalg.norm(grad[\"db\"]))\n",
    "\n",
    "        ### END YOUR CODE ### \n",
    "            \n",
    "        metrics.update_epoch(i+1)\n",
    "            \n",
    "        if debug:\n",
    "            metrics.print_latest_errors()\n",
    "        \n",
    "    metrics.print_latest_costs()\n",
    "    metrics.print_latest_errors()\n",
    "\n",
    "    return {\"W\": W, \"b\": b}, metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Parameters\n",
    "\n",
    "Implement a utility to generate intialized parameters. \n",
    "\n",
    "As part of exercise 1f (below), different strategies should be considered. In a first round (exercise 1b and following) just use the first setting below (with weights and bias equals 0).\n",
    "\n",
    "1. All weights and biases set to zero: $b=0, W=0$\n",
    "2. Biases set to zero, weights generated as independent standard normal random numbers (mean zero, standard deviation 1)\n",
    "3. Biases set to zero, weights generated as independent normal random numbers with mean zero and standard deviation properly scaled (divided by $\\sqrt{n}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_params(nx, ny, method=\"zero\"):\n",
    "    \"\"\"\n",
    "    This function provides initialized parameters: a weights matrix and a bias vector. \n",
    "    \n",
    "    Argument:\n",
    "    nx -- number of input features\n",
    "    ny -- number of output dimensions (number of different labels)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized weights matrix of shape (ny,nx)\n",
    "    b -- initialized bias vector of shape (ny,1)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "\n",
    "    b = np.zeros((ny, 1))\n",
    "    \n",
    "    if method == \"standard_normal\":\n",
    "        w = np.random.normal(loc=0, scale=1, size=(ny, nx))\n",
    "    elif method == \"scaled_normal\":\n",
    "        w = np.random.normal(loc=0, scale=1/np.sqrt(nx), size=(ny, nx))\n",
    "    else:        \n",
    "        w = np.zeros((ny, nx))\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ### \n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y, shape = load_mnist(data_home)\n",
    "x_train1, x_test1, y_train, y_test = prepare_train_test(x, y, test_size=0.20)\n",
    "x_train,x_test = normalize(x_train1,x_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Training for Specific Setting and Plot Learning Curves\n",
    "\n",
    "Run the training of the model with a first setting (e.g. alpha=0.2, nepochs=20, batchsize=64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = initialize_params(28*28, 10)\n",
    "\n",
    "### START YOUR CODE ### \n",
    "\n",
    "params, metrics = optimize(W, b, x_train, y_train, x_test, y_test, nepochs=20, alpha=0.2, batchsize=64, debug=False)\n",
    "metrics.plot_cost_curves(ymin=0.0, ymax=1.0,logy=False)\n",
    "metrics.plot_error_curves(ymin=0.0, ymax=0.2,logy=False)\n",
    "metrics.print_latest_errors()\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import clear_output   \n",
    "\n",
    "def plot_hyperparam_grid(init_params_method, alphas=[0.01, 0.1, 0.5, 1], batch_sizes=[16, 32, 64, 128], nepochs=30):\n",
    "    fig, axes = plt.subplots(len(batch_sizes), len(alphas), figsize=(20, 20))\n",
    "\n",
    "    axes = axes.reshape(-1)\n",
    "\n",
    "    i = 0\n",
    "    for batch_size, alpha in itertools.product(batch_sizes, alphas):\n",
    "        plt.sca(axes[i])\n",
    "        i += 1\n",
    "\n",
    "        plt.title(f\"alpha={alpha}, batch_size={batch_size}\")\n",
    "        W,b = initialize_params(28*28, 10, method=init_params_method)\n",
    "        params, metrics = optimize(W, b, x_train, y_train, x_test, y_test, nepochs=nepochs, alpha=alpha, batchsize=batch_size, debug=False)\n",
    "        metrics.plot_error_curves(logy=False, ymin=0.0, ymax=0.2, show=False)\n",
    "\n",
    "    clear_output()\n",
    "    plt.show()\n",
    "\n",
    "plot_hyperparam_grid(init_params_method=\"zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b Explore Hyper-Parameter Settings and Describe your Findings\n",
    "\n",
    "Now run the training with different settings: \n",
    "* Different learning rate\n",
    "* Different number of epochs\n",
    "* Different batch size \n",
    "\n",
    "Explore which combination is best suited to obtain good test performance. Keep an eye on random estimates for the error rates due to random parameter initialisation and randomly shuffled mini-batches. \n",
    "\n",
    "Specify your choice of these hyper-parameters and justify why you consider your choice best suited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR FINDINGS ...\n",
    "\n",
    "A learning rate of `0.01` with `64` batch size and `30` epochs seemed to get the best results. Those parameters are reasonable because we can observe a nice convergence but also some stochasicity due to the mini-batch procedure which means that the model will be able to escape sharp narrow minima durring the optimization process. As the cost function also shows convergence, no more epochs are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b = initialize_params(28*28, 10)\n",
    "\n",
    "params, metrics = optimize(W, b, x_train, y_train, x_test, y_test, nepochs=30, alpha=0.01, batchsize=64, debug=False)\n",
    "metrics.plot_cost_curves(logy=False, ymin=0.0, ymax=0.1)\n",
    "metrics.plot_error_curves(logy=False, ymin=0.0, ymax=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c Compute the Error Rates for the individual Digits\n",
    "\n",
    "Now compute and print (or plot) the rate of misclassified images per digit (i.e. How many digits with label k are not been classified as label k).\n",
    "\n",
    "Which one seems most difficult to classify?\n",
    "\n",
    "Plot a few images of the wrongly classified images that have the label of the class that is most difficult to classify.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = params['W']\n",
    "b = params['b']\n",
    "y_pred = predict(W,b,x_test)\n",
    "\n",
    "### START YOUR CODE ### \n",
    "\n",
    "for d in range(10):\n",
    "    x_test_d = x_test[:, (y_test == d).reshape(-1)]\n",
    "    y_pred_d = y_pred[:, (y_test == d).reshape(-1)]\n",
    "    \n",
    "    misclassified = (np.argmax(y_pred_d, axis=0) != d).sum()\n",
    "    errors = misclassified / y_pred_d.shape[1]\n",
    "    print(f\"Digit {d}: {errors}% ({misclassified} of {y_pred_d.shape[1]} samples)\")\n",
    "\n",
    "misclassified = (np.argmax(y_pred, axis=0) != y_test).sum()\n",
    "errors = misclassified / y_pred.shape[1]\n",
    "print(f\"Total: {errors}% ({misclassified} of {y_pred.shape[1]} samples)\")\n",
    "    \n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digit `8` seems to be the hardest one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot some misclassified of the most difficult class:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ### \n",
    "\n",
    "d = 8\n",
    "x_test_d = x_test[:, (y_test == d).reshape(-1)]\n",
    "y_pred_d = y_pred[:, (y_test == d).reshape(-1)]\n",
    "y_test_d = y_test[:, (y_test == d).reshape(-1)]\n",
    "    \n",
    "false_classified_indices = np.argwhere((np.argmax(y_pred_d, axis=0) != y_test_d).reshape(-1)).reshape(-1)\n",
    "idx = np.random.choice(false_classified_indices, size=20)\n",
    "plot_digits(x_test_d,y_test_d, idx, shape, cols=5)\n",
    "\n",
    "\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d Analyse Wrongly Classified Images\n",
    "\n",
    "For the given best choice of hyper-parameters explore the mis-classified images.\n",
    "Select the images the model was wrong and most uncertain with and characterize which digits were most often confused and why.\n",
    "For plotting and inspecting, you can use the `plot_digits`-function defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ### \n",
    "\n",
    "false_classified_indices = np.argwhere((np.argmax(y_pred, axis=0) != y_test).reshape(-1)).reshape(-1)\n",
    "idx = np.random.choice(false_classified_indices, size=20)\n",
    "plot_digits(x_test,y_test, idx, shape, cols=5)\n",
    "\n",
    "\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the misclassified digits seem to exhibit some kind of bad handwriting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Trained Weights as Image \n",
    "\n",
    "The following cell allows you to plot the trained weights as images and the trained bias for the 10 digits. This helps to understand what the given model actually is doing. For larger (deeper) models, this won't be that easy any more. \n",
    "\n",
    "__QUESTION:__ \n",
    "* What could you tell about the predictions made by the model if one of the bias terms would be much larger than all the others (e.g. $b_5=10$ while $b_k\\in[-0.1,0.1]$ for $k\\ne5$)?\n",
    "\n",
    "  - <font style=\"color:red\">That would mean the model thinks that such a digit is generally more likely to be seen in a image, which would indicate an inbalance in the used training data.</font>\n",
    "\n",
    "* What could you tell about the predictions made by the model if one of the bias terms would be much smaller than all the others (e.g. $b_5=-10$ while $b_k\\in[-0.1,0.1]$ for $k\\ne5$)?\n",
    "\n",
    "  -  <font style=\"color:red\">That would mean the model thinks that such a digit is generally less likely to be seen in a image, which would indicate an inbalance in the used training data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = params['W']\n",
    "biases = params['b']\n",
    "cols = 5\n",
    "rows = 2\n",
    "plt.figure(figsize=(20,4*rows))\n",
    "for i in range(10):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(np.reshape(weights[i], (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit %i'%i, fontsize = 12)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(range(10), [biases[i] for i in range(10)], '+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f Analyse Weights Initialisation \n",
    "\n",
    "Implement and compare the weights initialisation strategies 1.-3.\n",
    "\n",
    "__QUESTION:__ Are there significant differences in the learning, the hyper parameter settings needed, the resulting error rates (and misclassified digits) for the different initialisation strategies? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hyperparam_grid(init_params_method=\"zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hyperparam_grid(init_params_method=\"standard_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hyperparam_grid(init_params_method=\"scaled_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like using a `standard_normal` initialisation is not as well suited for the MNIST problem as it is the case for `0` weights. The optimization process shows faster convergence using the later one. The `scaled_normal` initialisation seems to perform rather similar to `0` weights. This is most probably due to the scaling term, which is in the MNIST case $784^{-1/2}=28^{-1}$ hence the weights will be sampled from $\\sim\\mathcal N(0, 28^{-1})$. As samples from that distribution will be very close to `0`, it is reasonable that such a initialization technique performs similar to simple `0` weights. <br>\n",
    "The reason that `0` weights initialisation performs better than `standard_normal` could be that the most of the pixels in the input image are not very relevant to detect the digit. Hence their optimal weight is very close to `0`. Note that in the weights plot we created above, most of the pixels are gray which indicates not very positive nor very negative values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
