{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Load first dataset  - Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetA = pd.read_csv('ex1-system-a.csv',names=['0','1','2','3','4','5','6','7','8','9','y_true','unused'], sep = \";\")\n",
    "datasetA = datasetA.drop('unused', axis = 1) # drop the last colomn without any information in it.\n",
    "classes_name = ['0','1','2','3','4','5','6','7','8','9']\n",
    "nb_classes = 10\n",
    "datasetA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = datasetA['y_true'].values\n",
    "y_scores_A = datasetA[classes_name].values   #isolate the matrix of scores\n",
    "y_pred_A = np.argmax(y_scores_A,axis=1)      #elect winner class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) What is the overall error rate of the system ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE\n",
    "overall_rate = \n",
    "### END YOUR CODE\n",
    "\n",
    "print('overall rate of the system : ',overall_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Compute and report the confusion matrix of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a second one our selfs and ckeck if it is correct with the \"correct\" one from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix2(y_true,y_pred,nb_classes):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y_true -- groud truth labels\n",
    "    y_pred -- predicted values\n",
    "    n_classes -- number of classes\n",
    "    \n",
    "    Returns:\n",
    "    the confusion matrix as 2d numpy array \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - we use the sklearn builit in functionality\n",
    "from sklearn.metrics import confusion_matrix\n",
    "verif_cm = confusion_matrix(y_true,y_pred_A)\n",
    "\n",
    "cm_A = confusion_matrix2(y_true, y_pred_A,nb_classes)\n",
    "# verification for our confusion matrix function\n",
    "print((cm_A == verif_cm).sum() == nb_classes**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cm_A, classes=classes_name,title='Confusion matrix for System A, without normalization')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_A, classes=classes_name, normalize=True,\n",
    "                      title='Normalized confusion matrix for System B')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) What are the worst and best classes in terms of precision and sensitivity (recall) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_per_class(cm):\n",
    "    \"\"\"\n",
    "    Aguments:\n",
    "    cm -- confusion matrix\n",
    "    Returns:\n",
    "    Precision per class, i.e. a numpy array of length given by the number of classes\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE  \n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_per_class(cm):\n",
    "    \"\"\"\n",
    "    Aguments:\n",
    "    cm -- confusion matrix\n",
    "    Returns:\n",
    "    Recall per class, i.e. a numpy array of length given by the number of classes\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE  \n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = precision_per_class(cm_A)\n",
    "# should return '1' as best (95.7%)\n",
    "# and '5' as worst (81.4%) \n",
    "\n",
    "best_class = np.argmax(precisions)\n",
    "worst_class = np.argmin(precisions)\n",
    "print('best precision class  : ', best_class, '[', precisions[best_class],']')\n",
    "print('worst precision class : ', worst_class, '[', precisions[worst_class],']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = recall_per_class(cm_A)\n",
    "# should return '1' as best (97.97%)\n",
    "# and '8' as worst (79.26%) \n",
    "\n",
    "best_class = np.argmax(recalls)\n",
    "worst_class = np.argmin(recalls)\n",
    "print('best recall class  : ', best_class, '[',recalls[best_class],']')\n",
    "print('worst recall class : ', worst_class, '[',recalls[worst_class],']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Load second dataset  - Scores\n",
    "\n",
    "Find the output of a second system B. It contains the same ground truth values - but different predictions.\n",
    "What is the best system between (a) and (b) in terms of error rate and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetB = pd.read_csv('ex1-system-b.csv',names=['0','1','2','3','4','5','6','7','8','9','y_true','unused'], sep = \";\")\n",
    "datasetB = datasetB.drop('unused', axis = 1) # drop the last colomn without any information in it.\n",
    "datasetB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = datasetB['y_true'].values\n",
    "y_scores_B = datasetB[classes_name].values   #isolate the matrix of scores\n",
    "y_pred_B = np.argmax(y_scores_B,axis=1)      #elect winner class\n",
    "\n",
    "cm_B = confusion_matrix2(y_true,y_pred_B,nb_classes)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_B, classes=classes_name,title='Confusion matrix for System B, without normalization')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_B, classes=classes_name, normalize=True,\n",
    "                      title='Normalized confusion matrix for System B')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_precision(cm):\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def system_recall(cm):\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "def system_accuracy(cm):\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "def system_f1_score(cm):\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('System A\\n\\trecall : ',system_recall(cm_A),'\\n\\tprecision : ',system_precision(cm_A),'\\n\\taccuracy : ',system_accuracy(cm_A),'\\n\\tf1-score : ',f1_score(cm_A))\n",
    "print('System B\\n\\trecall : ',system_recall(cm_B),'\\n\\tprecision : ',system_precision(cm_B),'\\n\\taccuracy : ',system_accuracy(cm_B),'\\n\\tf1-score : ',f1_score(cm_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: check with the recall_score and precision_score function of SciKit Learn ```from sklearn.metrics import recall_score, precision_score``` that you obtain correct recall and precision values (use the argument ```average=\"macro\"```)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
