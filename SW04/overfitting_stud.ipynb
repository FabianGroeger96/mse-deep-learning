{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import pi,exp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data from file ex1-data-train.csv. The first two columns are x1 and x2. The last column holds the class label y. Compose suitable numpy array structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = pd.read_csv('scores_train_2.csv',names=['x1','x2','y'])\n",
    "testset = pd.read_csv('scores_test_2.csv',names=['x1','x2','y'])\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array(trainset['x1'].values)\n",
    "x2 = np.array(trainset['x2'].values)\n",
    "m = x1.size\n",
    "x_train0 = np.concatenate([x1,x2],axis=0).reshape(2,m)\n",
    "y_train = np.array(trainset['y'].values).reshape(1,m)\n",
    "\n",
    "x1 = np.array(testset['x1'].values)\n",
    "x2 = np.array(testset['x2'].values)\n",
    "x_test0 = np.concatenate([x1,x2],axis=0).reshape(2,m)\n",
    "y_test = np.array(testset['y'].values).reshape(1,m)\n",
    "\n",
    "print(\"Training Set: \", x_train0.shape, y_train.shape)\n",
    "print(\"Test Set:     \", x_test0.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y):\n",
    "    n = x.shape[1]\n",
    "    x1 = x[0,:].reshape(1,n)\n",
    "    x2 = x[1,:].reshape(1,n)\n",
    "    plot_data(x1,x2,y)\n",
    "    \n",
    "    \n",
    "def plot_data(x1,x2,y):\n",
    "    indices_pass = np.where(y==1)\n",
    "    indices_fail = np.where(y==0)\n",
    "\n",
    "    x1_pass = x1[indices_pass]\n",
    "    x2_pass = x2[indices_pass]\n",
    "    x1_fail = x1[indices_fail]\n",
    "    x2_fail = x2[indices_fail]\n",
    "    plt.scatter(x1_fail,x2_fail,marker='o',color='green',label='not admitted')\n",
    "    plt.scatter(x1_pass,x2_pass,marker='x',color='red',label='admitted')\n",
    "    plt.xlabel('Exam 1 score $x_1$')\n",
    "    plt.ylabel('Exam 2 score $x_2$')\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1))\n",
    "    axes = plt.gca()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(x_train0, y_train)\n",
    "plot_dataset(x_test0, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,mu,stdev = normalize(x_train0)\n",
    "x_test = (x_test0-mu)/stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Predictor\n",
    "\n",
    "Dummy recognition system that takes decisions randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_predictor(x):\n",
    "    rnd = np.random.uniform(size=(1,x.shape[1]))\n",
    "    return np.round(rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_rate(x):\n",
    "    n_pass = np.sum(dummy_predictor(x_train))\n",
    "    rate = n_pass/x_train.shape[1]\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the performance $N_{correct}/N$ of this system on the test set ex1-data-train.csv, with $N$ the number of test samples and $N_{correct}$ the number of correct decision in comparison to the ground truth. This dummy recognition system should have a performance of \\~50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pass_rate(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_check = [pass_rate(x_train) for i in range(1000)]\n",
    "print(np.mean(performance_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Modelling\n",
    "\n",
    "We consider different models of different complexities involving different number of parameters. All these models involve combinations of powers in $x_1,x_2$ and are of the form\n",
    "\n",
    "$\\quad g(x_1,x_2) = \\sigma(h(x_1,x_2)), \\quad h(x_1,x_2)=\\sum_{k=0}^n w_k \\phi_k(x_1,x_2)$\n",
    "\n",
    "with $\\phi_k$  multinomials in $x_1,x_2$ (i.e. combinations of powers in $x_1,x_2$). The decision boundary is then given by $h(x_1,x_2)=0$. This can be formulated by a linear model of the form $\\mathbf{W}\\cdot\\mathbf{x}$ by adding different dimensions to the input data with suitable powers of the prime input data $x_1,x_2$.\n",
    "\n",
    "Specifically, we consider the following situations:\n",
    "\n",
    "* Linear Affine: $h(x_1,x_2) = b_0 + w_1x_1 + w_2x_2$ where $\\mathbf{x}=(1,x_1,x_2)$\n",
    "\n",
    "* Quadratic: $h_2(x_1,x_2) = b_0 + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5$ where $\\mathbf{x}=(1,x_1,x_2,x_1^2,x_2^2,x_1x_2)$\n",
    "\n",
    "* etc.\n",
    "\n",
    "All the above models are linear in the parameters. We can use the same optimisation function.\n",
    "\n",
    "The method `polynomial_features` below will help you to extend the input dataset by additional dimensions up to a given polynomial order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, order):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input data as numpy array of shape (2,m) where m is the number of samples\n",
    "    order -- the max order of terms to be added (x1^j*x2^i and i+j<=order)\n",
    "    \n",
    "    Returns:\n",
    "    numpy array of shape (n,m) where n = (order+1)*(order+2)/2 (all the monomials x1^j*x2^i and i+j<=order)\n",
    "    \"\"\"\n",
    "    m = x.shape[1]\n",
    "    x1,x2 = x[0,:].reshape(1,m),x[1,:].reshape(1,m)\n",
    "    features = np.concatenate([np.ones((m),dtype='float').reshape(1,m),x1,x2]).reshape(3,m)\n",
    "    n = 3\n",
    "    if order > 1:\n",
    "        for i in range(2,order+1):\n",
    "            for term in range(i+1):\n",
    "                features = np.append(features, (x1**(i-term)*x2**term).reshape(1,m), axis=0)\n",
    "                n += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict, Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(n):\n",
    "    return np.random.normal(size=(1,n))*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary methods suited for performing the optimize-step below\n",
    "\n",
    "def predict(X,W):\n",
    "    \"\"\"\n",
    "    Computes the predicted value - given the inpute feature matrix of shape (n,m) and weights vector of shape (1,n).\n",
    "    The number of features n also includes the bias term.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "def cost(A,Y):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy cost.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- Activations\n",
    "    Y -- Labels\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    c = -np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))/m\n",
    "    return c\n",
    "\n",
    "def error_rate(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the error rate.\n",
    "\n",
    "    Arguments:\n",
    "    A -- Activations\n",
    "    Y -- Labels\n",
    "    \"\"\"\n",
    "    Ypred = np.round(A)\n",
    "    return np.sum(Y != Ypred) / Y.size\n",
    "\n",
    "def gradient_cost(A,X,Y):\n",
    "    \"\"\"\n",
    "    Computes the gradient for the cost with respect to the weights vector of size (1,n)\n",
    "    \n",
    "    Arguments:\n",
    "    A -- Activations of shape (1,m)\n",
    "    X -- Input of shape (n,m)\n",
    "    Y -- Labels of shape (1,m)\n",
    "    \n",
    "    Returns:\n",
    "    Vector of shape (n,m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(Xtrain,Ytrain,Xtest,Ytest,nepochs,learningrate):\n",
    "    \"\"\"\n",
    "    Implements (batch) gradient descent for minimizing cross-entropy cost. It returns the learning curves \n",
    "    for cost and error rate (test and training). The curves are returned as numpy array of lenghth nepochs+1 \n",
    "    (the +1 for the initial values).  \n",
    "    \n",
    "    Arguments:\n",
    "    Xtrain -- input data for training, numpy array with shape (n,m)\n",
    "    Ytrain -- labels for training, numpy array with shape (1,m)\n",
    "    Xtest -- input data for test, numpy array with shape (n,m)\n",
    "    Ytest -- labels for test, numpy array with shape (1,m)\n",
    "    nepochs -- number of epochs\n",
    "    learningrate -- learning rate\n",
    "    \n",
    "    Returns:\n",
    "    traincosts -- learning curve with the cost on the training dataset, a numpy array of shape (nepochs+1)\n",
    "    testcosts -- learning curve with the cost on the test dataset, a numpy array of shape (nepochs+1) \n",
    "    trainerror -- learning curve with the error rate on the training dataset, a numpy array of shape (nepochs+1)\n",
    "    testerror -- learning curve with the error rate on the test dataset, a numpy array of shape (nepochs+1)\n",
    "    W -- parameter vector, a numpy array of shape (1,n+1)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_train,y_train,x_test,y_test,pol_degree, nepochs, learningrate):\n",
    "    \"\"\"\n",
    "    Evaluate a model by training it, plotting the learning curves and the decision boundary and \n",
    "    returning the performance (final cost and error rate obtained for training and test set)\n",
    "    \"\"\"\n",
    "    Xtrain = polynomial_features(x_train, pol_degree)\n",
    "    Xtest = polynomial_features(x_test,pol_degree)\n",
    "    traincosts, testcosts, trainerror, testerror, W = optimize(Xtrain,y_train,Xtest,y_test,nepochs,learningrate)\n",
    "    plot_curves(traincosts, testcosts, trainerror, testerror)\n",
    "    Jtrain, Jtest, etrain, etest = traincosts[-1],testcosts[-1],trainerror[-1],testerror[-1]\n",
    "    print(Jtrain, Jtest, etrain, etest)\n",
    "    print(W)\n",
    "    return Jtrain, Jtest, etrain, etest, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(traincosts, testcosts, trainerror, testerror):\n",
    "    iterations = range(traincosts.size)\n",
    "    f = plt.figure(figsize=(10,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(iterations, traincosts,label=\"train\")\n",
    "    plt.plot(iterations, testcosts, label=\"test\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(iterations, trainerror, label=\"train\")\n",
    "    plt.plot(iterations, testerror, label=\"test\")\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(x, y, W, pol_degree):\n",
    "    x1 = x[0,:]\n",
    "    x2 = x[1,:]\n",
    "    indices_pass = np.where(y[0,:]==1)\n",
    "    indices_fail = np.where(y[0,:]==0)\n",
    "\n",
    "    x1_pass = x1[indices_pass]\n",
    "    x2_pass = x2[indices_pass]\n",
    "    x1_fail = x1[indices_fail]\n",
    "    x2_fail = x2[indices_fail]\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(x1_fail,x2_fail,marker='o',color='green',label='not admitted')\n",
    "    ax.scatter(x1_pass,x2_pass,marker='x',color='red',label='admitted')\n",
    "    plt.xlabel('Exam 1 score $x_1$')\n",
    "    plt.ylabel('Exam 2 score $x_2$')\n",
    "    ax.legend(bbox_to_anchor=(1.1, 1))\n",
    "    axes = plt.gca()\n",
    "\n",
    "    x1_min, x1_max = x1.min() - 1, x1.max() + 1\n",
    "    x2_min, x2_max = x2.min() - 1, x2.max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, (x1_max-x1_min)/100), np.arange(x2_min, x2_max, (x2_max-x2_min)/100))\n",
    "    xx10 = xx1.reshape(1,xx1.size)\n",
    "    xx20 = xx2.reshape(1,xx2.size)\n",
    "    xx = np.concatenate((xx10, xx20), axis=0) \n",
    "    yy = W.dot(polynomial_features(xx, pol_degree)).reshape(xx1.shape)\n",
    "    \n",
    "    ax.contour(xx1, xx2, yy, levels=[0], cmap=plt.cm.Paired)\n",
    "    #ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the different models\n",
    "\n",
    "Evaluate different polynomial models of the form as described above (starting with linear of order=1, then proceeding to quadratic of order=2 and to higher order models).\n",
    "\n",
    "Use the \"evaluate\" function above that will provide also some diagnostic plot. Carefully tune the inputs such as the nepcohs and learning rate - do this for each selected model. Inspect the learning curves to judge whether the training has converged.\n",
    "\n",
    "Remember the error rates for training set and test for the different models and create a plot showing the error rates at different model complexity.\n",
    "\n",
    "Describe what you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pol_degree = 1\n",
    "nepochs = 1000\n",
    "learningrate = 0.1\n",
    "\n",
    "traincost1, testcost1, trainerror1, testerror1, W1 = evaluate(x_train,y_train,x_test,y_test,pol_degree, nepochs, learningrate)\n",
    "plot_decision_boundary(x_train, y_train, W1, pol_degree)\n",
    "plot_decision_boundary(x_test, y_test, W1, pol_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
