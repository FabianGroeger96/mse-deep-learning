{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with sklearn\n",
    "\n",
    "The goal of this exercise is to \n",
    "* explore some of the sklearn functionality for training a MLP classifier (see https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)  \n",
    "* by using cross validation \n",
    "* learn how to compute the confusion matrix and its derived quantities and how to interpret them\n",
    "* explore the test error as a function of the complexity (number of units, number of layers)\n",
    "* explore the impact of L2 regularisation\n",
    "\n",
    "__IMPORTANT REMARK__: We here follow the convention of sklearn to enumerate the samples with the first index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/tmp/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_train,x_test):\n",
    "    \"\"\"\n",
    "    Normalizes the pixels values of the images - mean and stdev are computed from the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    x_train -- Array of training samples of shape (n,m1) where n,m1 are the number of features and samples, respectively.  \n",
    "    x_test -- Array of test samples of shape (n,m2) where n,m2 are the number of features and samples, respectively. \n",
    "    \n",
    "    Returns:\n",
    "    The arrays with the normalized train and test samples.  \n",
    "    \"\"\"\n",
    "    mean = np.mean(x_train)\n",
    "    std = np.std(x_train)\n",
    "    x_train -= mean\n",
    "    x_test -= mean\n",
    "    x_train /= std\n",
    "    x_test /= std\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = fetch_openml('mnist_784', data_home=datadir, return_X_y=True)\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=10000, random_state=1)\n",
    "x_train, x_test = normalize(x_train0, x_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Model Family and learn how to compute the metrics\n",
    "\n",
    "#### Model\n",
    "Use the functionality of scikit learn to configure a MLP and its training procedure with\n",
    "* hidden layers: 0-2 layers with suitable number of units per layer\n",
    "* mini-batch gradient descent with given batch_size (no advanced optimisers)\n",
    "* constant learning rate (no learning rate schedules)\n",
    "* number of epochs\n",
    "* no regularisation such as L2 penalty or early stopping\n",
    "\n",
    "#### Metrics\n",
    "Compute the train and test error resp. accuracy as well as the class precision, recall, f1-score.\n",
    "\n",
    "__See__:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "* https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Training Run\n",
    "\n",
    "Run the training and plot the training loss with a first set of values:\n",
    "* no hidden layers\n",
    "* mini-batchsize: 64\n",
    "* learning rate: 0.1\n",
    "* 100 epochs\n",
    "\n",
    "Compute the Metrics.\n",
    "Which digits are hard to predict?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Hyperparameters\n",
    "hidden_layer_sizes = ()\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "nepochs = 100\n",
    "\n",
    "# Regularisation:\n",
    "alpha = 0.0 # L2 regularisation constant\n",
    "early_stopping = False\n",
    "n_iter_no_change = 10\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Model instantiation and training\n",
    "\n",
    "cls = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    alpha=alpha, batch_size=batch_size,\n",
    "    learning_rate_init=learning_rate,\n",
    "    early_stopping=early_stopping,\n",
    "    n_iter_no_change=n_iter_no_change,\n",
    "    max_iter=nepochs)\n",
    "\n",
    "cls.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "\n",
    "\n",
    "plt.plot(range(len(cls.loss_curve_)), cls.loss_curve_)\n",
    "\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# train and test error, accuracy\n",
    "# per class accuracy, precision, f1 score\n",
    "y_pred = cls.predict(x_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(f1_score(y_val, y_pred, average=\"macro\"))\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model without Hidden Layer\n",
    "\n",
    "By first varying just the parameters \n",
    "* mini-batchsize\n",
    "* learning rate\n",
    "* epochs\n",
    "\n",
    "with adding any hidden layer.\n",
    "\n",
    "Summarize what the best combination of the abover hyper-parameters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def explore_basic_hyperparams(\n",
    "        hidden_layer_size,\n",
    "        early_stopping = False,\n",
    "        n_iter_no_change = 10,\n",
    "        alphas = [0.0], # L2 regularisation constant\n",
    "        batch_sizes = [16, 32, 64, 128],\n",
    "        learning_rates = [0.001, 0.01, 0.1, 1],\n",
    "        nepochss = [100, 200]):\n",
    "    f1s = []\n",
    "    params = list(itertools.product(batch_sizes, learning_rates, nepochss, alphas))\n",
    "\n",
    "    i = 0\n",
    "    for batch_size, learning_rate, nepochs, alpha in params:\n",
    "        print(batch_size, \"; \", learning_rate, \"; \", nepochs, \"; \", alpha, \"; \", f\" ({i} / {len(params)})\", end=\"\\r\")\n",
    "        i += 1\n",
    "        # Model instantiation and training\n",
    "\n",
    "        cls = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_size,\n",
    "            alpha=alpha, batch_size=batch_size,\n",
    "            learning_rate_init=learning_rate,\n",
    "            early_stopping=early_stopping,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            max_iter=nepochs)\n",
    "\n",
    "        cls.fit(x_train, y_train)\n",
    "        y_pred = cls.predict(x_val)\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "        f1s.append(f1)\n",
    "\n",
    "    max_idx = np.argmax(f1s)\n",
    "    print(f\"best params are {params[max_idx]} with f1 of {f1s[max_idx]}\")\n",
    "    return params[max_idx], f1s[max_idx], f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Keep hidden_layer_sizes = () \n",
    "# Vary the following\n",
    "params, f1, f1s = explore_basic_hyperparams(hidden_layer_size=())\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLPClassifier(\n",
    "    hidden_layer_sizes=(),\n",
    "    alpha=0.0, batch_size=64,\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=False,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=100)\n",
    "\n",
    "cls.fit(x_train, y_train)\n",
    "\n",
    "y_pred = cls.predict(x_train)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "y_pred = cls.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f1_score(y_val, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (no hidden layer)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "train / validation error : 6% / 8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding one Hidden layer\n",
    "\n",
    "Explore the performance of the model by varying the parameters \n",
    "* mini-batchsize\n",
    "* learning rate\n",
    "* epochs\n",
    "* complexity (number of units in the one hidden layer)\n",
    "\n",
    "For given complexity, summarize what the best combination of other hyper-parameters is - compute this for several complexities.\n",
    "\n",
    "Compute also the \"best\" train and validation error (or accuracy) for given complexity - as a function of the complexity and plot the curve (for selected number of units - e.g. 10 different values). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Keep hidden_layer_sizes = () \n",
    "# Vary the following\n",
    "\n",
    "hidden_layer_sizes = [(int(x),) for x in np.linspace(10, 1000, 10)] # just one layer \n",
    "\n",
    "f1s = []\n",
    "\n",
    "i = 0\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    print(f\"working on {hidden_layer_size}, ({i}/{len(hidden_layer_sizes)})\")\n",
    "    i += 1\n",
    "    params, f1, _ = explore_basic_hyperparams(\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        batch_sizes = [64],\n",
    "        learning_rates = [0.001],\n",
    "        nepochss = [100]\n",
    "    )\n",
    "    \n",
    "    f1s.append(f1)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error vs Complexity__:\n",
    "\n",
    "Plot with the train and test error vs complexity (number of units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(10, 1000, 10), f1s)\n",
    "\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLPClassifier(\n",
    "    hidden_layer_sizes=(780,),\n",
    "    alpha=0.0, batch_size=64,\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=False,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=100)\n",
    "\n",
    "cls.fit(x_train, y_train)\n",
    "\n",
    "y_pred = cls.predict(x_train)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "y_pred = cls.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(f1_score(y_val, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (one hidden layer)\n",
    "\n",
    "hidden_layer_sizes = (780,)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "train / validation error : 0% / 2%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Regularisation\n",
    "\n",
    "Explore the Impact of Using L2 Regularisation (still adding just one hidden layer) again by varying mini-batchsize, learning rate, epochs, complexity.\n",
    "\n",
    "Can you reach a better best model performance (on validation set)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Vary the following\n",
    "\n",
    "hidden_layer_sizes = [(int(x),) for x in np.linspace(10, 1000, 10)] # just one layer \n",
    "\n",
    "f1s = []\n",
    "\n",
    "i = 0\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    print(f\"working on {hidden_layer_size}, ({i}/{len(hidden_layer_sizes)})\")\n",
    "    i += 1\n",
    "    params, f1, _ = explore_basic_hyperparams(\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        alphas=[0.1],\n",
    "        batch_sizes = [64],\n",
    "        learning_rates = [0.001],\n",
    "        nepochss = [100]\n",
    "    )\n",
    "    \n",
    "    f1s.append(f1)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error vs Complexity__:\n",
    "\n",
    "Plot with the train and test error vs complexity (number of units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(10, 1000, 10), f1s)\n",
    "\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLPClassifier(\n",
    "    hidden_layer_sizes=(780),\n",
    "    alpha=0.1, batch_size=64,\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=False,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=100)\n",
    "\n",
    "cls.fit(x_train, y_train)\n",
    "\n",
    "y_pred = cls.predict(x_train)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "y_pred = cls.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(f1_score(y_val, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (one hidden layer)\n",
    "\n",
    "hidden_layer_sizes = (780)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.001 \n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "alpha =  .1 # L2 regularisation constant\n",
    "\n",
    "train / validation error : 2% / 3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding up to 3 Hidden Layers\n",
    "\n",
    "Now consider using a model with more than one hidden layer (at max 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Vary the following\n",
    "\n",
    "hidden_layer_sizes =  [(int(x),) for x in np.linspace(10, 500, 5)] # single\n",
    "hidden_layer_sizes += [(int(x),int(x)) for x in np.linspace(10, 500, 5)] # double\n",
    "hidden_layer_sizes += [(int(x),int(x), int(x)) for x in np.linspace(10, 500, 5)] # trible\n",
    "\n",
    "f1s = []\n",
    "\n",
    "i = 0\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    print(f\"working on {hidden_layer_size}, ({i}/{len(hidden_layer_sizes)})\")\n",
    "    i += 1\n",
    "    params, f1, _ = explore_basic_hyperparams(\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        alphas=[0, 0.1],\n",
    "        batch_sizes = [64],\n",
    "        learning_rates = [0.001],\n",
    "        nepochss = [100]\n",
    "    )\n",
    "    \n",
    "    f1s.append(f1)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error vs Complexity__:\n",
    "\n",
    "Plot with the train and test error vs complexity (number of units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "\n",
    "plt.title(\"single layer\")\n",
    "plt.plot([np.prod(y) for y in hidden_layer_sizes][:5], f1s[:5])\n",
    "plt.xlabel(\"parameters\")\n",
    "plt.ylabel(\"f1\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"two layers\")\n",
    "plt.plot([np.prod(y) for y in hidden_layer_sizes][5:10], f1s[5:10])\n",
    "plt.xlabel(\"parameters\")\n",
    "plt.ylabel(\"f1\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"three layers\")\n",
    "plt.plot([np.prod(y) for y in hidden_layer_sizes][10:], f1s[10:])\n",
    "plt.xlabel(\"parameters\")\n",
    "plt.ylabel(\"f1\")\n",
    "plt.show()\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLPClassifier(\n",
    "    hidden_layer_sizes=(377, 377, 377),\n",
    "    alpha=0.0, batch_size=64,\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=False,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=100)\n",
    "\n",
    "cls.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = cls.predict(x_train)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "y_pred = cls.predict(x_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(f1_score(y_val, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (1-3 hidden layers)\n",
    "\n",
    "hidden_layer_sizes = (377, 377, 377)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "alpha =  0.0 # L2 regularisation constant\n",
    "\n",
    "train / validation error : 0% / 2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Performance of Best Model\n",
    "\n",
    "Test Error: 2% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLPClassifier(\n",
    "    hidden_layer_sizes=(780,),\n",
    "    alpha=0.0, batch_size=64,\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=False,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=100)\n",
    "\n",
    "cls.fit(x_train, y_train)\n",
    "\n",
    "y_pred = cls.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
