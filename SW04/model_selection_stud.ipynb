{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with sklearn\n",
    "\n",
    "The goal of this exercise is to \n",
    "* explore some of the sklearn functionality for training a MLP classifier (see https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)  \n",
    "* by using cross validation \n",
    "* learn how to compute the confusion matrix and its derived quantities and how to interpret them\n",
    "* explore the test error as a function of the complexity (number of units, number of layers)\n",
    "* explore the impact of L2 regularisation\n",
    "\n",
    "__IMPORTANT REMARK__: We here follow the convention of sklearn to enumerate the samples with the first index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_train,x_test):\n",
    "    \"\"\"\n",
    "    Normalizes the pixels values of the images - mean and stdev are computed from the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    x_train -- Array of training samples of shape (n,m1) where n,m1 are the number of features and samples, respectively.  \n",
    "    x_test -- Array of test samples of shape (n,m2) where n,m2 are the number of features and samples, respectively. \n",
    "    \n",
    "    Returns:\n",
    "    The arrays with the normalized train and test samples.  \n",
    "    \"\"\"\n",
    "    mean = np.mean(x_train)\n",
    "    std = np.std(x_train)\n",
    "    x_train -= mean\n",
    "    x_test -= mean\n",
    "    x_train /= std\n",
    "    x_test /= std\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you have trouble with the fetch_openml, use this code\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = fetch_openml('mnist_784', data_home=datadir, return_X_y=True)\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x, y, test_size=10000, random_state=1)\n",
    "x_train, x_test = normalize(x_train0, x_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Model Family and learn how to compute the metrics\n",
    "\n",
    "#### Model\n",
    "Use the functionality of scikit learn to configure a MLP and its training procedure with\n",
    "* hidden layers: 0-2 layers with suitable number of units per layer\n",
    "* mini-batch gradient descent with given batch_size (no advanced optimisers)\n",
    "* constant learning rate (no learning rate schedules)\n",
    "* number of epochs\n",
    "* no regularisation such as L2 penalty or early stopping\n",
    "\n",
    "#### Metrics\n",
    "Compute the train and test error resp. accuracy as well as the class precision, recall, f1-score.\n",
    "\n",
    "__See__:\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "* https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Training Run\n",
    "\n",
    "Run the training and plot the training loss with a first set of values:\n",
    "* no hidden layers\n",
    "* mini-batchsize: 64\n",
    "* learning rate: 0.1\n",
    "* 100 epochs\n",
    "\n",
    "Compute the Metrics.\n",
    "Which digits are hard to predict?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Basic Hyperparameters\n",
    "hidden_layer_sizes = ()\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "nepochs = 100\n",
    "\n",
    "# Regularisation:\n",
    "alpha = 0.0 # L2 regularisation constant\n",
    "early_stopping = False\n",
    "n_iter_no_change = 10\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Model instantiation and training\n",
    "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=learning_rate,\n",
    "                    max_iter=nepochs,\n",
    "                    alpha=alpha,\n",
    "                    early_stopping=early_stopping,\n",
    "                    n_iter_no_change=n_iter_no_change,\n",
    "                    solver='sgd')\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.title('Loss curve')\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "# train and test error, accuracy\n",
    "# per class accuracy, precision, f1 score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred_train = mlp.predict(x_train)\n",
    "y_pred_test = mlp.predict(x_test)\n",
    "\n",
    "print('Train Set:')\n",
    "print('Accuracy Score: %.2f' % accuracy_score(y_train, y_pred_train))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "print('\\n')\n",
    "print('Test Set:')\n",
    "print('Accuracy Score: %.2f' % accuracy_score(y_test, y_pred_test))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model without Hidden Layer\n",
    "\n",
    "By first varying just the parameters \n",
    "* mini-batchsize\n",
    "* learning rate\n",
    "* epochs\n",
    "\n",
    "with adding any hidden layer.\n",
    "\n",
    "Summarize what the best combination of the abover hyper-parameters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Keep hidden_layer_sizes = () \n",
    "# Vary the following\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "nepochs = 100\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(),\n",
    "                    learning_rate='constant')\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1,],\n",
    "    'max_iter': [25, 50, 100],\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(mlp, parameters, \n",
    "                        n_jobs=-1, cv=5, \n",
    "                        verbose=2, return_train_score=True)\n",
    "grid_clf.fit(x_train, y_train)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n %s' % grid_clf.best_params_)\n",
    "\n",
    "print('Scores:')\n",
    "means = grid_clf.cv_results_['mean_test_score']\n",
    "stds = grid_clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_clf.cv_results_['params']):\n",
    "    print(\"test err: %0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train error: %.3f (+/-%0.03f)' % (grid_clf.cv_results_['mean_train_score'].mean(),\n",
    "                                        grid_clf.cv_results_['std_train_score'].mean()))\n",
    "print('Test error: %.3f (+/-%0.03f)' % (grid_clf.cv_results_['mean_test_score'].mean(),\n",
    "                                        grid_clf.cv_results_['std_test_score'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (no hidden layer)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "nepochs = 50\n",
    "\n",
    "train / validation error : 0.91/0.89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding one Hidden layer\n",
    "\n",
    "Explore the performance of the model by varying the parameters \n",
    "* mini-batchsize\n",
    "* learning rate\n",
    "* epochs\n",
    "* complexity (number of units in the one hidden layer)\n",
    "\n",
    "For given complexity, summarize what the best combination of other hyper-parameters is - compute this for several complexities.\n",
    "\n",
    "Compute also the \"best\" train and validation error (or accuracy) for given complexity - as a function of the complexity and plot the curve (for selected number of units - e.g. 10 different values). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Keep hidden_layer_sizes = () \n",
    "# Vary the following\n",
    "\n",
    "hidden_layer_sizes = (100,) # just one layer \n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "nepochs = 100\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(),\n",
    "                    learning_rate='constant')\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': [128, 256],\n",
    "    'learning_rate_init': [0.01],\n",
    "    'max_iter': [50],\n",
    "    'hidden_layer_sizes': [(int(x),) for x in np.linspace(10, 1000, 10)],\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(mlp, parameters, \n",
    "                        n_jobs=-1, cv=5, \n",
    "                        verbose=2, return_train_score=True)\n",
    "grid_clf.fit(x_train, y_train)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n %s' % grid_clf.best_params_)\n",
    "\n",
    "print('Scores:')\n",
    "means = grid_clf.cv_results_['mean_test_score']\n",
    "stds = grid_clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_clf.cv_results_['params']):\n",
    "    print(\"test err: %0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train error: %.3f (+/-%0.03f)' % (grid_clf.cv_results_['mean_train_score'].mean(),\n",
    "                                        grid_clf.cv_results_['std_train_score'].mean()))\n",
    "print('Test error: %.3f (+/-%0.03f)' % (grid_clf.cv_results_['mean_test_score'].mean(),\n",
    "                                        grid_clf.cv_results_['std_test_score'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error vs Complexity__:\n",
    "\n",
    "Plot with the train and test error vs complexity (number of units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Train/Test Scores Mean\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "    \n",
    "    scores_mean_train = cv_results['mean_train_score']\n",
    "    scores_mean_train = np.array(scores_mean_train).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val) + ' (test)')\n",
    "        ax.plot(grid_param_1, scores_mean_train[idx,:], '-.o', label= name_param_2 + ': ' + str(val) + ' (train)')\n",
    "\n",
    "    ax.set_title(\"Error vs. Complexity\")\n",
    "    ax.set_xlabel(name_param_1)\n",
    "    ax.set_ylabel('CV Average Score')\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid('on')\n",
    "\n",
    "plot_grid_search(grid_clf.cv_results_, [(50,), (100,), (200,)], [128, 256], 'N. of hidden layers', 'Batch size')\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (one hidden layer)\n",
    "\n",
    "hidden_layer_sizes = (200,)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "nepochs = 50\n",
    "\n",
    "train / validation error : 0.984 (+/-0.002) / 0.956 (+/-0.002)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Regularisation\n",
    "\n",
    "Explore the Impact of Using L2 Regularisation (still adding just one hidden layer) again by varying mini-batchsize, learning rate, epochs, complexity.\n",
    "\n",
    "Can you reach a better best model performance (on validation set)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Vary the following\n",
    "\n",
    "# Basic Hyperparameters\n",
    "hidden_layer_sizes = (100,)\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "nepochs = 100\n",
    "\n",
    "# Regularisation:\n",
    "alpha = 0.0 # L2 regularisation constant\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(),\n",
    "                    learning_rate='constant')\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': [128, 256],\n",
    "    'learning_rate_init': [0.01],\n",
    "    'max_iter': [50],\n",
    "    'hidden_layer_sizes': [(int(x),) for x in np.linspace(10, 1000, 10)],\n",
    "    'alpha': [0.0, 0.1, 0.01],\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(mlp, parameters, \n",
    "                        n_jobs=-1, cv=5, \n",
    "                        verbose=2, return_train_score=True)\n",
    "grid_clf.fit(x_train, y_train)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n %s' % grid_clf.best_params_)\n",
    "\n",
    "print('Scores:')\n",
    "means = grid_clf.cv_results_['mean_test_score']\n",
    "stds = grid_clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_clf.cv_results_['params']):\n",
    "    print(\"test err: %0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error vs Complexity__:\n",
    "\n",
    "Plot with the train and test error vs complexity (number of units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "plot_grid_search(grid_clf.cv_results_, [0.0, 0.1, 0.01], [128, 256], 'Alpha', 'Batch size')\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (one hidden layer)\n",
    "\n",
    "hidden_layer_sizes = (200,)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "nepochs = 50\n",
    "\n",
    "alpha =  0.0 # L2 regularisation constant\n",
    "\n",
    "train / validation error : 0.968 (+/-0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding up to 3 Hidden Layers\n",
    "\n",
    "Now consider using a model with more than one hidden layer (at max 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Vary the following\n",
    "\n",
    "# Basic Hyperparameters\n",
    "hidden_layer_sizes = (100,0,0)\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "nepochs = 100\n",
    "\n",
    "# Regularisation:\n",
    "alpha = 0.0 # L2 regularisation constant\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(),\n",
    "                    learning_rate='constant')\n",
    "\n",
    "hidden_layer_sizes =  [(int(x),) for x in np.linspace(10, 500, 5)] # single\n",
    "hidden_layer_sizes += [(int(x),int(x)) for x in np.linspace(10, 500, 5)] # double\n",
    "hidden_layer_sizes += [(int(x),int(x), int(x)) for x in np.linspace(10, 500, 5)] # trible\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': [128, 256],\n",
    "    'learning_rate_init': [0.01],\n",
    "    'max_iter': [50],\n",
    "    'hidden_layer_sizes': hidden_layer_sizes,\n",
    "    'alpha': [0.0],\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(mlp, parameters, \n",
    "                        n_jobs=-1, cv=5, \n",
    "                        verbose=2, return_train_score=True)\n",
    "grid_clf.fit(x_train, y_train)\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n %s' % grid_clf.best_params_)\n",
    "\n",
    "print('Scores:')\n",
    "means = grid_clf.cv_results_['mean_test_score']\n",
    "stds = grid_clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_clf.cv_results_['params']):\n",
    "    print(\"test err: %0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error vs Complexity__:\n",
    "\n",
    "Plot with the train and test error vs complexity (number of units in the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "plot_grid_search(grid_clf.cv_results_, [(50,50,50), (100,100,100)], [128, 256], 'N. of hidden layers', 'Batch size')\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BEST MODEL__ (1-3 hidden layers)\n",
    "\n",
    "hidden_layer_sizes = (100, 100, 100)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "learning_rate = 0.0\n",
    "\n",
    "nepochs = 50\n",
    "\n",
    "alpha =  0.0 # L2 regularisation constant\n",
    "\n",
    "train / validation error : 0.968 (+/-0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Performance of Best Model\n",
    "\n",
    "Test Error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = grid_clf.best_estimator_.predict(x_train)\n",
    "y_pred_test = grid_clf.best_estimator_.predict(x_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print('Train acc: %.3f' % acc_train)\n",
    "print('Test acc: %.3f' % acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
