{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last revision: Martin Melchior - 25.02.2021\n",
    "\n",
    "In this exercise you implement a _Rosenblatt Perceptron_ and the _Perceptron Learning Rule_. Then you apply it to linearly separable data. \n",
    "\n",
    "In a first step, you apply it to a dummy test data set, a **low-dimensional dataset** that is generated on the fly. And you can convince yourself that your system is properly implemented and has found the separating line. \n",
    "\n",
    "In a second (optional) part, you can apply it to a somewhat more advanced dataset, the **lightweight MNIST dataset** which consists of small (8x8) images, i.e. 64 dimensional arrays. Here, the linear separability can no longer be easily visualized. \n",
    "\n",
    "Some emphasis should be given to properly handle numpy arrays. These will be much more extensively used in upcoming exercises of later weeks. So, we recommend to take a serious glance at them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Low-Dimensional Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "Instead of providing a fixed input dataset, we here generate it randomly.\n",
    "For easier comparison, we want to make sure that the same data is produced. Therefore, we set a random seed (set to 1 below).\n",
    "\n",
    "The data will be generated in form of a 2d array, the first index enumerating the dimensions (rows, in the 2d case index 0 and 1), the second enumerating the samples (columns). \n",
    "\n",
    "Furthermore, we provide a suitable plotting utility that allows you to inspect the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_data(m,m1,a,s,width=0.6,eps=0.5, seed=1):\n",
    "    \"\"\"\n",
    "    Generates a random linearly separable 2D test set and associated labels (0|1).\n",
    "    The x-values are distributed in the interval [-0.5,0.5]. \n",
    "    With the parameters a,s you can control the line that separates the two classes. \n",
    "    This turns out to be the line with the widest corridor between the two classes (with width 'width').\n",
    "    If the random seed is set, the set will always look the same for given input parameters. \n",
    "    \n",
    "    Arguments:\n",
    "    a -- y-intercept of the seperating line\n",
    "    s -- slope of the separating line\n",
    "    m -- number of samples\n",
    "    m1 -- number of samples labelled with '1'\n",
    "    width -- width of the corridor between the two classes\n",
    "    eps -- measure for the variation of the samples in x2-direction\n",
    "    \n",
    "    Returns:\n",
    "    x -- generated 2D data of shape (2,n)\n",
    "    y -- labels (0 or 1) of shape (1,n)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(m, m1, replace=False)\n",
    "    y = np.zeros(m, dtype=int).reshape(1,m)\n",
    "    y[0,idx] = 1\n",
    "    \n",
    "    x = np.random.rand(2,m).reshape(2,m) # random numbers uniformly distributed in [0,1]\n",
    "    x[0,:]-= 0.5\n",
    "    idx1 = y[0,:]==1\n",
    "    idx2 = y[0,:]==0\n",
    "    x[1,idx1] = (a+s*x[0,idx1]) + (width/2+eps*x[1,idx1])\n",
    "    x[1,idx2] = (a+s*x[0,idx2]) - (width/2+eps*x[1,idx2])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def line(a, s, n=100):\n",
    "    \"\"\"    \n",
    "    Returns a line 2D array with x and y=a+s*x.\n",
    "    \n",
    "    Parameters:\n",
    "    a -- intercept\n",
    "    s -- slope\n",
    "    n -- number of points\n",
    "    \n",
    "    Returns:\n",
    "    2d array of shape (n,2) \n",
    "    \"\"\"\n",
    "    x = np.linspace(-0.5, 0.5, n)\n",
    "    l = np.array([x,a+s*x]).reshape(2,n)\n",
    "    return l\n",
    "\n",
    "def plot(x, y, params_best=None, params_before=None, params_after=None, misclassified=None, selected=None):\n",
    "    \"\"\"\n",
    "    Plot the 2D data provided in form of the x-array. \n",
    "    Use markers depending on the label ('1 - red cross, 0 - blue cross').\n",
    "    Optionally, you can pass tuples with parameters for a line (a: y-intercept, s: slope) \n",
    "    * params_best: ideal separating line (green dashed) \n",
    "    * params: predicted line (magenta)\n",
    "    Finally, you can also mark single points:\n",
    "    * misclassified: array of misclassified points (blue circles)\n",
    "    * selected: array of selected points (green filled circles)\n",
    "    \n",
    "    Parameters:\n",
    "    x -- 2D input dataset of shape (2,n)\n",
    "    y -- ground truth labels of shape (1,n)\n",
    "    params_best -- parameters for the best separating line\n",
    "    params -- any line parameters\n",
    "    misclassified -- array of points to be marked as misclassified\n",
    "    selected -- array of points to be marked as selected\n",
    "    \"\"\"\n",
    "    idx1 = y[0,:]==1\n",
    "    idx2 = y[0,:]==0\n",
    "    plt.plot(x[0,idx1], x[1,idx1], 'r+', label=\"label 1\")\n",
    "    plt.plot(x[0,idx2], x[1,idx2], 'b+', label=\"label 0\")    \n",
    "    if not params_best is None:\n",
    "        a = params_best[0]\n",
    "        s = params_best[1]\n",
    "        l = line(a,s)\n",
    "        plt.plot(l[0,:], l[1,:], 'g--')\n",
    "    if not params_before is None:\n",
    "        a = params_before[0]\n",
    "        s = params_before[1]\n",
    "        l = line(a,s)\n",
    "        plt.plot(l[0,:], l[1,:], 'm--')\n",
    "    if not params_after is None:\n",
    "        a = params_after[0]\n",
    "        s = params_after[1]\n",
    "        l = line(a,s)\n",
    "        plt.plot(l[0,:], l[1,:], 'm-')\n",
    "    if not misclassified is None:\n",
    "        plt.plot(x[0,misclassified], x[1,misclassified], 'o', label=\"misclassified\")\n",
    "    if not selected is None:\n",
    "        plt.plot(x[0,selected], x[1,selected], 'oy', label=\"selected\")\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Plot the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = prepare_data(200,100,0,0.5,width=0.3,eps=0.5, seed=1)\n",
    "plot(x, y, params_before=(0,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for the decision boundary\n",
    "\n",
    "Here, you should implement a function that translates the weights vector $(w_1,w_2)$ and the bias $b$ into parameters of a straight line ( $x_2 = a + s \\cdot x_1$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lineparams(weight, bias):\n",
    "    \"\"\"\n",
    "    Translates the weights vector and the bias into line parameters with a x2-intercept 'a' and a slope 's'.\n",
    "\n",
    "    Parameters:\n",
    "    weight -- weights vector of shape (1,2)\n",
    "    bias -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    a -- x2-intercept\n",
    "    s -- slope of the line in the (x1,x2)-plane\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    w1, w2 = np.squeeze(weight)\n",
    "    \n",
    "    a = - bias / w2\n",
    "    s = - w1 / w2\n",
    "    ### END YOUR CODE ###\n",
    "    return a,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Perceptron Learning Algorithm\n",
    "\n",
    "by implementing the functions\n",
    "* predict\n",
    "* update\n",
    "* select_datapoint\n",
    "* train\n",
    "\n",
    "Follow the descriptions of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(x,w,b):\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a perceptron (single LTU).\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input dataset of shape (2,m)\n",
    "    w -- weights vector of shape (1,2)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    y -- prediction of a perceptron (single LTU) of shape (1,m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    y = (np.dot(w, x) + b) >= 0\n",
    "    ### END YOUR CODE ###\n",
    "    return y\n",
    "\n",
    "def update(x,y,w,b,alpha=1.0):\n",
    "    \"\"\"\n",
    "    Performs an update step in accordance with the perceptron learning algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input data point of shape (2,1)\n",
    "    y -- true label ('ground truth') for the specified point\n",
    "    w -- weight vector of shape (1,2)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    w1 -- updated weight vector\n",
    "    b1 -- updated bias\n",
    "    \"\"\"\n",
    "    ypred = predict(x,w,b)\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    w1 = w - alpha * (ypred - y) * x\n",
    "    b1 = b - alpha * (ypred - y)\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return w1, b1\n",
    "\n",
    "\n",
    "def select_datapoint(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Identifies the misclassified data points and selects one of them.\n",
    "    In case all datapoints are correctly classified None is returned. \n",
    "\n",
    "    Parameters:\n",
    "    x -- input dataset of shape (2,m)\n",
    "    y -- ground truth labels of shape (1,m)\n",
    "    w -- weights vector of shape (1,2)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    x1 -- one of the wrongly classified datapoint (of shape (2,1))\n",
    "    y1 -- the associated true label\n",
    "    misclasssified -- array with indices of wrongly classified datapoints or empty array\n",
    "    \"\"\"\n",
    "    ypred = predict(x,w,b)\n",
    "    wrong_mask = (ypred != y)[0]\n",
    "    misclassified = np.where(wrong_mask)[0]\n",
    "    if len(misclassified)>0:\n",
    "        x1 = x[:,misclassified[0]]\n",
    "        y1 = y[0,misclassified[0]]\n",
    "        return x1, y1, misclassified\n",
    "    return None, None, []\n",
    "\n",
    "def train(weight_init, bias_init, x, y, alpha=1.0, debug=False, params_best=None, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Trains the perceptron (single LTU) for the given data x and ground truth labels y\n",
    "    by using the perceptron learning algorithm with learning rate alpha (default is 1.0).\n",
    "    The max number of iterations is limited to 1000.\n",
    "    \n",
    "    Optionally, debug output can be provided in form of plots with showing the effect \n",
    "    of the update (decision boundary before and after the update) provided at each iteration.    \n",
    "    \n",
    "    Parameters:\n",
    "    weight_init -- weights vector of shape (1,2)\n",
    "    bias_init -- bias (a number)\n",
    "    x -- input dataset of shape (2,m)\n",
    "    y -- ground truth labels of shape (1,m)\n",
    "    alpha -- learning rate\n",
    "    debug -- flag for whether debug information should be provided for each iteration\n",
    "    params_best -- needed if debug=True for plotting the true decision boundary\n",
    "    \n",
    "    Returns:\n",
    "    weight -- trained weights\n",
    "    bias -- trained bias\n",
    "    misclassified_counts -- array with the number of misclassifications at each iteration\n",
    "    \"\"\"\n",
    "    weight = weight_init\n",
    "    bias = bias_init\n",
    "    iterations = 0\n",
    "    misclassified_counts = [] # we track them to show how the system learned in the end \n",
    "    ### START YOUR CODE ###\n",
    "    while iterations<=max_iter:\n",
    "        # get the params before\n",
    "        params_before = lineparams(weight, bias)\n",
    "        \n",
    "        # pick sample\n",
    "        x_i, y_i, misclassified = select_datapoint(x, y, weight, bias)\n",
    "        misclassified_counts.append(len(misclassified))\n",
    "        \n",
    "        # check if there are still missclassified samples\n",
    "        if x_i is None and y_i is None:\n",
    "            break\n",
    "        \n",
    "        # update parameters\n",
    "        weight, bias = update(x_i, y_i, weight, bias, alpha)\n",
    "        \n",
    "        # update the iterations\n",
    "        iterations += iterations\n",
    "        \n",
    "        if debug:\n",
    "            params_after = lineparams(weight, bias)\n",
    "            plot(x,y,\n",
    "                 params_best=params_best, \n",
    "                 params_before=params_before, \n",
    "                 params_after=params_after, \n",
    "                 misclassified=misclassified, \n",
    "                 selected=np.array([misclassified[0]]))\n",
    "    ### END YOUR CODE ###\n",
    "        \n",
    "    return weight, bias, misclassified_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weights_and_bias(a,s):\n",
    "    \"\"\"\n",
    "    Computes weights vector and bias from line parameters x2-intercept and slope.\n",
    "    \"\"\"\n",
    "    w1 = - s\n",
    "    w2 = 1.0\n",
    "    weight = np.array([w1,w2]).reshape(1,2)\n",
    "    bias = - a\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "1/ Prepare the dataset by using the prepare_data function defined above and plot it. Use the parameters specified below (a=1, s=2, n=100, n1=50).\n",
    "\n",
    "2/ Run the training with the default learning rate (alpha=1).\n",
    "Paste the plots with the situation at the start and with the situation at the end of the training in a text document.\n",
    "Paste also the start parameters (weight and bias) and trained parameters.\n",
    "\n",
    "3/ Create a plot with the number of mis-classifications vs iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1/ Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "m1 = 50\n",
    "a = 1\n",
    "s = 2\n",
    "x,y = prepare_data(m,m1,a,s)\n",
    "\n",
    "params_best = (a,s)\n",
    "weight_best, bias_best = weights_and_bias(a, s)\n",
    "print(\"weight: \", weight_best, \"  bias: \", bias_best)\n",
    "plot(x,y,params_best=params_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2/ Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 0\n",
    "s1 = 0\n",
    "alpha = 1.0\n",
    "\n",
    "weight1, bias1 = weights_and_bias(a1,s1)\n",
    "print(\"Initial Params: \",weight1,bias1)\n",
    "params = lineparams(weight1, bias1)\n",
    "print(params)\n",
    "plot(x,y,params_best, params)\n",
    "\n",
    "#weight1,bias1,misclassified_counts = train(weight1, bias1, x, y)\n",
    "weight1,bias1,misclassified_counts = train(weight1, bias1, x, y, debug=True, params_best=params_best)\n",
    "params = lineparams(weight1, bias1)\n",
    "print(\"Iterations: \", len(misclassified_counts)-1)\n",
    "print(\"Trained Params: \", weight1,bias1)\n",
    "plot(x,y, params_best=params_best, params_after=params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3/ Create the plot with the misclassifications per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = len(misclassified_counts)\n",
    "it = np.linspace(0,nit,nit)\n",
    "\n",
    "plt.plot(it, misclassified_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "The LTU converges after 29 iterations to a feasible solution. However, this solution is not the same as the green dotted decision boundary, but still, one that is suitable to differentiate the two classes. Further, it can be seen that the algorithm first tries to find the correct intersect and then only in the last few steps optimizes the slope, but this is heavily dependant on the selected data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Lightweight MNIST Dataset (optional)\n",
    "The following method allows to load the dataset. It generates two sets of (x,y)-pairs where\n",
    "* x are 64-dim arrays corresponding to the grayscale values of a 8x8 pixel image\n",
    "* y are numbers indicating the digit (1 indicating a first selected digit and 0 a second selected digit).\n",
    "\n",
    "The first of the two sets is then used for training the perceptron model, i.e. determining the weights and bias. The second is used for testing.\n",
    "\n",
    "### Prepare Dataset\n",
    "See the doc string of the method below for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(selected_digits):\n",
    "    \"\"\"\n",
    "    Loads the Lightweight MNIST dataset from sklearn, filters the data for two digits (two distinct label values between 0 and 9) \n",
    "    and split the resulting dataset of (x,y)-pairs (where x are 64 dimensional arrays corresponding to the grayscale values of a \n",
    "    8x8 pixel image) in a two distinct subsets: one will be used for tuning the weights of the learning algorithm, the second for \n",
    "    testing it. The samples with the first digit will be associated with a y-value equal to 1, the second with a y-value equal to 0.  \n",
    "    \n",
    "    Parameters:\n",
    "    selected_digits -- tuple with two distinct digit\n",
    "    \n",
    "    Returns:\n",
    "    x_train -- x-values of the subset of images used for training (containing only selected digits)\n",
    "    x_test -- x-values of the subset of images used for testing (containing only selected digits)\n",
    "    y_train -- y-values of the subset of labels (associated with the images) used for training ('1' indicating that the associated \n",
    "    image depicts the first selected digit, '0' indicating that the associated image depicts the second selected digit)\n",
    "    y_test -- y-values as above but used for testing\n",
    "    \"\"\"\n",
    "    # load digits\n",
    "    digits = load_digits()\n",
    "    x = digits.data.T\n",
    "    y = digits.target.reshape(1,x.shape[1])\n",
    "    \n",
    "    # select two given digits - will the train a model that learns to differentiate between the two\n",
    "    digit1 = selected_digits[0]\n",
    "    digit2 = selected_digits[1]\n",
    "    mask1 = y[0,:]==digit1\n",
    "    mask2 = y[0,:]==digit2\n",
    "    x1 = x[:,mask1 | mask2]\n",
    "    y1 = y[0,mask1 | mask2]\n",
    "    y1 = y1.reshape(1,y1.size)\n",
    "    \n",
    "    ## Define the label for the binary classification\n",
    "    mask1 = y1[0,:]==digit1\n",
    "    mask2 = y1[0,:]==digit2\n",
    "    y1[0,mask1] = 1\n",
    "    y1[0,mask2] = 0\n",
    "\n",
    "    print(\"Selecting %i images with digit %i and %i images with digit %i\"%(np.sum(mask1),digit1,np.sum(mask2),digit2))\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x1.T, y1.T, test_size=0.20, random_state=1)\n",
    "\n",
    "    # reshape - transpose back the output obtained from the train_test_split-function\n",
    "    x_train = x_train.T\n",
    "    x_test = x_test.T\n",
    "    m_train = x_train.shape[1]\n",
    "    m_test = x_test.shape[1]\n",
    "    y_train=y_train.reshape(1,m_train)\n",
    "    y_test=y_test.reshape(1,m_test)\n",
    "\n",
    "    print(\"Shape training set: \", x_train.shape, y_train.shape)\n",
    "    print(\"Shape test set:     \", x_test.shape, y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Perceptron Learning Algorithm\n",
    "\n",
    "Adjust the implementation with similar functions as above - here denoted by\n",
    "* predict_digit\n",
    "* update_weights\n",
    "* select_digit\n",
    "* train_weights\n",
    "\n",
    "to cope with the different digits dataset. Specifically, check that you get the shapes of the numpy arrays correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(x,w,b):\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a perceptron (single LTU).\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input dataset of shape (n,m)\n",
    "    w -- weights vector of shape (1,n)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    y -- prediction of a perceptron (single LTU) of shape (1,m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    y = (np.dot(w, x) + b) >= 0\n",
    "    ### END YOUR CODE ###\n",
    "    return y\n",
    "\n",
    "def update_weights(x,y,w,b,alpha=1.0):\n",
    "    \"\"\"\n",
    "    Performs an update step in accordance with the perceptron learning algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    x -- input data point of shape (n,1)\n",
    "    y -- true label ('ground truth') for the specified point\n",
    "    w -- weight vector of shape (1,n)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    w1 -- updated weight vector\n",
    "    b1 -- updated bias\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    y_pred = predict_digit(x,w,b)\n",
    "    w1 = w - alpha * (y_pred - y) * x\n",
    "    b1 = b - alpha * (y_pred - y)\n",
    "    ### END YOUR CODE ###\n",
    "    return w1, b1\n",
    "\n",
    "\n",
    "def select_digit(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Identifies the misclassified data points and selects one of them.\n",
    "    In case all datapoints are correctly classified None is returned. \n",
    "\n",
    "    Parameters:\n",
    "    x -- input dataset of shape (n,m)\n",
    "    y -- ground truth labels of shape (1,m)\n",
    "    w -- weights vector of shape (1,n)\n",
    "    b -- bias (a number)\n",
    "    \n",
    "    Returns:\n",
    "    x1 -- one of the wrongly classified datapoint (of shape (n,1))\n",
    "    y1 -- the associated true label\n",
    "    misclasssified -- array with indices of wrongly classified datapoints or empty array\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    y_pred = predict_digit(x,w,b)\n",
    "    wrong_mask = (y_pred != y)[0]\n",
    "    misclassified = np.where(wrong_mask)[0]\n",
    "    if len(misclassified)>0:\n",
    "        x1 = x[:,misclassified[0]]\n",
    "        y1 = y[0,misclassified[0]]\n",
    "        return x1, y1, misclassified\n",
    "    return None, None, []\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "def train_weights(weight_init, bias_init, x, y, alpha=1.0, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Trains the perceptron (single LTU) for the given data x and ground truth labels y\n",
    "    by using the perceptron learning algorithm with learning rate alpha (default is 1.0).\n",
    "    The max number of iterations is limited to 1000.\n",
    "    \n",
    "    Optionally, debug output can be provided in form of plots with showing the effect \n",
    "    of the update (decision boundary before and after the update) provided at each iteration.    \n",
    "    \n",
    "    Parameters:\n",
    "    weight_init -- weights vector of shape (1,n)\n",
    "    bias_init -- bias (a number)\n",
    "    x -- input dataset of shape (n,m)\n",
    "    y -- ground truth labels of shape (1,m)\n",
    "    alpha -- learning rate\n",
    "    debug -- flag for whether debug information should be provided for each iteration\n",
    "    \n",
    "    Returns:\n",
    "    weight -- trained weights\n",
    "    bias -- trained bias\n",
    "    misclassified_counts -- array with the number of misclassifications at each iteration\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    weight = weight_init\n",
    "    bias = bias_init\n",
    "    iterations = 0\n",
    "    misclassified_counts = [] # we track them to show how the system learned in the end\n",
    "    while iterations<=max_iter:\n",
    "        # pick sample\n",
    "        x_i, y_i, misclassified = select_digit(x, y, weight, bias)\n",
    "        misclassified_counts.append(len(misclassified))\n",
    "        \n",
    "        # check if there are still missclassified samples\n",
    "        if x_i is None and y_i is None:\n",
    "            break\n",
    "        \n",
    "        # update parameters\n",
    "        weight, bias = update_weights(x_i, y_i, weight, bias, alpha)\n",
    "        \n",
    "        # update the iterations\n",
    "        iterations += iterations\n",
    "    ### END YOUR CODE ###\n",
    "    return weight, bias, misclassified_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Run Training\n",
    "\n",
    "With the digits (1,7) the perceptron learning algorithm should converge. If you are interested, check whether there are digit pairs fro which it does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_digits = (1,7)\n",
    "x_train, x_test, y_train, y_test = load_data(selected_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect two sample images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x_train[:,17]\n",
    "plt.figure(1)\n",
    "plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "print(y_train[0,17])\n",
    "image = x_test[:,25]\n",
    "plt.figure(2)\n",
    "plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "print(y_train[0,25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros((1,64),dtype=float)\n",
    "bias = 0.0\n",
    "weight1,bias1,misclassified_counts = train_weights(weights, bias, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_digit(x_train,weight1,bias1)\n",
    "print(len(np.where(y_pred[0,:] != y_train[0,:])[0]))\n",
    "y_pred = predict_digit(x_test,weight1,bias1)\n",
    "print(len(np.where(y_pred[0,:] != y_test[0,:])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize how training has progressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = len(misclassified_counts)\n",
    "it = np.linspace(0,nit,nit)\n",
    "\n",
    "plt.plot(it, misclassified_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all the (distinct) digit pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "available_digits = list(range(10))\n",
    "for digit_1, digit_2 in itertools.combinations(available_digits, 2):\n",
    "    # select the two digits to differentiate\n",
    "    selected_digits = (digit_1, digit_2)\n",
    "    print('Measuring performance for digits: {0}, {1}'.format(digit_1, digit_2))\n",
    "    x_train, x_test, y_train, y_test = load_data(selected_digits)\n",
    "    # train the LTU\n",
    "    weights = np.zeros((1,64),dtype=float)\n",
    "    bias = 0.0\n",
    "    weight1,bias1,misclassified_counts = train_weights(weights, bias, x_train, y_train)\n",
    "    print('N. of iterations needed to converge: %s' % len(misclassified_counts))\n",
    "    # measure performance\n",
    "    y_train_pred = predict_digit(x_train,weight1,bias1)\n",
    "    y_test_pred = predict_digit(x_test,weight1,bias1)\n",
    "    train_err = len(np.where(y_train_pred[0,:] != y_train[0,:])[0])\n",
    "    test_err = len(np.where(y_test_pred[0,:] != y_test[0,:])[0])\n",
    "    print('Train err: {0}, Test err: {1}'.format(train_err, test_err))\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The LTU converges for all distinct digit combination under the max iterations (1000). The results from the LTU fitted on different digit combinations shows that there is a difference in the performance depending on the chosen combination. The results also show that the model can fit the train set perfectly in all cases but cannot generalize to unseen ones in some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
