{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed2583f",
   "metadata": {},
   "source": [
    "**a) Give two desired properties of word embeddings when used in a classification task (such as sentiment classification).** <br><br>\n",
    "Removing html tags, multiple spaces, single characters, etc. (?)\n",
    "\n",
    "**b) What are the three strategies to use an embedding layer in a deep system ?**<br><br>\n",
    "One Hot Encodings: To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. <br>\n",
    "Averaged: Representation vector of unknown token is set to the vector of a word with frequency 1 that is the closest to an average of vectors of words;\n",
    "random: Representation vector of unknown token is set to a representation of a random word. <br>\n",
    "\n",
    "**c) Why is the system of exercise 1 working in the end ?**<br><br>\n",
    "\n",
    "**d) What is the difference between Word2Vec and FastText ?**<br><br>\n",
    "Fasttext (which is essentially an extension of word2vec model), treats each word as composed of character ngrams. So the vector for a word is made of the sum of this character n grams. For example the word vector “apple” is a sum of the vectors of the n-grams “<ap”, “app”, ”appl”, ”apple”, ”apple>”, “ppl”, “pple”, ”pple>”, “ple”, ”ple>”, ”le>” (assuming hyperparameters for smallest ngram[minn] is 3 and largest ngram[maxn] is 6).\n",
    "Source: https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText<br>\n",
    "\n",
    "**e) What are the different forms of sequence mapping allowed by recurrent neural networks ? Give for each form an example of application.**<br><br>\n",
    "\n",
    "**f) Compute the number of parameters to be trained for a two-layer SimpleRNN and softmax with hidden state dimensions 32 and 64, respectively, 10 classes to classify in the softmax and inputs given by sequences of length 100 and each element a vector of dimension 30.**<br><br>\n",
    "\n",
    "**g) Compute the number of parameters to be trained for a two-layer LSTM and softmax with hidden state dimensions 32 and 64, respectively, 10 classes to classify in the softmax and inputs given by sequences of length 100 and each element a vector of dimension 30.**<br><br>\n",
    "\n",
    "**h) Why is gradient clipping rather needed in long than in short sentences ?**<br><br>\n",
    "\n",
    "**i) In what situations would you expect LSTMs (by its design) to perform better than SimpleRNNs ?**<br><br>\n",
    "It’s too difficult for RNN to learn to preserve information over many timesteps. Smaller gradient means it will not affect the weight updation. Due to this, the network does not learn the effect of earlier inputs. Thus, causing the short-term memory problem. Solution for vanishing gradient which is provided by LSTM.<br>\n",
    "\n",
    "**j) Describe why SimpleRNNs have problems in learning long-term dependencies.**<br><br>\n",
    "Standard RNNs (Recurrent Neural Networks) suffer from vanishing and exploding gradient problems. LSTMs (Long Short Term Memory) deal with these problems by introducing new gates, such as input and forget gates, which allow for a better control over the gradient flow and enable better preservation of “long-range dependencies”.<br>\n",
    "\n",
    "**k) How can you define a generative system for sequence data ? Describe the two approaches seen in the class to build generative systems for sequence data**.<br><br>\n",
    "\n",
    "**l) Draw the architecture of a Seq2Seq model and explain the encoder/decoder concept.**<br><br>\n",
    "![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png)\n",
    "The Encoder, consisting of RNNs, takes the sequence as an input and generates a final embedding at the end of the sequence. This is then sent to the Decoder, which then uses it to predict a sequence, and after every successive prediction, it uses the previous hidden state to predict the next instance of the sequence.\n",
    "![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ3.png)\n",
    "\n",
    "**m) What is the main problem of a basic Seq2Seq model used for machine translation and the solution that can be used to overcome this problem ?**<br><br>\n",
    "Mapping is more difficult for long sequences. A solution is to plug-in attention mechanisms.\n",
    "Attention is an interface between the encoder and decoder that provides the decoder with information from all (or parts of) the encoder hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e227f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
