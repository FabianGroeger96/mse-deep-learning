{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Keras on CIFAR10\n",
    "\n",
    "Modify your MLP version from the previous exercise towards Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import TF and get its version.\n",
    "import tensorflow as tf\n",
    "tf_version = tf.__version__\n",
    "\n",
    "# Check if version >=2.0.0 is used\n",
    "if not tf_version.startswith('2.'):\n",
    "    print('WARNING: TensorFlow >= 2.0.0 will be used in this course.\\nYour version is {}'.format(tf_version) + '.\\033[0m')\n",
    "else:\n",
    "    print('OK: TensorFlow >= 2.0.0' + '.\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(X):\n",
    "    plt.figure(1)\n",
    "    k = 0\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            plt.subplot2grid((5,5),(i,j))\n",
    "            plt.imshow(X[k], cmap='gray')\n",
    "            k = k+1\n",
    "            plt.axis('off')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "# Load data & split data between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "CLASS_NAMES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "show_imgs(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "n_classes = 10\n",
    "# Conversion to class vectors\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 30\n",
    "B = 128\n",
    "D = X_train.shape[1:]\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), input_shape=D, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log = model.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network\n",
    "\n",
    "#### Loss evolution during training\n",
    "This can be done first looking at the history of the training (output of the `fit()` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Val acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall accuracy evaluation on test set\n",
    "We can compute the overall performance on test set calling the `evaluate()` function on the model. The function returns the loss and the metrics used to compile the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, metric_train = model.evaluate(X_train, Y_train, verbose=0)\n",
    "loss_test, metric_test = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('TRAIN: \\t loss: {0:.4f}\\t accuracy: {1:.4f}'.format(loss_train, metric_train))\n",
    "print('TEST: \\t loss: {0:.4f}\\t accuracy: {1:.4f}'.format(loss_test, metric_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "project_id = 'deep-learning-sw08'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train.shape[1:]\n",
    "\n",
    "# initialize wandb with your project name and optionally with configutations.\n",
    "run = wandb.init(project=project_id,\n",
    "           config={\n",
    "              \"learning_rate\": 0.005,\n",
    "              \"epochs\": 25,\n",
    "              \"batch_size\": 64,\n",
    "               \"activation\": 'relu',\n",
    "              \"loss_function\": \"categorical_crossentropy\",\n",
    "              \"architecture\": \"CNN\",\n",
    "              \"dataset\": \"CIFAR-10\"\n",
    "           })\n",
    "config = wandb.config\n",
    "\n",
    "def create_model(config):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), input_shape=D, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=config.loss_function, \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = create_model(config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the network\n",
    "In Keras, we call the methods `compile()` and `fit()`. For the compile phase, we need to specify the **loss** function which should be set in the case of multi-class classification to `categorical_crossentropy`. We also need to specify the optimizer strategy. In this case the `rmsprop` or `adam` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in order to get prediction on small subset of images.\n",
    "val_images, val_labels = X_test[:32], Y_test[:32]\n",
    "\n",
    "log = model.fit(X_train, Y_train, \n",
    "                epochs=config.epochs, \n",
    "                batch_size=config.batch_size,\n",
    "                validation_data=(X_test, Y_test),\n",
    "                callbacks=[WandbCallback(data_type='image', \n",
    "                                   training_data=(val_images, val_labels), \n",
    "                                   labels=CLASS_NAMES)],\n",
    "                verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network\n",
    "\n",
    "#### Loss evolution during training\n",
    "This can be done first looking at the history of the training (output of the `fit()` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Val acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall accuracy evaluation on test set\n",
    "We can compute the overall performance on test set calling the `evaluate()` function on the model. The function returns the loss and the metrics used to compile the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, metric_train = model.evaluate(X_train, Y_train, verbose=0)\n",
    "loss_test, metric_test = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('TRAIN: \\t loss: {0:.4f}\\t accuracy: {1:.4f}'.format(loss_train, metric_train))\n",
    "print('TEST: \\t loss: {0:.4f}\\t accuracy: {1:.4f}'.format(loss_test, metric_test))\n",
    "\n",
    "# log to wandb\n",
    "wandb.log({'Train Error Rate': round((1-metric_train)*100, 2)})\n",
    "wandb.log({'Test Error Rate': round((1-metric_test)*100, 2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    model = Sequential()\n",
    "    \n",
    "    ### 1. layer\n",
    "    model.add(Conv2D(filters=config.filters[0], \n",
    "                     kernel_size=(config.kernel, config.kernel), \n",
    "                     strides=1, \n",
    "                     padding=\"same\", \n",
    "                     activation=None,\n",
    "                     kernel_initializer=config.initializer,\n",
    "                     input_shape=(32,32,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(config.activation))\n",
    "    \n",
    "    ### 2. layer\n",
    "    model.add(Conv2D(filters=config.filters[0], \n",
    "                     kernel_size=(config.kernel, config.kernel), \n",
    "                     strides=1, \n",
    "                     padding=\"same\", \n",
    "                     activation=None,\n",
    "                     kernel_initializer=config.initializer))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(config.activation))\n",
    "    \n",
    "    ### 3.+ layers\n",
    "    for layer in config.filters[1:]:        \n",
    "        model.add(Conv2D(filters=layer, \n",
    "                         kernel_size=(config.kernel, config.kernel), \n",
    "                         strides=1, \n",
    "                         padding=\"same\", \n",
    "                         activation=None,\n",
    "                         kernel_initializer=config.initializer))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(config.activation))\n",
    "        model.add(MaxPooling2D())\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    ### 2nd last layer\n",
    "    model.add(Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1024, activation=None, kernel_initializer=config.initializer))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(config.activation))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    ### last layer\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Specify the hyperparameter to be tuned along with\n",
    "    # an initial value\n",
    "    config_defaults = {\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001,\n",
    "        'filters': [32, 64, 128, 256, 512],\n",
    "        'activation': 'relu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "        'kernel': 3\n",
    "    }\n",
    "    \n",
    "    # Initialize wandb with a sample project name\n",
    "    wandb.init(project=project_id,\n",
    "               config=config_defaults)\n",
    "\n",
    "    # Specify the other hyperparameters to the configuration, if any\n",
    "    wandb.config.epochs = 30\n",
    "    wandb.config.loss_function = 'categorical_crossentropy'\n",
    "    config = wandb.config\n",
    "    \n",
    "    # define model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_model(config)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss=config.loss_function, \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # train model\n",
    "    _ = model.fit(X_train, Y_train, \n",
    "                epochs=config.epochs, \n",
    "                batch_size=config.batch_size,\n",
    "                validation_data=(X_test, Y_test),\n",
    "                callbacks=[WandbCallback()],\n",
    "                verbose=0)\n",
    "    \n",
    "    # evaluate model\n",
    "    loss_test, metric_test = model.evaluate(X_test, Y_test, verbose=0, callbacks=[WandbCallback()])\n",
    "    wandb.log({'Test Error Rate': round((1-metric_test)*100, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  'method': 'bayes', \n",
    "  'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'\n",
    "  },\n",
    "  'early_terminate':{\n",
    "      'type': 'hyperband',\n",
    "      'min_iter': 5\n",
    "  },\n",
    "  'parameters': {\n",
    "      'batch_size': {\n",
    "          'values': [32, 64, 128, 256]\n",
    "      },\n",
    "      'learning_rate': {\n",
    "          'values': [0.005, 0.001, 0.0005, 0.0001]\n",
    "      },\n",
    "      'filters': {\n",
    "          'values': [\n",
    "              [32, 64, 128, 256, 512],\n",
    "              [32, 64, 128, 256, 512, 512],\n",
    "          ]\n",
    "      },\n",
    "      'activation': {\n",
    "          'values': ['elu', 'selu', 'relu']\n",
    "      },\n",
    "      'initializer': {\n",
    "          'values': ['he_normal', 'he_uniform', 'glorot_uniform', 'glorot_normal', 'lecun_normal']\n",
    "      },\n",
    "      'kernel': {\n",
    "          'values': [3]\n",
    "      }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=project_id)\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('wandb_export_sweep.csv')\n",
    "df_results.sort_values('val_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "config.activation = 'relu'\n",
    "config.batch_size = 32\n",
    "config.filters = [32,64,128,256,512,512]\n",
    "config.initializer = 'he_normal'\n",
    "config.learning_rate = 0.0005\n",
    "config.kernel = 3\n",
    "config.loss_function = 'categorical_crossentropy'\n",
    "config.epochs = 30\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "best_model = create_model(config)\n",
    "\n",
    "# compile model\n",
    "best_model.compile(loss=config.loss_function, \n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate), \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                  patience=5,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# train model\n",
    "log = best_model.fit(X_train, Y_train, \n",
    "                    epochs=config.epochs, \n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=0)\n",
    "\n",
    "# evaluate model\n",
    "loss_train, metric_train = best_model.evaluate(X_train, Y_train, verbose=0)\n",
    "loss_test, metric_test = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('TRAIN: \\t loss: {0:.4f}\\t accuracy: {1:.4f}'.format(loss_train, metric_train))\n",
    "print('TEST: \\t loss: {0:.4f}\\t accuracy: {1:.4f}'.format(loss_test, metric_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Val acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Model**   \n",
    "TRAIN:\n",
    "- loss: 0.0319\n",
    "- accuracy: 0.9929   \n",
    "\n",
    "TEST:\n",
    "- loss: 0.4820\n",
    "- accuracy: 0.8633"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
