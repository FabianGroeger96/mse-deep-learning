{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron from MNIST raw data using Keras\n",
    "\n",
    "This notebook will guide you through the use of the `keras` package to train a multilayer perceptron for handwritten digits classification. You are going to use the `mnist` dataset from LeCun et al. 1998\n",
    "\n",
    "We assume you are using TF 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: TensorFlow >= 2.0.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# First, import TF and get its version.\n",
    "import tensorflow as tf\n",
    "tf_version = tf.__version__\n",
    "\n",
    "# Check if version >=2.0.0 is used\n",
    "if not tf_version.startswith('2.'):\n",
    "    print('WARNING: TensorFlow >= 2.0.0 will be used in this course.\\nYour version is {}'.format(tf_version) + '.\\033[0m')\n",
    "else:\n",
    "    print('OK: TensorFlow >= 2.0.0' + '.\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "#  COMPLETE CODE BELOW WHERE YOU SEE # ...   #\n",
    "##############################################\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ... import here the different keras libraries you need\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw data\n",
    "Keras provides easy access to different datasets including MNIST. First load the `mnist` dataset and normalize it to be in the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(X):\n",
    "    plt.figure(1)\n",
    "    k = 0\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            plt.subplot2grid((5,5),(i,j))\n",
    "            plt.imshow(X[k], cmap='gray')\n",
    "            k = k+1\n",
    "            plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Load data & split data between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = # ...\n",
    "\n",
    "show_imgs(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  #...                      change the shape towards (60000, 784)\n",
    "X_test =   # ...                     idem (10000, 784)\n",
    "X_train =  # ...                     change the type towards float32\n",
    "X_test =   # ...                     idem\n",
    "X_train /= # ...                     normalize the range to be between 0.0 and 1.0\n",
    "X_test /=  # ...\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values of the network are supposed to be 1-hot targets. Now the `y_train` is an array with scalar values as in `[5 0 4 1 ...]` and it should be a 1-hot array `Y_train` as in : \n",
    "\n",
    "`[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]...]`\n",
    " \n",
    "Note the change of capital letter in the `Y_train` to denote, per convention, an array with multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "Y_train = # ...                  modify targets to 1-hot using utils.to_categorical()\n",
    "Y_test = # ...                   idem \n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-layer network and weight visualisation\n",
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 5                # number of epochs\n",
    "B = 128               # batch size\n",
    "D = X_train.shape[1]  # dimension of input sample - 784 for MNIST\n",
    "\n",
    "# ... define the model as a Sequential type\n",
    "# ... add a layer of type Dense with softmax activation\n",
    "\n",
    "# ... print model infomration with summary() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train the network\n",
    "In Keras, we call the methods `compile()` and `fit()`. For the compile phase, we need to specify the **loss** function which should be set in the case of multi-class classification to `categorical_crossentropy`. We also need to specify the optimizer strategy. In this case the `rmsprop` or `adam` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... compile the model with a crossentropy loss, rmsprop optimizer \n",
    "#      and defining metrics to be stored as history of the training\n",
    "\n",
    "# ... call the training with the fit() function giving the tensors as \n",
    "#     inputs, defining batch size, number of epochs and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network\n",
    "\n",
    "We can do this at three levels: (1) plot of the loss during the training phase, (2) overall accuracy evaluation on test set and (3) per class evaluation with confusion matrix on test set.\n",
    "\n",
    "### Loss evolution during training\n",
    "This can be done first looking at the history of the training (output of the `fit()` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Testing acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "We can compute the overall performance on test set calling the `evaluate()` function on the model. The function returns the loss and the metrics used to compile the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = # ... evaluate model performance on test set\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "We can call the `predict_classes()` function to get the predicted classes. The output of this function is an array with the predicted class labels as in `[5 0 4 1 ...]`. The output array of ground truth `y_test` and the predicted classes can then be fed to the `confusion_matrix()` function of [sklearn metrics package](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test, verbose=1)\n",
    "me.confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the weights\n",
    "The weights connected to a given neuron, when using a one-layer network, have the same shape as the input. They can therefore be plot. To do so we need to re-scale the weight values into 0-255 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "for w in weights:\n",
    "    print(w.shape)\n",
    "w1 = weights[0]\n",
    "f = plt.figure(figsize=(12,12))\n",
    "for i in range(10):\n",
    "    ax = f.add_subplot(1, 10, 1+i)\n",
    "    im = #...                      get the weights landing to neuron i\n",
    "    im = #...                      reshape the vector of weights into 28x28 image\n",
    "    # now put back the pixel values to 0-256 doing a min-max norm and multiplying by 256\n",
    "    min = #...                     get the min of im\n",
    "    max = #...                     get the max of im\n",
    "    im = #...                      perform a min-max norm\n",
    "    im = #...                      convert to 8 bits pixel values\n",
    "    im = #...                      round and convert to int\n",
    "    ax.axis('off')\n",
    "    ax.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
