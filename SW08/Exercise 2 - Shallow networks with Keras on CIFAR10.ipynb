{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow networks with Keras on CIFAR10\n",
    "\n",
    "Modify the example given in the class on the MNIST dataset to train and evaluate a shallow\n",
    "networks on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import TF and get its version.\n",
    "import tensorflow as tf\n",
    "tf_version = tf.__version__\n",
    "\n",
    "# Check if version >=2.0.0 is used\n",
    "if not tf_version.startswith('2.'):\n",
    "    print('WARNING: TensorFlow >= 2.0.0 will be used in this course.\\nYour version is {}'.format(tf_version) + '.\\033[0m')\n",
    "else:\n",
    "    print('OK: TensorFlow >= 2.0.0' + '.\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(X):\n",
    "    plt.figure(1)\n",
    "    k = 0\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            plt.subplot2grid((5,5),(i,j))\n",
    "            plt.imshow(X[k], cmap='gray')\n",
    "            k = k+1\n",
    "            plt.axis('off')\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "# Load data & split data between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "show_imgs(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.dtype)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "n_classes = 10\n",
    "# Conversion to class vectors\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-layer network and weight visualisation\n",
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 5                # number of epochs\n",
    "B = 128              # batch size\n",
    "D = X_train.shape[1:] # dimension of input sample - 784 for MNIST\n",
    "\n",
    "regularizer = tf.keras.regularizers.l2(0.01)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=D))\n",
    "model.add(Dense(n_classes, activation='softmax', kernel_regularizer=regularizer))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the network\n",
    "In Keras, we call the methods `compile()` and `fit()`. For the compile phase, we need to specify the **loss** function which should be set in the case of multi-class classification to `categorical_crossentropy`. We also need to specify the optimizer strategy. In this case the `rmsprop` or `adam` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "log = model.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the weights\n",
    "The weights connected to a given neuron, when using a one-layer network, have the same shape as the input. They can therefore be plot. To do so we need to re-scale the weight values into 0-255 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "for w in weights:\n",
    "    print(w.shape)\n",
    "w1 = weights[0]\n",
    "f = plt.figure(figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    ax = f.add_subplot(1, 10, 1+i)\n",
    "    im = w1[:,i]\n",
    "    im = im.reshape(32, 32, 3)\n",
    "    # now put back the pixel values to 0-256 doing a min-max norm and multiplying by 256\n",
    "    min = np.min(im)\n",
    "    max = np.max(im)\n",
    "    im = np.round((im - min)/(max - min) * 255).astype(int)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layers network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 30\n",
    "B = 128\n",
    "D = X_train.shape[1:]\n",
    "\n",
    "def create_model(n_neurons:int=128):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=D))\n",
    "    model.add(Dense(n_neurons, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the network\n",
    "In Keras, we call the methods `compile()` and `fit()`. For the compile phase, we need to specify the **loss** function which should be set in the case of multi-class classification to `categorical_crossentropy`. We also need to specify the optimizer strategy. In this case the `rmsprop` or `adam` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log = model.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the network\n",
    "\n",
    "### Loss evolution during training\n",
    "This can be done first looking at the history of the training (output of the `fit()` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,4))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "ax1.plot(log.history['loss'], label='Training loss')\n",
    "ax1.plot(log.history['val_loss'], label='Testing loss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.plot(log.history['accuracy'], label='Training acc')\n",
    "ax2.plot(log.history['val_accuracy'], label='Val acc')\n",
    "ax2.legend()\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "We can compute the overall performance on test set calling the `evaluate()` function on the model. The function returns the loss and the metrics used to compile the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [128]\n",
    "epochs = [30]\n",
    "n_neurons = [128, 256]\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  n_neurons=n_neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
